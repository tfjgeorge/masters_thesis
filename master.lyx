#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsbook
\use_default_options true
\begin_removed_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_removed_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Chapter
Neural networks
\end_layout

\begin_layout Section
Artificial intelligence
\end_layout

\begin_layout Section
Machine learning
\end_layout

\begin_layout Subsection
Parametric functions and learning
\end_layout

\begin_layout Itemize
task : given input datum, give an answer
\end_layout

\begin_layout Itemize
common probabilistic tasks 
\end_layout

\begin_layout Itemize
common industry tasks
\end_layout

\begin_layout Subsection
Empirical risk
\end_layout

\begin_layout Itemize
estimate the true distribution using a dataset of examples
\end_layout

\begin_layout Itemize
data augmentation
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Itemize
bias / variance trade off
\end_layout

\begin_layout Itemize
overfitting
\end_layout

\begin_layout Section
Neural networks
\end_layout

\begin_layout Itemize
stack layers that produce new representations of the incoming data
\end_layout

\begin_layout Itemize
standard architecture : linear transformation / non-linearity elementwise
\end_layout

\begin_layout Section
Common types of neural networks
\end_layout

\begin_layout Subsection
Recurrent neural networks
\end_layout

\begin_layout Subsection
Convolutions
\end_layout

\begin_layout Subsection
Autoencoders
\end_layout

\begin_layout Subsection
Residual networks
\end_layout

\begin_layout Section
More elaborated cost functions
\end_layout

\begin_layout Itemize
quadratic, cross entropy
\end_layout

\begin_layout Itemize
neural art
\end_layout

\begin_layout Itemize
GANs
\end_layout

\begin_layout Chapter
An example task for neural networks: learning to generate texture images
\end_layout

\begin_layout Chapter
Optimization of neural networks
\end_layout

\begin_layout Section
Gradient descent and backpropagation
\end_layout

\begin_layout Itemize
no closed form solution
\end_layout

\begin_layout Itemize
gradient descent
\end_layout

\begin_layout Itemize
backpropagation
\end_layout

\begin_layout Standard
Starting from a model, and supposing this model capable of solving a given
 task, a main challenge of machine learning remains to learn from the data,
 to find the correct parameters that will make the model useful.
 Certain tasks have closed-form solutions, such as finding the correct weights
 and biases for a linear model and a regression task.
 This task can be formulated as finding weight matrix 
\begin_inset Formula $W$
\end_inset

 and bias vector 
\begin_inset Formula $b$
\end_inset

 corresponding to a dataset 
\begin_inset Formula $\mathcal{D}$
\end_inset

 of pairs of vectors 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $y_{i}$
\end_inset

 such that 
\begin_inset Formula $R_{\mathcal{D}}=\sum_{i}\left\Vert Wx_{i}+b-y_{i}\right\Vert ^{2}$
\end_inset

 is minimized.
 If we stack our training examples in matrices 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 with each row representing an example and its target value, this reduces
 to the following linear algebra problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
W,b & = & \text{argmin}_{W,b}\left\Vert XW^{T}+b-Y\right\Vert ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In contrary, for neural networks, as for other models, we can not derive
 a closed-form solution [TODO] explication (plusieurs minima globaux).
 [WHO] proposed to use gradient descent to solve such tasks.
 Instead of finding the target solution directly, we will update our parameters
 
\begin_inset Formula $\theta$
\end_inset

 by a small increment 
\begin_inset Formula $\theta\leftarrow\theta+\Delta\theta$
\end_inset

 that decreases the value of the empirical risk.
 It can be any arbitrary value, as long as it make the empirical risk decrease.
 Such a value is given by the inverse of the derivative multiplied by a
 small scalar value: 
\begin_inset Formula $\Delta\theta=-\lambda\frac{\partial R}{\partial\theta}$
\end_inset

.
 TODO theoretical convergence proofs for gradient descent: is guaranteed
 to converge in 
\begin_inset Formula $O\left(\frac{1}{t}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
What also makes gradient descent suitable for neural networks, is that it
 can conveniently be computed using forward and backward propagation of
 computations that take advantage of the chain-rule for derivative, and
 of the sequential structure of neural networks computations.
\end_layout

\begin_layout Standard
[TODO figure fprop backprop]
\end_layout

\begin_layout Section
Stochastic gradient descent
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "bottou2010large"

\end_inset


\end_layout

\begin_layout Itemize
gradient estimate
\end_layout

\begin_layout Itemize
computational considerations
\end_layout

\begin_layout Itemize
regularization effect of stochasticity ?
\end_layout

\begin_layout Itemize
memory considerations (how much memory needed by mb gradient descent)
\end_layout

\begin_layout Section
Hyperparameters and focus on the learning rate
\end_layout

\begin_layout Itemize
hyperparameters (difference with parameters)
\end_layout

\begin_layout Itemize
HP tuning (grid search, random search 
\begin_inset CommandInset citation
LatexCommand cite
key "bergstra2012random"

\end_inset

, 
\begin_inset Quotes eld
\end_inset

biased
\begin_inset Quotes erd
\end_inset

 random search)
\end_layout

\begin_layout Itemize
learning rate using the hessian + mathematical results
\end_layout

\begin_layout Standard
Amongst other hyperparameters, the learning rate of standard (stochastic)
 gradient descent plays a particular role which we will show in the following.
 We can rewrite the standard gradient descent result using a second order
 Taylor expansion of the empirical risk:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
R\left(\theta+\Delta\theta\right) & = & R\left(\theta\right)+\left(\nabla R\right)_{\theta}\Delta\theta+\frac{1}{2}\Delta\theta^{T}\mathbf{H}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert ^{3}\right)\\
 & \approx & R\left(\theta\right)+\left(\nabla R\right)_{\theta}\Delta\theta+\frac{1}{2}\Delta\theta^{T}\mathbf{H}\Delta\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Limits of (stochastic) gradient descent
\end_layout

\begin_layout Itemize
gradient magnitudes (exploding/vanishing gradient) between layers
\end_layout

\begin_layout Itemize
gradient magnitures between examples ?
\end_layout

\begin_layout Itemize
follow directions of steepest descent, instable in valleys
\end_layout

\begin_layout Section
A standard benchmark: Autoencoding written digits
\end_layout

\begin_layout Itemize
presentation mnist
\begin_inset CommandInset citation
LatexCommand cite
key "lecun2010mnist"

\end_inset


\end_layout

\begin_layout Itemize
task, history (hinton)
\end_layout

\begin_layout Itemize
limits of the architecture
\end_layout

\begin_deeper
\begin_layout Itemize
sigmoids
\end_layout

\begin_layout Itemize
multi layer perceptron instead of conv
\end_layout

\end_deeper
\begin_layout Itemize
limits of the benchmark
\end_layout

\begin_deeper
\begin_layout Itemize
specific task
\end_layout

\begin_layout Itemize
quadratic loss
\end_layout

\end_deeper
\begin_layout Itemize
HP plots
\end_layout

\begin_layout Chapter
Advanced optimization
\end_layout

\begin_layout Section
Gradient smoothing methods
\end_layout

\begin_layout Itemize
cost landscape with valleys and plateaus
\end_layout

\begin_layout Itemize
momentum
\end_layout

\begin_layout Section
Batch normalization
\end_layout

\begin_layout Itemize
covariate shift
\end_layout

\begin_layout Itemize
effect on the forward pass, and the gradient
\end_layout

\begin_layout Section
Second order methods
\end_layout

\begin_layout Itemize
quadratic hypothesis
\end_layout

\begin_layout Itemize
Newton, gauss-newton
\end_layout

\begin_layout Section
Gradient linearization
\end_layout

\begin_layout Itemize
motivation: do not update all parameters at once but instead layer by layer,
 updating the gradient to account for modification of the parameters before
 updating other layers
\end_layout

\begin_layout Itemize
expressions for common loss functions
\end_layout

\begin_layout Section
Second order: a new perspective
\end_layout

\begin_layout Itemize
factorization of the hessian & 
\end_layout

\begin_layout Itemize
measure of the induced change on the output of the function/on the loss
 (jaco, gradient, and fisher)
\end_layout

\begin_layout Section
Methods using the FIM
\end_layout

\begin_layout Itemize
kfac
\end_layout

\begin_layout Itemize
nnn
\end_layout

\begin_layout Itemize
overcoming catastrophic forgetting
\end_layout

\begin_layout Section
Links with well-known methods
\end_layout

\begin_layout Itemize
rmsprop
\end_layout

\begin_layout Itemize
msprop
\end_layout

\begin_layout Chapter*
Conclusions
\end_layout

\begin_layout Standard
best master's thesis ever
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
