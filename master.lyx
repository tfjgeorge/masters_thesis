#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsbook
\begin_preamble
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{dsfont}
\newcommand{\mathbbOld}{\mathbb}
\renewcommand{\mathbb}{\mathds}
\end_preamble
\use_default_options true
\begin_removed_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_removed_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Chapter
Neural networks
\end_layout

\begin_layout Standard
In this chapter, we will progressively introduce concepts and techniques
 that are used in artificial intelligence tasks which get more and more
 complex.
 In particular, we will introduce neural networks, that have proven a powerful
 model and produced state of the art results in a variety of tasks.
\end_layout

\begin_layout Section
Artificial intelligence
\end_layout

\begin_layout Standard
Intelligence is a difficult concept to define.
 We will use the following definition: the ability to make sensible decisions
 in a given situation, possibly making use of a memory of past events that
 share similarities with the current situation.
 The most powerful individual intelligent agent that we can experience nowadays
 is certainly the human being.
 Human beings are constantly making decisions given their perception of
 the world that is provided by their 5 senses, using knowledge that they
 have studied or experienced in their life.
 But there is no 
\shape italic
a priori
\shape default
 reason to think that intelligence could not be present in other systems,
 and in particular artificial intelligence is a scientific field that aims
 at implementing intelligence in non-living machines.
\end_layout

\begin_layout Standard
How our society of humans can benefit from artificial intelligence is still
 an open question, out of the scope of the present document.
 Regardless, given the recent popularity of artificial intelligence among
 public research laboratories and in the industry, and the recent successes
 at solving complex tasks, we can say without taking risks that artificial
 intelligence will continue to play a big role in shaping the future of
 our society.
\end_layout

\begin_layout Standard
From a more practical perspective, implementing an artificial intelligent
 machine requires designing a system that takes data that represent the
 current situation, data that represents the memory of the machine, and
 output a decision using this data.
\end_layout

\begin_layout Standard
To put things into context, we will now describe an example task.
 We want to design a program that takes a picture of an animal and a sound
 as input, and outputs whether it thinks the animal present in the picture
 makes the provided sound.
 In a computer, a picture is often encoded as a tensor of pixels, the sound
 as a timeseries of samples of the sound wave, and the final decision can
 be a single scalar values, which will be close to 0 is the animal is very
 unlikely to make the noise, or to 1 if the animal is very likely to make
 the noise.
 The complex machinery inbetween is the intelligent part.
\end_layout

\begin_layout Standard
Manually designing this intelligent part is an overwhelming task.
 Even provided that the input image is quite a small image of 
\begin_inset Formula $32\times32$
\end_inset

 RGB pixels and the sound lasts 
\begin_inset Formula $1s$
\end_inset

 recorded at a sample rate of 20kHz we have a total of 
\begin_inset Formula $32\times32\times3+20\,000=23\,072$
\end_inset

 scalars.
 If we restrict each of these numbers to have 
\begin_inset Formula $256$
\end_inset

 possible values, it leaves us with 
\begin_inset Formula $256\times23\,072\approx6\cdot10^{6}$
\end_inset

 possible combinations.
\end_layout

\begin_layout Standard
Instead, the most successful attempts at solving such tasks use a procedure
 called 
\series bold
machine learning
\series default
: instead of manually defining our intelligent program, we define a generic
 model, and we use a dataset of annotated examples of picture, sound, and
 the corresponding answer, and we leave the computer extract informations
 from the dataset to tune the model so as to obtain the desired program.
\end_layout

\begin_layout Section
Machine learning
\end_layout

\begin_layout Subsection
Parametric functions and learning
\end_layout

\begin_layout Standard
Generally speaking, machine learning consists in finding an unknown function
 
\begin_inset Formula $f$
\end_inset

 from a family of functions 
\begin_inset Formula $\mathcal{F}$
\end_inset

, that will solve a certain task.
 We typically restrict our search to a smaller family of functions, which
 consists in parametrized functions 
\begin_inset Formula $\mathcal{F}_{\theta}$
\end_inset

.
 We will denote by 
\begin_inset Formula $f_{\theta}$
\end_inset

 such a function, parametrized by a vector of parameters 
\begin_inset Formula $\theta$
\end_inset

.
 Adapting the value of the parameters will change the output of the function
 
\begin_inset Formula $f_{\theta}$
\end_inset

.
 The challenge of machine learning is to find a correct parametrization
 so that our desired functions can be approached by an instance of 
\begin_inset Formula $\mathcal{F}_{\theta}$
\end_inset

, and to learn the parameters of this target function.
\end_layout

\begin_layout Standard
To this end, we chose a loss function 
\begin_inset Formula $l$
\end_inset

 adapted to the task we aim at solving, that measures how wrong is a given
 function at solving the task for a data point.
 The remaining part is the data generating distribution 
\begin_inset Formula $p$
\end_inset

 which allows to sample datapoints 
\begin_inset Formula $x\sim p$
\end_inset

.
 A measure of the performance of a function 
\begin_inset Formula $f_{\theta}$
\end_inset

 for the given task is given by the risk:
\begin_inset CommandInset label
LatexCommand label
name "cost-function"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{R}\left(\theta,p\right) & = & \mathbb{E}_{x\sim p}\left[l\left(f_{\theta}\left(x\right)\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathcal{R}$
\end_inset

 is a scalar value.
 If this value is high, then 
\begin_inset Formula $f_{\theta}$
\end_inset

 is bad at solving the desired task.
 In the opposite, the best function can be found by adjusting 
\begin_inset Formula $\theta$
\end_inset

 so as to reach the smallest value of 
\begin_inset Formula $\mathcal{R}$
\end_inset

.
 The best value for the parameter vector 
\begin_inset Formula $\theta^{*}$
\end_inset

 is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta^{*} & = & \text{argmin}_{\theta}\mathcal{R}\left(\theta,p\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We now present two common tasks and their corresponding loss functions.
 We will restrict to the less general setting of 
\shape italic
supervised
\shape default
 learning, where each data point is composed of an input 
\begin_inset Formula $x$
\end_inset

 and a target 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Standard
In
\series bold
 regression
\series default
, the input vector 
\begin_inset Formula $x$
\end_inset

 is mapped to a numerical value 
\begin_inset Formula $y$
\end_inset

.
 To assess the performance of 
\begin_inset Formula $f_{\theta}$
\end_inset

, we use the loss function 
\begin_inset Formula $l\left(f_{\theta}\left(x\right),y\right)=\left\Vert f_{\theta}\left(x\right)-y\right\Vert _{2}^{2}$
\end_inset

, called the quadratic error.
 It reaches its minimum 
\begin_inset Formula $0$
\end_inset

 when 
\begin_inset Formula $f_{\theta}\left(x\right)=y$
\end_inset

.
 For example we can design a model that predicts the price of a real estate,
 given some features such as the size of the house, the number of bedrooms
 and whether it possesses a fireplace.
\end_layout

\begin_layout Standard
In supervised 
\series bold
classification
\series default
, we classify each data point 
\begin_inset Formula $x$
\end_inset

 into a category 
\begin_inset Formula $y$
\end_inset

.
 A natural loss that comes up is the misclassification indicator function
 
\begin_inset Formula $\mathbb{1}\left(f_{\theta}\left(x\right),y\right)=\begin{cases}
0 & \text{if }f_{\theta}\left(x\right)=y\\
1 & \text{otherwise}
\end{cases}$
\end_inset

.
 It counts the examples that are misclassified.
 This function present the disadvantage of not being differentiable (it
 is not even continuous), and we will see in future sections that differentiabil
ity is a valuable property for machine learning.
 Instead, we usually make our function 
\begin_inset Formula $f_{\theta}$
\end_inset

 output a vector of the number of categories, which represents the probability
 of being a member of each category (a scalar between 
\begin_inset Formula $0$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

).
 We use the loss called 
\shape italic
cross entropy 
\begin_inset Formula $l\left(f_{\theta}\left(x\right),y\right)=-\log\left(\left(f_{\theta}\left(x\right)\right)_{y}\right)$
\end_inset


\shape default
.
 This will push the probability of the correct category toward 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Subsection
Empirical risk and bias/variance tradeoff
\end_layout

\begin_layout Standard
In practice, we often do not have access to a data generating function 
\begin_inset Formula $p$
\end_inset

, but instead we have a limited number of samples from it.
 This dataset of examples gives us an estimate of the true risk, by replacing
 the expectation with a finite sum, called the empirical risk 
\begin_inset Formula $R$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\begin{array}{ccccc}
\mathcal{R}\left(\theta,p\right) & \approx & R\left(\theta,\mathcal{D}\right) & = & \frac{1}{n}\sum_{x\in\mathcal{D}}l\left(f_{\theta}\left(x\right)\right)\end{array}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $n$
\end_inset

 is the total number of examples in 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
\end_layout

\begin_layout Standard
For random values of the parameters 
\begin_inset Formula $\theta$
\end_inset

, the empirical risk and the true risk will have similar values.
 But this is not the case when the parameters have been tuned so that the
 empirical risk is minimum.
 In the extreme case, consider a model that has memorized all examples of
 the training set by heart.
 In order to make a prediction for a new example, this example will seek
 the closest example in 
\begin_inset Formula $\mathcal{D}$
\end_inset

, in term of the euclidean distance, and output the exact same answer than
 this closest example.
 This model is called a 1-nearest neighbour regressor or classifier regarding
 the considered task.
 In this case the empirical risk is 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Standard
A model with too much expressivity, or 
\shape italic
variance
\shape default
, will be able to learn all examples in the training set by heart without
 having the ability to generalize on new examples, which is called overfitting.
 A model with not enough expressivity will not be able to perform well even
 on the training set, but in the meantime it will have a similar performance
 on the true data generating distribution.
 We say that there is a 
\shape italic
bias
\shape default
 toward a family of model.
 The bias variance trade-off consists in selecting a model that has sufficient
 expressivity to have a good performance on the train set, while not having
 too much expressivity so that it will not overfit, and still have good
 performance on the true data generating distribution.
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Standard
A way of combatting overfitting is to 
\end_layout

\begin_layout Standard
Data augmentation is another mean of combatting overfitting.
 We can use the knowledge that we have of our dataset to create new examples.
 For example for a classification task of images, we know from our experience
 of the world that rotating or translating an image will not change its
 content.
 We can thus artificially augment our training set by including rotated
 and translated versions of the same images.
\end_layout

\begin_layout Itemize
overfitting
\end_layout

\begin_layout Itemize
patience
\end_layout

\begin_layout Itemize
l2 regul
\end_layout

\begin_layout Itemize
data augmentation
\end_layout

\begin_layout Section
Neural networks
\end_layout

\begin_layout Itemize
stack layers that produce new representations of the incoming data
\end_layout

\begin_layout Itemize
standard architecture : linear transformation / non-linearity elementwise
\end_layout

\begin_layout Standard
Neural networks have empirically proven very powerful at solving complex
 tasks.
 Along with the availability of easy to use frameworks to build neural networks
 and learn from data, has developed new interests from industry to integrate
 artificial intelligence inspired techniques in more and more products.
 The first commercial successes date back to 1998 ???? when AT&T developed
 an automated system to read handwritten digits on bank checks, using convolutio
nal neural networks.
 Recent successes include advances in machine translation, image and voice
 recognition, close-to-realistic image generation.
 They have applications in online services integrated in smartphones, but
 also enable the invention of new automated systems that will benefit more
 traditional industries, (energy, agriculture, arts, ..)
\end_layout

\begin_layout Section
Common types of neural networks
\end_layout

\begin_layout Subsection
Multilayer perceptron
\end_layout

\begin_layout Standard
We now define the simplest neural network structure called the perceptron
 
\begin_inset CommandInset citation
LatexCommand cite
key "rosenblatt1961principles"

\end_inset

.
 From an input data vector 
\begin_inset Formula $x$
\end_inset

, it creates a prediction 
\begin_inset Formula $y$
\end_inset

 using the relation 
\begin_inset Formula $y\left(x\right)=f\left(\left\langle w,x\right\rangle +b\right)$
\end_inset

.
 
\begin_inset Formula $w$
\end_inset

 is called the weight vector, and 
\begin_inset Formula $b$
\end_inset

 is the bias.
 
\begin_inset Formula $f$
\end_inset

 is a function, and is sometimes called the nonlinearity as it allows the
 function 
\begin_inset Formula $y$
\end_inset

 to be different than just a linear function of its input 
\begin_inset Formula $x$
\end_inset

.
 From a trained perceptron, we take a decision for an example 
\begin_inset Formula $x$
\end_inset

 by comparing the value of the corresponding 
\begin_inset Formula $y$
\end_inset

 using a threshold value.
 The perceptrons were implemented before the invention of modern computers,
 as complex electronic circuits.
 The weights were encoded in hardware potentiometers and trained using an
 error-propagating process.
 Remarkably, these complex pieces of machinery were capable of obtaining
 good results for the task of recognizing simple shape images.
\end_layout

\begin_layout Standard
These perceptrons were designed to approximately replicate the computations
 made by a biological neuron.
 Each neuron gets input data from several other neurons, consisting in voltage
 spikes.
 The rate at which these spikes occur can be intepreted as whether a neuron
 is excited or not.
 Each neuron has different sensibilities regarding how it will react to
 an increase in spike rate from other neurons, this sensibility being mimicked
 by the weights in artificial neural networks.
\end_layout

\begin_layout Standard
This single perceptron is extended in a more complex model called the multilayer
 perceptron.
 It consists in alternatively stacking layers of linear transformation and
 nonlinearity, using a vectorized generalization of the perceptron: 
\begin_inset Formula $y\left(x\right)=f\left(\underbrace{Wx+b}_{\text{linear transformation}}\right)$
\end_inset

.
 
\begin_inset Formula $W$
\end_inset

 is now a weight matrix, and 
\begin_inset Formula $b$
\end_inset

 a bias vector.
 
\begin_inset Formula $f$
\end_inset

 is an elementwise function.
 We stack these transformations to get more complex functions.
 An example for 2 layers gives a function 
\begin_inset Formula $y\left(x\right)=f_{2}\left(W_{2}f_{1}\left(W_{1}x+b_{1}\right)+b_{2}\right)$
\end_inset

.
 The intermediate values obtained at each layer 
\begin_inset Formula $f_{1}\left(W_{1}x+b_{1}\right)$
\end_inset

 are called the hidden representations as they are new representations the
 the same input data, but encoded in a different way.
 A trained neural network will create representations that are better suited
 for its task.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/mlp.pdf
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
A multilayer perceptron consists in alternatively stacking layers of a linear
 transformation and a nonlinearity
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Recurrent neural networks
\end_layout

\begin_layout Subsection
Convolutions
\end_layout

\begin_layout Subsection
Autoencoders
\begin_inset CommandInset label
LatexCommand label
name "subsec:Autoencoders"

\end_inset


\end_layout

\begin_layout Subsection
Residual networks
\end_layout

\begin_layout Section
More elaborated cost functions
\end_layout

\begin_layout Standard
We can often associate a task and its corresponding loss function: regression
 with the quadratic error loss, and classification with the cross entropy
 loss 
\begin_inset CommandInset ref
LatexCommand ref
reference "cost-function"

\end_inset

.
 Some more recent advances in neural networks make use of more complex cost
 functions.
\end_layout

\begin_layout Standard
Neural art 
\begin_inset CommandInset citation
LatexCommand cite
key "gatys2015neural"

\end_inset

 and its feed-forward extension 
\begin_inset CommandInset citation
LatexCommand cite
key "ulyanov2016texture"

\end_inset

 tackle the task of generating artwork images from a real world picture,
 that mimick the style of a given painting.
 To this end, they create a cost function that measures how a generated
 image resembles both the picture and the painting:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\text{total}}\left(p,a,x\right) & = & \alpha\mathcal{L}_{\text{content}}\left(p,x\right)+\beta\mathcal{L}_{\text{style}}\left(a,x\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $p$
\end_inset

 is the picture, 
\begin_inset Formula $a$
\end_inset

 is the artwork that we want to extract the style, and 
\begin_inset Formula $x$
\end_inset

 is any image.
 
\begin_inset Formula $\mathcal{L}_{\text{content}}$
\end_inset

 is a loss function that measures how close 
\begin_inset Formula $x$
\end_inset

 is from 
\begin_inset Formula $p$
\end_inset

 in terms of contents, and 
\begin_inset Formula $\mathcal{L}_{\text{style}}$
\end_inset

 is a loss function that measures a distance from 
\begin_inset Formula $a$
\end_inset

 to 
\begin_inset Formula $x$
\end_inset

 in terms of artistic style.
 By minimizing 
\begin_inset Formula $\mathcal{L}_{\text{total}}\left(p,a,x\right)$
\end_inset

 with respect to 
\begin_inset Formula $x$
\end_inset

 for given 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $a$
\end_inset

, we obtain the desired image in 
\begin_inset Formula $x$
\end_inset

.
 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are scalar values that control the influence of each part of the loss.
 In the original paper 
\begin_inset CommandInset citation
LatexCommand cite
key "gatys2015neural"

\end_inset

 we start from a randomly initialized 
\begin_inset Formula $x$
\end_inset

 and we perform gradient descent on each pixel of 
\begin_inset Formula $x$
\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "ulyanov2016texture"

\end_inset

 we use a convolutional neural network to generate 
\begin_inset Formula $x$
\end_inset

, which takes the picture as input, and outputs the desired stylized image.
 This network is trained using 
\begin_inset Formula $\mathcal{L}_{\text{total}}$
\end_inset

.
 It has the main advantage of being very fast as generating new images once
 it has be trained on a specific artwork.
 We will explain how to create 
\begin_inset Formula $\mathcal{L}_{\text{style}}$
\end_inset

 and 
\begin_inset Formula $\mathcal{L}_{\text{content}}$
\end_inset

 in details in 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:ff-texture-gen"

\end_inset

.
\end_layout

\begin_layout Standard
Another family of cost functions that becomes more and more popular is that
 of the discriminators in Generative Adversarial Networks 
\begin_inset CommandInset citation
LatexCommand cite
key "goodfellow2014generative"

\end_inset

, that can be thought of as learned cost functions.
 In this setup, 2 networks are trained one against each other : the generator
 part takes random noise and generate a sample that tries to fool the discrimina
tor, which is also a trained network that tries to classify whether its
 input is from a given data distribution, or if it is generated by the generator.
 Training these network is very unstable, and is the object of many research
 at the time of this writing.
 But provided that we successfully trained both parts, we get a generator
 that is able to generate new samples of complex data, such as realistic
 images.
\end_layout

\begin_layout Chapter
Optimization of neural networks
\end_layout

\begin_layout Section
Gradient descent and backpropagation
\begin_inset CommandInset label
LatexCommand label
name "sec:Gradient-descent-and"

\end_inset


\end_layout

\begin_layout Standard
Once we have chosen a model, and supposing that this model capable of solving
 a given task, the main challenge is now to learn the parameters of the
 model from the data.
 Some simple models have closed form solutions, this is for example the
 case for a linear model and a regression task.
 For more complex models such as neural networks, we can not derive a simple
 formula for getting the values of all parameters given a dataset.
 In this case, we start from an initialized network and iterate updates
 for our parameters until we get the expected results.
 To this end, we must find an efficient way of getting an update 
\begin_inset Formula $\Delta\theta$
\end_inset

 of our parameters 
\begin_inset Formula $\theta$
\end_inset

.
 Considering that we aim at finding the minimum of the empirical risk, such
 an update is given by the steepest direction of descent of the empirical
 risk, given by minus the gradient of the empirical risk, with respect to
 the parameters, denoted by 
\begin_inset Formula $\nabla_{\theta}R$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\nabla_{\theta}R & = & \frac{1}{n}\sum_{i}\nabla_{\theta}l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Once we have a direction, we must choose how far to move in this direction.
 Several line search algorithms have been developed, but they require evaluating
 our objective several times, which can be costly for deep networks or big
 datasets.
 We will stick to a simple fixed learning rate, so that each iteration becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta & \leftarrow & -\lambda\nabla_{\theta}R
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
no closed form solution
\end_layout

\begin_layout Itemize
gradient descent
\end_layout

\begin_layout Itemize
backpropagation
\end_layout

\begin_layout Standard
Starting from a model, and supposing this model capable of solving a given
 task, a main challenge of machine learning remains to learn from the data,
 to find the correct parameters that will make the model useful.
 Certain tasks have closed-form solutions, such as finding the correct weights
 and biases for a linear model and a regression task.
 This task can be formulated as finding weight matrix 
\begin_inset Formula $W$
\end_inset

 and bias vector 
\begin_inset Formula $b$
\end_inset

 corresponding to a dataset 
\begin_inset Formula $\mathcal{D}$
\end_inset

 of pairs of vectors 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $y_{i}$
\end_inset

 such that 
\begin_inset Formula $R_{\mathcal{D}}=\sum_{i}\left\Vert Wx_{i}+b-y_{i}\right\Vert ^{2}$
\end_inset

 is minimized.
 If we stack our training examples in matrices 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 with each row representing an example and its target value, this reduces
 to the following linear algebra problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
W,b & = & \text{argmin}_{W,b}\left\Vert XW^{T}+b-Y\right\Vert ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In contrary, for neural networks, as for other models, we can not derive
 a closed-form solution [TODO] explication (plusieurs minima globaux).
 [WHO] proposed to use gradient descent to solve such tasks.
 Instead of finding the target solution directly, we will update our parameters
 
\begin_inset Formula $\theta$
\end_inset

 by a small increment 
\begin_inset Formula $\theta\leftarrow\theta+\Delta\theta$
\end_inset

 that decreases the value of the empirical risk.
 It can be any arbitrary value, as long as it make the empirical risk decrease.
 Such a value is given by the inverse of the derivative multiplied by a
 small scalar value: 
\begin_inset Formula $\Delta\theta=-\lambda\frac{\partial R}{\partial\theta}$
\end_inset

.
 TODO theoretical convergence proofs for gradient descent: is guaranteed
 to converge in 
\begin_inset Formula $O\left(\frac{1}{t}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
What also makes gradient descent suitable for neural networks, is that it
 can conveniently be computed using forward and backward propagation of
 computations that take advantage of the chain-rule for derivative, and
 of the sequential structure of neural networks computations.
\end_layout

\begin_layout Standard
[TODO figure fprop backprop]
\end_layout

\begin_layout Standard
automatic differentiation tool
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/backprop.pdf
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Forward (in black) and backward (in red) propagation of the intermediate
 results of the process of computing the output of the network and the gradient
 corresponding to this output and the desired "true" output.
 The green arrows represent the computation of the gradients with respect
 to the parameters, given the gradients with respect to the pre-activations.
\begin_inset CommandInset label
LatexCommand label
name "fprop-bprop"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Stochastic gradient descent
\begin_inset CommandInset label
LatexCommand label
name "sec:Stochastic-gradient-descent"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "bottou2010large"

\end_inset


\end_layout

\begin_layout Standard
While backpropagation is an efficient way of computing the exact gradient
 of the empirical risk with respect to the parameters, in practice we are
 not required to use its exact value, but rather we can use an estimate
 of the gradient, as long as this estimate will make our objective decrease.
 It is worth recalling at this point that even the true gradient of the
 empirical risk is different of the real gradient we would like to follow,
 which is the gradient of the true risk.
\end_layout

\begin_layout Standard
A good estimate is obtained by computing the gradient using a smaller subset
 of our dataset, called a mini-batch.
 Replacing the gradient descent update with this estimate is called 
\series bold
stochastic gradient descent
\series default
 (SGD).
 The main benefit of using SGD instead of full gradient descent is that
 we can reduce the memory required to compute the gradient.
 The memory required to backpropagate the intermediate gradients (red arrows
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

) is proportional to the size of the minibatch.
 As datasets become bigger and bigger, while GPUs used for accelerating
 the computations have limited memory, adjusting the batch size is a good
 way of making an experiment fit on a selected computer.
\end_layout

\begin_layout Section
Hyperparameters
\end_layout

\begin_layout Standard
In the preceding sections we have introduced the learning rate 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Gradient-descent-and"

\end_inset

 and the minibatch size 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Stochastic-gradient-descent"

\end_inset

.
 These values are called hyperparameters, which is another kind of parametrizati
on of our learning procedure.
 Hyperparameters also include the structure of our model, such as the number
 of hidden layers and hidden units, the number of training iterations, the
 coefficients of the regularization terms.
 We do not find the optimal value for the hyperparameters using gradient
 descent, but instead we tune it by running several time the same experiment
 with different hyperparameter values, and control 
\end_layout

\begin_layout Standard
A difficulty in comparing optimization algorithms resides in the fact that
 there performances can change drastically for different values of the same
 hyperparameters.
 Optimization papers sometimes mention heuristics that they experimentally
 found provide with a sensible value for some hyperparmeters.
 But to overcome this difficulty and provide 
\begin_inset Quotes eld
\end_inset

fair
\begin_inset Quotes erd
\end_inset

 benchmarks, we usually tune the values of the hyperparameters by trying
 several sets of values.
 Hyperparameters tuning is a research field on its own, so we will just
 introduce 3 methods and motivate our use of a technique we call biased
 random search.
\end_layout

\begin_layout Standard
The most simple hyperparamater tuning procedure, called 
\series bold
grid search
\series default
, consists in selecting values at fixed length intervals, or using a logarithmic
 scale.
 A simple example would be a training procedure involving only one hyperparamete
r: the learning rate.
 We can launch several experiments for all values in 
\begin_inset Formula $\left\{ 10^{-3},10^{-2},10^{-1},1\right\} $
\end_inset

 for a fixed number of updates and select the value for which we obtained
 the best value for our target criteria such as the validation loss.
 When generalizing to several hyperparameters, we have to select all combination
s of values, which make our search space grow exponentially, and similarly
 for the number of experiments we will have to run.
\end_layout

\begin_layout Standard
A first extension to grid search replaces the fixed length intervals by
 random samples in our search space.
 It is called 
\series bold
random search
\series default
.
 Its main advantage over grid search shows up when any hyperparameters has
 no important effect on the learning algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "bergstra2012random"

\end_inset

.
 It will explore more different values for the other hyperparameters.
\end_layout

\begin_layout Standard
In the rest of this work, we will use an extension of random search that
 we call 
\series bold
biased random search
\series default
.
 During the hyperparameter tuning procedure, we create a model of our cost
 landscape in the space of hyperparameters.
 As the number of experiments grows, the cost landscape is refined.
 We use this estimated cost landscape to bias our random search, so that
 regions of the hyperparameter space that are expected to provide a better
 result will have higher probability of being explored.
 In practice, we use a simple 1-nearest neighbor regressor 
\begin_inset CommandInset citation
LatexCommand cite
key "altman1992introduction"

\end_inset

 to model the cost landscape.
 Using the estimated value of the criteria 
\begin_inset Formula $c_{estimate}$
\end_inset

, we decide to keep the sampled value with probability 
\begin_inset Formula $p$
\end_inset

, or otherwise we reject the value and sample a new one, and so on until
 we get a value that is not rejected, which will be our next experiment.
 We can choose the value of 
\begin_inset Formula $p$
\end_inset

 using different heuristics, in practice we use 
\begin_inset Formula $p=\frac{c_{max}-c_{estimate}}{c_{max}-c_{min}}$
\end_inset

 (in this notation, the criteria needs to be minimized).
 This value for 
\begin_inset Formula $p$
\end_inset

 will almost surely reject values that are close to the worst experiments,
 and almost surely accept values that are close to the best experiments.
\end_layout

\begin_layout Standard
As an illustration, we ran all 3 methods on the task described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:autoencoder-benchmark"

\end_inset

, using standard stochastic gradient descent with fixed minibatch size,
 for a fixed number of parameter updates.
 We tune 2 hyperparameters: the learning rate and the variance of the initial
 random weights, and plot the result in 
\begin_inset CommandInset ref
LatexCommand ref
reference "hptune-comparison"

\end_inset

.
 These plots show the interaction between 2 hyperparameters.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of hyperparameter tuning methods
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "hptune-comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
The learning rate
\end_layout

\begin_layout Standard
Amongst other hyperparameters, the learning rate of standard (stochastic)
 gradient descent plays a particular role which we will show in the following.
 We can rewrite the standard gradient descent result using a second order
 Taylor expansion of the empirical risk:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
R\left(\theta+\Delta\theta\right) & = & R\left(\theta\right)+\left(\nabla R\right)_{\theta}\Delta\theta+\frac{1}{2}\Delta\theta^{T}\mathbf{H}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert ^{3}\right)\\
 & \approx & R\left(\theta\right)+\left(\nabla R\right)_{\theta}\Delta\theta+\frac{1}{2}\Delta\theta^{T}\mathbf{H}\Delta\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
learning rate using the hessian + mathematical results
\end_layout

\begin_layout Section
Limits of (stochastic) gradient descent and some directions to overcome
 them
\end_layout

\begin_layout Itemize
gradient magnitudes (exploding/vanishing gradient) between layers
\end_layout

\begin_layout Itemize
gradient magnitures between examples ?
\end_layout

\begin_layout Itemize
follow directions of steepest descent, instable in valleys
\end_layout

\begin_layout Itemize
initialization of weights 
\begin_inset CommandInset citation
LatexCommand cite
key "glorot2010understanding"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "he2015delving"

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "initialization-of-weights"

\end_inset


\end_layout

\begin_layout Section
A standard benchmark: Autoencoding written digits
\begin_inset CommandInset label
LatexCommand label
name "sec:autoencoder-benchmark"

\end_inset


\end_layout

\begin_layout Itemize
HP plots
\end_layout

\begin_layout Standard
We now describe the main benchmark that we will be using in the rest of
 this document.
 The dataset MNIST 
\begin_inset CommandInset citation
LatexCommand cite
key "lecun2010mnist"

\end_inset

 is composed of 60.000 
\begin_inset Formula $28\times28$
\end_inset

 grayscale images of handwritten digits, and the corresponding value of
 the digit that is represented in the image.
 For this benchmark, we use an autoencoder 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Autoencoders"

\end_inset

 with layer sizes 
\begin_inset Formula $\left\{ 784,1000,500,250,30,250,500,1000,784\right\} $
\end_inset

.
 The autoencoder encodes the input image into a vector of size 
\begin_inset Formula $30$
\end_inset

, and then decodes it to reconstruct the original image.
 We use the quadratic error 
\begin_inset Formula $l\left(f\left(x\right),y\right)=\left\Vert f\left(x\right)-y\right\Vert _{2}^{2}$
\end_inset

.
 The benchmark consists in minimizing the empirical risk over the train
 set after a fixed time on the same architecture.
\end_layout

\begin_layout Standard
This benchmark has a long history in the neural network optimization litterature
 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006reducing,martens2010deep,martens2015optimizing,desjardins2015natural"

\end_inset

.
 To assess the performance of an algorithm, we can use 2 metrics: the empirical
 risk after a given number of iterations of the algorithm, and the empirical
 risk after a fixed elapsed time for a given computer.
 In real world tasks, the latter is more useful.
 It gives a better understanding of the trade-off between a more complex
 update that takes longer to compute and gives a better improvement, and
 a fast update that gives a small improvement, but that can be iterated
 several times in the meantime.
\end_layout

\begin_layout Standard
The limits of the benchmark are many.
 In particular the fact that the state of the art papers in computer vision
 do not use MLPs and sigmoid activations but rather variants of mixed convolutio
nal networks and residual connections, and variants of ReLU activations.
 Another limit is in the use of the quadratic loss.
 Nonetheless, we still use this benchmark as it is used by several other
 papers which allows for a fair comparison, and because it is reasonably
 deep (8 layers) and wide (the biggest weight matrix has size 
\begin_inset Formula $1000\times784$
\end_inset

).
\end_layout

\begin_layout Chapter
Advanced optimization
\end_layout

\begin_layout Section
Gradient smoothing methods
\end_layout

\begin_layout Itemize
cost landscape with valleys and plateaus
\end_layout

\begin_layout Itemize
momentum + nesterov
\end_layout

\begin_layout Section
Batch normalization
\end_layout

\begin_layout Standard
Batch normalization 
\begin_inset CommandInset citation
LatexCommand cite
key "ioffe2015batch,ioffe2017batch"

\end_inset

 is a very popular way to make the learning procedure more stable in neural
 networks.
 It consists in a simple reparametrization that effectively normalize the
 activations using statistics (mean, variance) computed from a batch of
 examples, defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\text{BN}\left(x\right) & = & \frac{x-\mu}{\sqrt{\sigma^{2}+\epsilon}}
\end{eqnarray*}

\end_inset

with 
\begin_inset Formula $\mu=\mathbb{E}\left[x\right]$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}=\mathbb{E}\left[\left(x-\mu\right)^{2}\right]$
\end_inset

 the empirical mean and variance computed over a minibatch in 
\begin_inset CommandInset citation
LatexCommand cite
key "ioffe2015batch"

\end_inset

 or using a running average in 
\begin_inset CommandInset citation
LatexCommand cite
key "ioffe2017batch"

\end_inset

.
 A 
\begin_inset Quotes eld
\end_inset

batch-normalized
\begin_inset Quotes erd
\end_inset

 layer is defined by:
\begin_inset Formula 
\[
h_{n}(x)=f_{n}\left(\gamma_{n}\odot\text{BN}\left(W_{n}h_{n-1}\right)+\beta_{n}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
After reparametrization, we train the parameters 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 using a gradient based method such as stochastic gradient descent.
 The new gradient 
\begin_inset Quotes eld
\end_inset

goes through
\begin_inset Quotes erd
\end_inset

 the BN operation (mean and square root of the variance) so both the forward
 pass and the backward pass are modified.
 We usually initialize the values of 
\begin_inset Formula $\gamma$
\end_inset

 to be a vector of 
\begin_inset Formula $1$
\end_inset

 and for 
\begin_inset Formula $\beta$
\end_inset

 a vector of 
\begin_inset Formula $0$
\end_inset

.
 
\end_layout

\begin_layout Standard
A first effect of this reparametrization, as pointed by the paper, is that
 it reduces the 
\shape italic
covariate shift
\shape default
 of each unit.
 If we consider a single unit, we can observe an empirical distribution
 of its activation over a dataset.
 We have seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "initialization-of-weights"

\end_inset

 that carefully tuning the initial weights and biases so that each unit
 has a desired distribution can improve drastically the efficiency or the
 learning procedure.
 This initial distribution changes throughout the learning, and this is
 what is called the 
\shape italic
covariate shift
\shape default
.
 Batch normalization forces the distribution of activations to have zero
 mean and unit variance.
\end_layout

\begin_layout Standard
For some experiments, 
\shape italic
mean-only 
\shape default
batch normalization can also be used to improve the learning procedure.
 We will cover this question in more details in 
\begin_inset CommandInset ref
LatexCommand ref
reference "centering-trick"

\end_inset

.
\end_layout

\begin_layout Itemize
effect on the forward pass, and the gradient
\end_layout

\begin_layout Section
Second order methods
\end_layout

\begin_layout Subsection
Newton steps
\end_layout

\begin_layout Standard
Second order methods refers to all optimization methods that make use of
 the second derivative or Hessian matrix of the function to be minimized.
 It follows from the Taylor series decomposition of the function to be minimized
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
f\left(x+\Delta x\right) & = & f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x+o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\left(\nabla^{2}f\right)_{x}$
\end_inset

 is the Hessian matrix, computed in 
\begin_inset Formula $x$
\end_inset

.
 We use the little-o notation 
\begin_inset Formula $o$
\end_inset

 that represents an unknown function with the only property that 
\begin_inset Formula $\lim_{x\rightarrow0}\frac{o\left(x\right)}{x}=0$
\end_inset

.
 By ignoring higher order terms (
\begin_inset Formula $o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)=0$
\end_inset

) we have a quadratic approximation for 
\begin_inset Formula $f$
\end_inset

.
 Using this approximation in a minimization problem, we get a closed form
 solution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x^{*} & = & \text{argmin}_{\Delta x}f\left(x+\Delta x\right)\\
 & \approx & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This expression is solved by taking the derivative with respect to 
\begin_inset Formula $\Delta x$
\end_inset

, and setting it to zero in order to obtain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left(\nabla^{2}f\right)_{x}\Delta x & = & -\left(\nabla f\right)_{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
If we assume that 
\begin_inset Formula $f$
\end_inset

 has a minimum in 
\begin_inset Formula $x^{*}$
\end_inset

, then the Hessian will be positive definite in 
\begin_inset Formula $x^{*}$
\end_inset

, and under the supplementary assumption that the Hessian is continuous,
 it will also be positive definite in a neighborhood of 
\begin_inset Formula $x^{*}$
\end_inset

.
 In this case, it is invertible and we get the solution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\Delta x & = & -\left(\nabla^{2}f\right)_{x}^{-1}\left(\nabla f\right)_{x}\label{eq:newton-step}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
This update 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:newton-step"

\end_inset

 is called the 
\series bold
Newton step
\series default
.
 By making several iterations of Newton, we will converge to a minimum.
\end_layout

\begin_layout Standard
The main difficulty of this algorithm is that it does not scale well when
 applied to neural network optimization.
 The limitations come from the following aspects:
\end_layout

\begin_layout Enumerate

\shape italic
Getting the value of the Hessian matrix
\shape default
: Using an automatic differentiation software, we can get an expression
 for the Hessian, by differentiating the symbolic expression of the gradient.
 But unlike the computation of the gradient, the graph produced to compute
 the Hessian will have much more nodes.
 We will explore this question in more details in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

.
\end_layout

\begin_layout Enumerate

\shape italic
Storing the Hessian matrix
\shape default
: The Hessian matrix is a square matrix with size 
\begin_inset Formula $n_{\text{parameters}}\times n_{parameters}$
\end_inset

.
 As the number of parameters grows, which is the case when building deep
 networks, the memory required to store the Hessian will grow in 
\begin_inset Formula $O\left(n^{2}\right)$
\end_inset

.
 We will present an approximation of the Hessian that saves memory in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Block-diagonal-Hessian"

\end_inset

.
\end_layout

\begin_layout Enumerate

\shape italic
Inverting the Hessian matrix
\shape default
: Inverting the Hessian matrix is also costly as it grows in 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

 with the size of the matrix.
 We present an algorithm that make use of second order information but without
 the need to invert the Hessian matrix known as 
\shape italic
Hessian Free
\shape default
 
\begin_inset CommandInset citation
LatexCommand cite
key "martens2010deep"

\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Hessian-free"

\end_inset

.
\end_layout

\begin_layout Subsection
Validity of Newton for non quadratic functions and Tikhonov regularization
\end_layout

\begin_layout Standard
In the previous section, we considered that our function was approximated
 by its second order Taylor series decomposition.
 While this is true in a neighborhood of 
\begin_inset Formula $x$
\end_inset

, the approximation becomes less precise as we move away from 
\begin_inset Formula $x$
\end_inset

.
 In particular this is the case when the Newton step provide big updates,
 that is when the Hessian has at least one small eigenvalue.
 The corresponding direction pointed by this eigenvalue will have a low
 curvature using the quadratic approximation, so the minimum following this
 direction will be far away.
 But the actual function that we are minimizing is not a quadratic, and
 the terms hidden in 
\begin_inset Formula $o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)$
\end_inset

 will become preponderant for bigger values of 
\begin_inset Formula $\Delta x$
\end_inset

.
\end_layout

\begin_layout Standard
To counter this undesirable effect, we simply add a regularization term
 that penalizes bigger values of 
\begin_inset Formula $\Delta x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x^{*} & = & \text{argmin}_{\Delta x}f\left(x+\Delta x\right)\\
 & \approx & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x+\frac{\epsilon}{2}\left\Vert \Delta x\right\Vert _{2}^{2}\\
 & = & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\left(\nabla^{2}f\right)_{x}+\epsilon\mathbf{I}\right)\Delta x
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This gives the Tikhonov regularized version of the Newton step:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x & = & -\left(\left(\nabla^{2}f\right)_{x}+\epsilon\mathbf{I}\right)^{-1}\left(\nabla f\right)_{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This new hyperparameter 
\begin_inset Formula $\epsilon$
\end_inset

 controls the size of the steps, and thus plays a very similar role than
 the learning rate.
\end_layout

\begin_layout Subsection
Gauss-Newton approximation of the Hessian
\begin_inset CommandInset label
LatexCommand label
name "subsec:gauss-newton"

\end_inset


\end_layout

\begin_layout Standard
In the case of neural network optimization, the Hessian matrix we need to
 evaluate is the second derivative of the empirical risk, with respect to
 the parameters.
 A first remark that we can make, is that it is also composed of a sum of
 second order derivatives, to be computed at each example of the dataset:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{H} & = & \frac{\partial^{2}R}{\partial\theta^{2}}\\
 & = & \frac{\partial^{2}}{\partial\theta^{2}}\left\{ \frac{1}{n}\sum_{i}l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} \\
 & = & \frac{1}{n}\sum_{i}\frac{\partial^{2}}{\partial\theta^{2}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
By making use of the chain rule we can also give an expression for the second
 derivative of the loss, for a single example.
 We start with the first derivative:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\}  & = & \mathbf{J}\left(x_{i},\theta\right)^{T}\left(\frac{\partial}{\partial f}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} \right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathbf{J}$
\end_inset

 is the jacobian of the output of the network 
\begin_inset Formula $f$
\end_inset

 with respect to the parameters 
\begin_inset Formula $\theta$
\end_inset

.
 In this notation we made the dependance in 
\begin_inset Formula $\theta$
\end_inset

 of both parts of the product explicit.
 Note that both parts also take different values for each examples 
\begin_inset Formula $x_{i}$
\end_inset

.
 We now derive this expression once more to obtain the Hessian:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}}{\partial\theta^{2}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\}  & = & \underbrace{\mathbf{J}\left(x_{i},\theta\right)^{T}\frac{\partial^{2}}{\partial f^{2}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} \mathbf{J}\left(x_{i},\theta\right)}_{G_{f}\left(x_{i},\theta\right)}\\
 &  & +\sum_{j}\left(\nabla^{2}f_{j}\left(x_{i},\theta\right)\right)\frac{\partial}{\partial f_{j}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $G_{f}\left(x_{i},\theta\right)$
\end_inset

 is called the Gauss-Newton approximation of the Hessian.
 The remainder is proportional to 
\begin_inset Formula $\frac{\partial}{\partial f_{j}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} $
\end_inset

.
 As we get closer to the the optimum, this part will go toward 
\begin_inset Formula $0$
\end_inset

 as it is a first derivative, so the approximation will get more precise.
 At a minimum for 
\begin_inset Formula $l\left(f\left(x_{i},\theta\right),y_{i}\right)$
\end_inset

, we will have 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta^{2}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\} =G_{f}\left(x_{i},\theta\right)$
\end_inset

 so it is a reasonable approximation to use in practice.
 Note that a minimum for the empirical risk 
\begin_inset Formula $R\left(\theta\right)$
\end_inset

 will not necessarily be a minimum for each example 
\begin_inset Formula $l\left(f\left(x_{i},\theta\right),y_{i}\right)$
\end_inset

, especially if the capacity of the neural network is not sufficient to
 model the data distribution.
\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $G_{f}\left(x_{i},\theta\right)$
\end_inset

 present a much more convenient expression for common loss functions, as
 the second derivative of the loss simplifies (
\begin_inset CommandInset ref
LatexCommand ref
reference "gn-loss"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="right" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="right" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Loss function
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $G_{f}\left(x_{i},\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
quadratic error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{J}\left(x_{i},\theta\right)^{T}\mathbf{J}\left(x_{i},\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for binary decision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{J}\left(x_{i},\theta\right)^{T}\text{diag}\left(\frac{y_{i}}{\left(f(x_{i},\theta)\right)^{2}}+\frac{1-y_{i}}{\left(1-f(x_{i},\theta)\right)^{2}}\right)\mathbf{J}\left(x_{i},\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for multiclass classification
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "gn-loss"

\end_inset

Expressions for the Gauss-Newton approximation of the Hessian, for a single
 example 
\begin_inset Formula $x_{i}$
\end_inset

.
 For the cross entropy, all operations (division, squarred value) are elementwis
e, and the 
\begin_inset Formula $\text{diag}$
\end_inset

 function transforms a vector into a diagonal matrix with the vector values
 on its diagonal.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We finally give an expression for the Gauss-Newton approximation of the
 Hessian for the empirical risk:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
G_{f}\left(\theta\right) & = & \frac{1}{n}\sum_{i}\mathbf{J}\left(x_{i},\theta\right)^{T}D\left(f\left(x_{i},\theta\right),y_{i}\right)\mathbf{J}\left(x_{i},\theta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We will show in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:2ndorder-newpers"

\end_inset

 that this matrix can be factorized to design optimization algorithms adapted
 to the particular structure of neural networks.
\end_layout

\begin_layout Subsection
Hessian free
\begin_inset CommandInset label
LatexCommand label
name "subsec:Hessian-free"

\end_inset


\end_layout

\begin_layout Subsection
Block diagonal Hessian 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Block-diagonal-Hessian"

\end_inset


\end_layout

\begin_layout Standard
Apart from the issue of computing a value for the hessian matrix, a main
 limit is that we need to invert it.
 The hessian matrix has size 
\begin_inset Formula $n_{\text{parameters}}\times n_{\text{parameters}}$
\end_inset

, and the procedure used for numerically inverting a square matrix requires
 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

 operations so it rapidly becomes unfeasible for deep networks.
 A first approximation we make is by ignoring the interactions between the
 parameters of different layers.
 We make the hessian block diagonal, each block having the size of the number
 of parameters of the corresponding layer.
 An interesting property of block diagonal matrices is that we get the inverse
 by inverting every smaller block separately:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left(\nabla^{2}f\right)^{-1} & \approx & \left(\begin{array}{cccc}
\mathbf{H}_{1} & 0 & \cdots & 0\\
0 & \mathbf{H}_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \mathbf{H}_{n}
\end{array}\right)^{-1}=\left(\begin{array}{cccc}
\mathbf{H}_{1}^{-1} & 0 & \cdots & 0\\
0 & \mathbf{H}_{2}^{-1} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \mathbf{H}_{n}^{-1}
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It also makes the implementation easier, as we can treat each block 
\begin_inset Quotes eld
\end_inset

locally
\begin_inset Quotes erd
\end_inset

 in the network, and use its inverse to update the gradient direction for
 the corresponding block using 
\begin_inset Formula $\theta_{i}\leftarrow\theta_{i}-\lambda\mathbf{H}_{i}^{-1}\frac{\partial C}{\partial\theta_{i}}$
\end_inset

.
 We do not need to store a big 
\begin_inset Formula $n_{\text{parameters}}\times n_{\text{parameters}}$
\end_inset

 matrix.
\end_layout

\begin_layout Section
Direct gradient update
\end_layout

\begin_layout Standard
In this section, we present a new technique that makes use of updated gradient
 directions to account for interactions between layers.
\end_layout

\begin_layout Standard
A major inconvenient of first order methods is that they update every individual
 parameter without taking into account for other parameter updates.
 As an illustration, take a neural network composed of only 2 layers 
\begin_inset Formula $f\left(x\right)=f_{2}\left(W_{2}f_{1}\left(W_{1}x+b_{1}\right)+b_{2}\right)$
\end_inset

 and a loss function 
\begin_inset Formula $l\left(f\left(x\right),y\right)$
\end_inset

.
 Suppose that we have computed all gradients with respect to each parameters
 
\begin_inset Formula $\frac{\partial l}{\partial W_{1}}$
\end_inset

, 
\begin_inset Formula $\frac{\partial l}{\partial b_{1}}$
\end_inset

, 
\begin_inset Formula $\frac{\partial l}{\partial W_{2}}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial l}{\partial b_{2}}$
\end_inset

.
 If we were using standard gradient descent, we would update the value of
 the parameters, all at once to obtain a better prediction function.
 Now suppose that we only update the parameters of layer 2, using the standard
 update rule 
\begin_inset Formula $W_{2}\leftarrow W_{2}-\lambda\frac{\partial l}{\partial W_{2}}$
\end_inset

 and 
\begin_inset Formula $b_{2}\leftarrow b_{2}-\lambda\frac{\partial l}{\partial b_{2}}$
\end_inset

.
 From this point, we have a different function 
\begin_inset Formula $f\left(x\right)$
\end_inset

.
 We could either update the parameters of layer 1 using the gradient already
 computed, or find the new values for the gradients for layer 1 using the
 updated 
\begin_inset Formula $f$
\end_inset

.
 We will do the latter, by using the property that the gradient for the
 parameters of layer 1 is a function of the parameters of layer 2, which
 can be written:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial l}{\partial W_{1}}\left(W_{2},b_{2}\right) & \text{ and } & \frac{\partial l}{\partial b_{1}}\left(W_{2},b_{2}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In particular, using a first order Taylor decomposition:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial l}{\partial a_{1}}\left(W_{2}+\Delta W_{2},b_{2}+\Delta b_{2}\right) & = & \frac{\partial l}{\partial a_{1}}\left(W_{2},b_{2}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Second order: a new perspective
\begin_inset CommandInset label
LatexCommand label
name "sec:2ndorder-newpers"

\end_inset


\end_layout

\begin_layout Standard
In this section, we will show a convenient factorization of the Gauss-Newton
 approximation of the Hessian, that was first applied to the Fisher Information
 Matrix in the litterature 
\begin_inset CommandInset citation
LatexCommand cite
key "martens2015optimizing"

\end_inset

.
 To this end, we will use an operation called the Kronecker product that
 permits giving simple expressions for the GN matrix.
 For 2 matrices 
\begin_inset Formula $A$
\end_inset

 of size 
\begin_inset Formula $m\times n$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 of size 
\begin_inset Formula $p\times q$
\end_inset

 it produces a new matrix 
\begin_inset Formula $A\otimes B$
\end_inset

 of size 
\begin_inset Formula $mp\times nq$
\end_inset

 defined by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
A\otimes B & = & \left(\begin{array}{ccc}
a_{11}B & \cdots & a_{1n}B\\
\vdots & \ddots & \vdots\\
a_{m1}B & \cdots & a_{mn}B
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Its most interesting property in the context of neural networks is its relations
hip with the 
\begin_inset Formula $vec$
\end_inset

 operation, that 
\begin_inset Quotes eld
\end_inset

flattens
\begin_inset Quotes erd
\end_inset

 a matrix into a vector.
 It is of great use for 2nd order, because the weight matrices can be vectorized
 using 
\begin_inset Formula $vec$
\end_inset

, to give matrix expressions for the Hessian, which otherwise could not
 be written.
 We will make use of the property:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
vec\left(AXB\right) & = & \left(B^{T}\otimes A\right)vec\left(X\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Getting back to the expression for the Gauss-Newton matrix derived in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

, we now focus on a single layer defined by the linear transformation 
\begin_inset Formula $a=Wh+b$
\end_inset

 and the nonlinearity 
\begin_inset Formula $h'=f\left(a\right)$
\end_inset

.
 We start from the jacobian of the output of the network, with respect to
 the output of the linear transformation 
\begin_inset Formula $a$
\end_inset

, denoted by 
\begin_inset Formula $\mathbf{J}_{a}$
\end_inset

.
 From this jacobian computed by backpropagation, we can get the jacobian
 with respect to the parameters of the layer by making use of the chain
 rule 
\begin_inset Formula $\mathbf{J}_{\theta}=\mathbf{J}_{a}\mathbf{J}_{\theta}^{a}$
\end_inset

.
 We use the notation 
\begin_inset Formula $\mathbf{J}_{\theta}^{a}$
\end_inset

 for the jacobian of 
\begin_inset Formula $a$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

.
 In order to get an expression for this jacobian, we now make use of the
 
\begin_inset Formula $vec$
\end_inset

 operator to transform 
\begin_inset Formula $W$
\end_inset

 into a vector:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a & = & vec\left(a\right)\\
 & = & vec\left(Wh\right)+b\\
 & = & \left(h^{T}\otimes\mathbf{I}\right)vec\left(W\right)+b
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathbf{I}$
\end_inset

 is the identity, of the same size than 
\begin_inset Formula $a$
\end_inset

, that is the output size of the layer.
 We can now give an expression for 
\begin_inset Formula $\mathbf{J}_{vec\left(W\right)}^{a}$
\end_inset

 and 
\begin_inset Formula $\mathbf{J}_{b}^{a}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{J}_{b}^{a} & = & \mathbf{I}\\
\mathbf{J}_{vec\left(W\right)}^{a} & = & h^{T}\otimes\mathbf{I}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Or, if we stack 
\begin_inset Formula $vec\left(W\right)$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 in a vector 
\begin_inset Formula $\theta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{J}_{\theta}^{a} & = & \left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\mathbf{I}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And finally by the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mathbf{J}_{\theta} & = & \left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\mathbf{J}_{a}\label{eq:jaco_factorization}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
This jacobian is a first order measure of how much the output of the network
 will change if we change the values of the parameters of this layer, for
 a single example.
 Let us now recall the expression of the GN matrix 
\begin_inset Formula $G_{f}=\frac{1}{n}\sum_{i}\mathbf{J}_{\theta}\left(x_{i}\right)^{T}D\left(x_{i}\right)\mathbf{J}_{\theta}\left(x_{i}\right)$
\end_inset

 from 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

.
 We use the block diagonal approximation presented in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Block-diagonal-Hessian"

\end_inset

 and we focus on a single layer.
 We can rewrite this expression using the factorization 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:jaco_factorization"

\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
G_{f} & = & \frac{1}{n}\sum_{i}\left[\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)\otimes\mathbf{J}_{a_{i}}\right]^{T}D\left(x_{i}\right)\left[\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)\otimes\mathbf{J}_{a_{i}}\right]\\
 & = & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)^{T}\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)\otimes\left(\mathbf{J}_{a_{i}}^{T}D\left(x_{i}\right)\mathbf{J}_{a_{i}}\right)\\
 & = & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\left(\mathbf{J}_{a_{i}}^{T}D\left(x_{i}\right)\mathbf{J}_{a_{i}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This factorization is interesting because it separates the GN matrix into
 a contribution from the backpropagated jacobian, and a part that only uses
 the forward statistics.
 While these 2 contributions are clearly separated for a single example,
 it becomes less clear as we sum up for several examples.
 As we will show in the next section, similar approximations were exploited
 in KFAC 
\begin_inset CommandInset citation
LatexCommand cite
key "martens2015optimizing"

\end_inset

 and Natural Neural Networks 
\begin_inset CommandInset citation
LatexCommand cite
key "desjardins2015natural"

\end_inset

 to build efficient optimization algorithms.
\end_layout

\begin_layout Section
Methods using the FIM
\end_layout

\begin_layout Subsection
Fisher Information Matrix
\end_layout

\begin_layout Standard
The Fisher information matrix (FIM) is a tool well used in statistics.
 In the context of machine learning, and in particular deep learning, we
 use its inverse as a preconditioner for the gradient descent algorithm,
 similarly to the Newton algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:newton-step"

\end_inset

.
 In this section, we show how the FIM can be derived from the KL divergence
 and how we get a better 
\begin_inset Quotes eld
\end_inset

natural
\begin_inset Quotes erd
\end_inset

 gradient using this information.
 Let us first recall the definition of the KL divergence for 2 distributions
 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\text{KL}\left(p\parallel q\right) & = & \mathbb{E}_{p}\left[\log\left(\frac{p}{q}\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It is a non-negative quantity that looks like a measure of how much 
\begin_inset Formula $q$
\end_inset

 differs from 
\begin_inset Formula $p$
\end_inset

.
 In particular, 
\begin_inset Formula $\text{KL}\left(p\parallel q\right)=0$
\end_inset

 when 
\begin_inset Formula $p=q$
\end_inset

.
 Note that it is not symmetric, so it can not directly be used as a metric.
\end_layout

\begin_layout Standard
The idea of the natural gradient is to use the KL divergence as a regularizer
 when doing gradient descent.
 We will denote by 
\begin_inset Formula $p_{\theta}$
\end_inset

 a parametric model and 
\begin_inset Formula $\Delta\theta$
\end_inset

 a change in its parameter values.
 
\begin_inset Formula $\text{KL}\left(p_{\theta}\parallel p_{\theta+\Delta\theta}\right)$
\end_inset

 is used as our regularizer, so that each change 
\begin_inset Formula $\Delta\theta$
\end_inset

 gives the same change in the distribution space.
 Instead of using the full expression for 
\begin_inset Formula $\text{KL}\left(p_{\theta}\parallel p_{\theta+\Delta\theta}\right)$
\end_inset

 we will use its second order Taylor series around 
\begin_inset Formula $\theta$
\end_inset

 (for full derivation see for instance 
\begin_inset CommandInset citation
LatexCommand cite
key "pascanu2013revisiting"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\text{KL}\left(p_{\theta}\Vert p_{\theta+\Delta\theta}\right) & = & \Delta\theta^{T}\mathbf{F}\Delta\theta+o(\left\Vert \Delta\theta\right\Vert _{2}^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This expression exhibits the FIM which can now be used directly as a regularizer.
 Interestingly, even if the KL divergence is not symmetric, its second order
 approximation is, as we also have 
\begin_inset Formula $\text{KL}\left(p_{\theta+\Delta\theta}\Vert p_{\theta}\right)=\Delta\theta^{T}\mathbf{F}\Delta\theta+o(\left\Vert \Delta\theta\right\Vert _{2}^{2})$
\end_inset

 (note that we swapped the terms in the KL).
\end_layout

\begin_layout Subsection
(Natural) gradient descent
\end_layout

\begin_layout Standard
The usual gradient descent algorithm can be formulated as the minimization
 of the following expression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta\theta & = & \text{argmin}_{\Delta\theta}\left\{ \Delta\theta^{T}\nabla\mathcal{L}+\frac{\epsilon}{2}\left\Vert \Delta\theta\right\Vert ^{2}\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We use the notation 
\begin_inset Formula $\mathcal{L}$
\end_inset

 for the expectation of the loss function over the distribution of the data.
 This expression can be easily solved giving the expression 
\begin_inset Formula $\Delta\theta=-\frac{1}{\lambda}\nabla\mathcal{L}$
\end_inset

.
 The parameter 
\begin_inset Formula $\epsilon$
\end_inset

 is the inverse of the learning rate, and controls how much each parameter
 can change.
 We will now add a new regularizer using the FIM, and transform the minimization
 problem into:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta\theta & = & \text{argmin}_{\Delta\theta}\left\{ \Delta\theta^{T}\nabla\mathcal{L}+\frac{\epsilon}{2}\left\Vert \Delta\theta\right\Vert ^{2}+\frac{\lambda}{2}\Delta\theta^{T}\mathbf{F}\Delta\theta\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We now constrain our gradient step to be small in term of change of parameter
 values, and also to be small in term of how much the resulting distribution
 changes.
 This expression can be solved to give 
\begin_inset Formula $\Delta\theta=\frac{1}{\lambda}\left(\mathbf{F}+\epsilon\mathbf{I}\right)^{-1}\nabla\mathcal{L}$
\end_inset

.
 This expression also gives an insight for the role of 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\epsilon$
\end_inset

, which control 2 different but related quantities expressed by our constraints.
 This new update is called the natural gradient.
\end_layout

\begin_layout Subsection
Factorizing the FIM for neural networks
\end_layout

\begin_layout Subsubsection
An expression for the FIM using jacobians
\end_layout

\begin_layout Standard
Interestingly, for the usual distributions expressed by neural networks,
 the FIM takes the following simple form as shown by 
\begin_inset CommandInset citation
LatexCommand cite
key "pascanu2013revisiting"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{F} & = & \mathbb{E}_{x\sim q}\left[\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}^{T}D\left(\boldsymbol{y}\left(x\right)\right)\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The values for 
\begin_inset Formula $x$
\end_inset

 are drawn from the data generating distribution 
\begin_inset Formula $q$
\end_inset

.
 The notation 
\begin_inset Formula $\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}$
\end_inset

 is used for the jacobian of the output of the network (i.e.
 the probability expressed at a given 
\begin_inset Formula $x$
\end_inset

 : 
\begin_inset Formula $p\left(y\mid x\right)$
\end_inset

), with respect to the parameters.
 In other words, it measures how much the output of the network will change
 for a given 
\begin_inset Formula $x$
\end_inset

 if we change the parameters.
 
\begin_inset Formula $D$
\end_inset

 is a diagonal matrix with non negative diagonal terms, and depends of the
 cost function used.
 For the quadratic loss it is the identity.
\end_layout

\begin_layout Subsubsection
Approximations to the FIM
\end_layout

\begin_layout Standard
The FIM is difficult to compute because of its size (
\begin_inset Formula $n_{parameters}\times n_{parameters}$
\end_inset

) and because in general we do not have an expression for 
\begin_inset Formula $q$
\end_inset

 but only samples from a training dataset.
\end_layout

\begin_layout Standard
A first approximation that we can make is by ignoring the interactions between
 layers.
 In this case the FIM takes the form of a block diagonal matrix, where each
 block is a square matrix which has the size of the parameters of a layer.
 For a neural network with 
\begin_inset Formula $n_{layers}$
\end_inset

 layers this reduces the FIM into 
\begin_inset Formula $n_{layers}$
\end_inset

 smaller matrices.
 We will denote by 
\begin_inset Formula $\mathbf{F}_{i}$
\end_inset

 the block corresponding to layer 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Standard
A second common approximation we make in practice is to use the empirical
 FIM for a training dataset of 
\begin_inset Formula $n$
\end_inset

 examples 
\begin_inset Formula $x_{i}$
\end_inset

: 
\begin_inset Formula $\mathbf{F}=\frac{1}{n}\sum_{i}\boldsymbol{J}_{\boldsymbol{y}\left(x_{i}\right)}^{T}D\left(\boldsymbol{y}\left(x_{i}\right)\right)\boldsymbol{J}_{\boldsymbol{y}\left(x_{i}\right)}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Factorization of the 
\begin_inset Formula $\mathbf{F}_{i}$
\end_inset

s using the Kronecker product
\end_layout

\begin_layout Standard
In the rest of this note, we will restrict our analysis to a multilayer
 perceptron.
 Each layer is parametrized using a weight matrix 
\begin_inset Formula $W$
\end_inset

 of size 
\begin_inset Formula $\left(\text{out}\times\text{in}\right)$
\end_inset

 and a bias vector 
\begin_inset Formula $b$
\end_inset

 of size 
\begin_inset Formula $\left(\text{out}\right)$
\end_inset

.
 A layer consists in a linear transformation and a non-linearity 
\begin_inset Formula $f$
\end_inset

 to give the hidden representation of the next layer:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
h_{l+1}=f_{l}\left(a_{l}\right) & \text{with} & a_{l}=W_{l}h_{l}+b_{l}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We will focus on a single layer, and drop the subscript 
\begin_inset Formula $l$
\end_inset

.
 In the following, we will also focus on a single example in the expectation
 for the FIM.
 Each individual example has its own jacobian with respect to the parameter.
 In other words, this jacobian measures how the output of the network changes
 for this example, if you move the parameter values.
\end_layout

\begin_layout Standard
The jacobians for parameters 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are obtained using the chain rule for derivation: 
\begin_inset Formula $\boldsymbol{J}_{\boldsymbol{y}}^{W}=\boldsymbol{J}_{\boldsymbol{y}}^{a}\boldsymbol{J}_{a}^{W}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{J}_{\boldsymbol{y}}^{b}=\boldsymbol{J}_{\boldsymbol{y}}^{a}\boldsymbol{J}_{a}^{b}$
\end_inset

.
 The jacobian for the bias simplifies as 
\begin_inset Formula $\boldsymbol{J}_{a}^{b}=\mathbf{I}$
\end_inset

.
 For the weight matrix, it is a little bit more tricky.
 As 
\begin_inset Formula $W$
\end_inset

 is a matrix and not a vector, we can not express a jacobian matrix directly.
 We will have to make use of the Kronecker product and the 
\begin_inset Formula $\text{vec}$
\end_inset

 operator.
 We start from the linear relation 
\begin_inset Formula $a=Wh+b$
\end_inset

 and remark that 
\begin_inset Formula $a=\text{vec}\left(a\right)$
\end_inset

 since 
\begin_inset Formula $a$
\end_inset

 is a vector.
 We can now make use of the formula 
\begin_inset Formula $\text{vec}\left(AXB\right)=\left(B^{T}\otimes A\right)\text{vec}\left(X\right)$
\end_inset

 to obtain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a & = & \left(h^{T}\otimes\mathbf{I}_{out}\right)\text{vec}\left(W\right)\\
\boldsymbol{J}_{a}^{\text{vec}\left(W\right)} & = & \left(h^{T}\otimes\mathbf{I}_{out}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Putting everything together we get the jacobians:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\boldsymbol{J}_{\boldsymbol{y}}^{\text{vec}\left(W\right)} & = & \boldsymbol{J}_{\boldsymbol{y}}^{a}\left(h^{T}\otimes\mathbf{I}_{out}\right)\\
 & = & h^{T}\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}\\
\boldsymbol{J}_{\boldsymbol{y}}^{b} & = & \boldsymbol{J}_{\boldsymbol{y}}^{a}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Now imagine that we stack all parameters 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 in a vector 
\begin_inset Formula $\theta=\left(\text{vec}\left(W\right)_{1}\cdots\text{vec}\left(W\right)_{in\times out}b_{1}\cdots b_{out}\right)$
\end_inset

.
 We get the full jacobian by stacking the jacobians:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\boldsymbol{J}_{\boldsymbol{y}}^{\theta} & = & \left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From this expression we can finally express the FIM in a factorized form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mathbf{F} & = & \mathbb{E}\left[\left\{ \left(\begin{array}{c}
h\\
1
\end{array}\right)\otimes\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}\right\} D\left(\boldsymbol{y}\right)\left\{ \left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}\right\} \right]\nonumber \\
 & = & \mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} & h\\
h^{T} & 1
\end{array}\right)\otimes\left(\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)\right]\label{eq:factoredfim}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In this expression, remember that all 3 variables 
\begin_inset Formula $h$
\end_inset

, 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

 have a different value for each individual example.
 The FIM is the sum of the contributions of each example.
 The use of the Kronecker product permits splitting the contribution of
 each example in a term that involves the input of the layer 
\begin_inset Formula $\left(\begin{array}{cc}
hh^{T} & h\\
h^{T} & 1
\end{array}\right)$
\end_inset

 and a term that involves the jacobian received on the output of the linear
 transformation 
\begin_inset Formula $\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}$
\end_inset

.
\end_layout

\begin_layout Subsection
KFAC
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "martens2015optimizing"

\end_inset

 use a simplification of the FIM that drastically reduce the computation
 required for inverting it.
 Remember that even if we consider a block diagonal approximation of the
 FIM, each block still has size 
\begin_inset Formula $n_{parameters}\times n_{parameters}$
\end_inset

.
 We need to invert this block, and the operation of inverting a square matrix
 is 
\begin_inset Formula $O\left(n_{parameters}^{3}\right)$
\end_inset

.
 They propose the following approximation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{F} & \approx & \mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} & h\\
h^{T} & 1
\end{array}\right)\right]\otimes\mathbb{E}\left[\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The Kronecker product has the nice property that for 2 invertible square
 matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

, 
\begin_inset Formula $\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}$
\end_inset

.
 It follows that inverting the FIM now requires inverting 2 smaller matrices.
 However the approximation they use is questionable, and they show some
 interesting experimental results.
\end_layout

\begin_layout Subsection
Natural Neural Networks
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "desjardins2015natural"

\end_inset

 exploit the same factorization 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:factoredfim"

\end_inset

 but only consider the term involving the input of the linear transformation
 (
\begin_inset Formula $h$
\end_inset

).
 They propose a reparametrization that will make 
\begin_inset Formula $\mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} & h\\
h^{T} & 1
\end{array}\right)\right]$
\end_inset

 equal the identity.
 To this view, they change the original linear transformation 
\begin_inset Formula $a=Wh+b$
\end_inset

 to become:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a & = & VU\left(h-\mu\right)+d
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $V$
\end_inset

 is the new weight matrix and 
\begin_inset Formula $d$
\end_inset

 are the new biases.
 
\begin_inset Formula $\mu=\mathbb{E}\left[h\right]$
\end_inset

 is the mean value for 
\begin_inset Formula $h$
\end_inset

 and 
\begin_inset Formula $U$
\end_inset

 is the square root of the inverse covariance of 
\begin_inset Formula $h$
\end_inset

, defined by 
\begin_inset Formula $U^{2}=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-1}$
\end_inset

, denoted by 
\begin_inset Formula $U=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-\frac{1}{2}}$
\end_inset

.
 
\begin_inset Formula $U$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

 are not trained using gradient descent but instead they are estimated using
 data from the training set.
\end_layout

\begin_layout Standard
Our new parameters 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 are trained using gradient descent, which will now have the desired property.
 We will denote by 
\begin_inset Formula $h_{e}=U\left(h-\mu\right)$
\end_inset

 our new 
\begin_inset Quotes eld
\end_inset

effective
\begin_inset Quotes erd
\end_inset

 input to the linear transformation induced by the weight matrix 
\begin_inset Formula $V$
\end_inset

.
 Let us first remark that 
\begin_inset Formula $\mathbb{E}\left[h_{e}\right]=U\left(\mathbb{E}\left[h\right]-\mu\right)=U\left(\mu-\mu\right)=0$
\end_inset

, so the new input is centered on average.
 A second remark is that 
\begin_inset Formula $\mathbb{E}\left[h_{e}h_{e}^{T}\right]=U\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]U^{T}=\mathbf{I}$
\end_inset

.
 By construction 
\begin_inset Formula $U$
\end_inset

 cancels out the covariance.
 Wrapping everything together we thus have the desired property that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}\left[\left(\begin{array}{cc}
h_{e}h_{e}^{T} & h_{e}\\
h_{e}^{T} & 1
\end{array}\right)\right] & = & \left(\begin{array}{cc}
\mathbb{E}\left[h_{e}h_{e}^{T}\right] & \mathbb{E}\left[h_{e}\right]\\
\mathbb{E}\left[h_{e}^{T}\right] & 1
\end{array}\right)\\
 & = & \left(\begin{array}{cc}
\mathbf{I} & 0\\
0 & 1
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The FIM for our new reparametrization thus has a better form, and they also
 show experimentally that this method is very efficient, and that by amortizing
 the cost of inverting the covariance matrix over several parameter updates,
 it can be faster that standard SGD.
\end_layout

\begin_layout Subsection
Overcoming catastrophic forgetting
\end_layout

\begin_layout Section
Focus on the covariance part of the decomposition
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\left(\mathbf{J}_{a_{i}}^{T}D\left(x_{i}\right)\mathbf{J}_{a_{i}}\right) & = & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\alpha_{i}\mathbf{I}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
inverse:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left(\begin{array}{cc}
\frac{1}{n}\sum_{i}h_{i}h_{i}^{T} & \frac{1}{n}\sum_{i}h_{i}\\
\frac{1}{n}\sum_{i}h_{i}^{T} & 1
\end{array}\right)^{-1} & = & \left(\begin{array}{cc}
\left(\frac{1}{n}\sum_{i}h_{i}h_{i}^{T}-\frac{1}{n}\sum_{i}h_{i}\frac{1}{n}\sum_{i}h_{i}^{T}\right)^{-1} & -\left(\frac{1}{n}\sum_{i}h_{i}h_{i}^{T}-\frac{1}{n}\sum_{i}h_{i}\frac{1}{n}\sum_{i}h_{i}^{T}\right)^{-1}\frac{1}{n}\sum_{i}h_{i}\\
-\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}h_{i}h_{i}^{T}-\frac{1}{n}\sum_{i}h_{i}\frac{1}{n}\sum_{i}h_{i}^{T}\right)^{-1} & \left(1-\frac{1}{n}\sum_{i}h_{i}^{T}\frac{1}{n}\sum_{i}h_{i}\right)^{-1}
\end{array}\right)\\
 & = & \left(\begin{array}{cc}
\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1} & -\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{i}h_{i}\\
-\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1} & \left(1-\frac{1}{n}\sum_{i}h_{i}^{T}\frac{1}{n}\sum_{i}h_{i}\right)^{-1}
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
gradient:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{1}{n}\sum_{j}\left(\begin{array}{c}
h_{j}\\
1
\end{array}\right)\otimes\left(\mathbf{J}^{a_{j}}\right)^{T}$
\end_inset


\end_layout

\begin_layout Standard
update:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{1}{n}\sum_{j}\left(\begin{array}{cc}
\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1} & -\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{i}h_{i}\\
-\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1} & 1+\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{i}h_{i}
\end{array}\right)\left(\begin{array}{c}
h_{j}\\
1
\end{array}\right)\otimes\mathbf{J}^{a_{j}}$
\end_inset


\end_layout

\begin_layout Standard
update for 
\begin_inset Formula $b$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta b & = & -\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{j}h_{j}\otimes\mathbf{J}^{a_{j}}+\left(1+\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{i}h_{i}\right)\otimes\frac{1}{n}\sum_{j}\mathbf{J}^{a_{j}}\\
 & = & -\frac{1}{n}\sum_{i}h_{i}^{T}\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{j}\left(h_{j}-\frac{1}{n}\sum_{i}h_{i}\right)\otimes\mathbf{J}^{a_{j}}+\frac{1}{n}\sum_{j}\mathbf{J}^{a_{j}}
\end{eqnarray*}

\end_inset

TBC
\end_layout

\begin_layout Standard
update for 
\begin_inset Formula $W$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta W & = & \left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{j}h_{j}\otimes\mathbf{J}^{a_{j}}-\left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{i}h_{i}\otimes\frac{1}{n}\sum_{j}\mathbf{J}^{a_{j}}\\
 & = & \left(\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)\left(h_{i}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}\right)^{-1}\frac{1}{n}\sum_{j}\left(h_{j}-\frac{1}{n}\sum_{i}h_{i}\right)\otimes\mathbf{J}^{a_{j}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Links with well-known methods
\end_layout

\begin_layout Itemize
rmsprop
\end_layout

\begin_layout Itemize
msprop
\end_layout

\begin_layout Itemize
centering trick (découle des méthodes àla second ordre)
\begin_inset CommandInset label
LatexCommand label
name "centering-trick"

\end_inset


\end_layout

\begin_layout Chapter*
Conclusions
\end_layout

\begin_layout Standard
best master's thesis ever
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "plain"

\end_inset


\end_layout

\begin_layout Chapter
\start_of_appendix
An example task for neural networks: learning to generate texture images
\begin_inset CommandInset label
LatexCommand label
name "chap:ff-texture-gen"

\end_inset


\end_layout

\begin_layout Standard
zam
\end_layout

\end_body
\end_document
