#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsbook
\begin_preamble
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{dsfont}
\newcommand{\mathbbOld}{\mathbb}
\renewcommand{\mathbb}{\mathds}
\usepackage{natbib}
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\begin_removed_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_removed_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\branch hidden
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Chapter
Neural networks
\end_layout

\begin_layout Standard
In this chapter, we will introduce concepts and techniques that are used
 in artificial intelligence tasks.
 In particular, we will introduce neural networks, that have proven a powerful
 model and produced state of the art results in a variety of tasks.
\end_layout

\begin_layout Section
Artificial intelligence
\end_layout

\begin_layout Standard
Intelligence is a difficult concept to define.
 We will use the following definition: the ability to make sensible decisions
 in a given situation, possibly making use of a memory of past events that
 share similarities with the current situation.
 The most intelligent individual agent that we are aware of nowadays is
 certainly the human being, amongst other animals.
 Human beings are constantly making decisions given their perception of
 the world that is provided by their 5 senses, using knowledge that they
 have studied or experienced in their life.
 But there is no 
\shape italic
a priori
\shape default
 reason to think that intelligence could not be present in other systems,
 and in particular artificial intelligence is a scientific field that aims
 at implementing intelligence in non-living machines.
\end_layout

\begin_layout Standard
How our society of humans can benefit from artificial intelligence is still
 an open question, out of the scope of the present document.
 Regardless, given the recent popularity of artificial intelligence among
 public research laboratories and in the industry, and the recent successes
 at solving complex tasks, we can say without taking risks that artificial
 intelligence will continue to play a big role in shaping the future of
 our society.
\end_layout

\begin_layout Standard
From a more practical perspective, implementing an artificial intelligent
 machine requires designing a system that takes data that represent the
 current situation, data that represents the memory of the machine, and
 output a decision using this data.
\end_layout

\begin_layout Standard
To put things into context, we will now describe an example task.
 We want to design a program that takes a picture of an animal and a sound
 as input, and outputs whether it thinks the animal present in the picture
 makes the provided sound.
 In a computer, a picture is often encoded as a mathematical tensor of scalar
 values or pixels, the sound as a timeseries of samples of the sound wave,
 and the final decision can be a single scalar value, which will be close
 to 0 if the animal is very unlikely to make the noise, or to 1 if the animal
 is very likely to make the noise.
 The complex machinery inbetween is the intelligent part.
\end_layout

\begin_layout Standard
Manually designing a program for such a task is an overwhelming task.
 Even provided that the input image is quite a small image of 
\begin_inset Formula $32\times32$
\end_inset

 RGB pixels and the sound lasts 
\begin_inset Formula $1s$
\end_inset

 recorded at a sample rate of 20kHz we have a total of 
\begin_inset Formula $32\times32\times3+20\,000=23\,072$
\end_inset

 scalars.
 If we restrict each of these numbers to have 
\begin_inset Formula $256$
\end_inset

 possible values, it leaves us with 
\begin_inset Formula $256^{23\,072}\approx10^{55\,000}$
\end_inset

 possible combinations.
 Even if we only keep the combinations that are plausible, there is too
 many to create a naive program.
 Even with carefully engineered feature extractors based on image and sound
 processing techniques, the remaining work is still challenging.
\end_layout

\begin_layout Standard
Instead, the most successful attempts at solving such tasks use a procedure
 called 
\series bold
machine learning
\series default
: instead of manually defining our program, we define a generic model, and
 we use a dataset of annotated examples of picture, sound, and the corresponding
 answer, and we leave to the computer the task of extracting information
 from the dataset to tune the model so as to obtain the desired program.
\end_layout

\begin_layout Section
Machine learning
\end_layout

\begin_layout Subsection
Parametric functions and learning
\end_layout

\begin_layout Standard
Generally speaking, machine learning consists in finding an unknown function
 
\begin_inset Formula $f$
\end_inset

 from a family of functions 
\begin_inset Formula $\mathcal{F}$
\end_inset

, that will solve a certain task.
 We typically restrict our search to a small family of functions, which
 consists in parametrized functions 
\begin_inset Formula $\mathcal{F}_{\theta}$
\end_inset

.
 We will denote by 
\begin_inset Formula $f_{\theta}$
\end_inset

 such a function, parametrized by a vector of parameters 
\begin_inset Formula $\theta$
\end_inset

.
 Adapting the value of the parameters will change the output of the function
 
\begin_inset Formula $f_{\theta}$
\end_inset

.
 The challenges of machine learning are to find a correct parametrization
 so that our desired function can be approached by a member of 
\begin_inset Formula $\mathcal{F}_{\theta}$
\end_inset

, and to learn the parameters of this target function.
\end_layout

\begin_layout Standard
To this end, we need a measure of the performance of a given function at
 solving our task.
 We choose a loss function 
\begin_inset Formula $l$
\end_inset

, adapted to this task.
 The better our function, the lower the value of 
\begin_inset Formula $l$
\end_inset

.
 The remaining ingredient is a data generating distribution 
\begin_inset Formula $p$
\end_inset

 from which we sample datapoints 
\begin_inset Formula $x\sim p$
\end_inset

 that are our examples.
 A measure of the performance of a function 
\begin_inset Formula $f_{\theta}$
\end_inset

 for the given task is given by the risk:
\begin_inset CommandInset label
LatexCommand label
name "cost-function"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{R}\left(\theta,p\right) & = & \mathbb{E}_{x\sim p}\left[l\left(f_{\theta}\left(x\right),x\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathcal{R}$
\end_inset

 is a scalar value.
 If this value is high, then 
\begin_inset Formula $f_{\theta}$
\end_inset

 is bad at solving the desired task.
 In the opposite, the best function can be found by adjusting 
\begin_inset Formula $\theta$
\end_inset

 so as to reach the smallest value of 
\begin_inset Formula $\mathcal{R}$
\end_inset

.
 The best value for the parameter vector 
\begin_inset Formula $\theta^{*}$
\end_inset

 is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta^{*} & = & \text{argmin}_{\theta}\mathcal{R}\left(\theta,p\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Finding this value 
\begin_inset Formula $\theta^{*}$
\end_inset

 is the task of learning from the data.
\end_layout

\begin_layout Standard
We now present two common tasks and their corresponding loss functions.
 We will restrict to the less general setting of 
\shape italic
supervised
\shape default
 learning, where each data point is composed of an input 
\begin_inset Formula $x$
\end_inset

 and a true target 
\begin_inset Formula $y$
\end_inset

.
 The risk can be written as 
\begin_inset Formula $\mathcal{R}\left(\theta,p\right)=\mathbb{E}_{x,y\sim p}\left[l\left(f_{\theta}\left(x\right),y\right)\right]$
\end_inset

 meaning that the function 
\begin_inset Formula $f_{\theta}$
\end_inset

 only uses the input, and returns a target 
\begin_inset Formula $\hat{y}=f_{\theta}\left(x\right)$
\end_inset

.
 In supervised learning, the loss function compares the true target 
\begin_inset Formula $y$
\end_inset

 with the current estimated target 
\begin_inset Formula $\hat{y}$
\end_inset

.
\end_layout

\begin_layout Standard
In
\series bold
 regression
\series default
, the input vector 
\begin_inset Formula $x$
\end_inset

 is mapped to a numerical value 
\begin_inset Formula $y$
\end_inset

.
 To assess the performance of 
\begin_inset Formula $f_{\theta}$
\end_inset

, we use the loss function 
\begin_inset Formula $l\left(f_{\theta}\left(x\right),y\right)=\left\Vert f_{\theta}\left(x\right)-y\right\Vert _{2}^{2}$
\end_inset

, called the quadratic error.
 It reaches its minimum 
\begin_inset Formula $0$
\end_inset

 when 
\begin_inset Formula $f_{\theta}\left(x\right)=y$
\end_inset

.
 For example we can design a model that predicts the price of a real estate,
 given some features such as the size of the house, the number of bedrooms
 and whether it possesses a fireplace.
\end_layout

\begin_layout Standard
In supervised 
\series bold
classification
\series default
, we classify each data point 
\begin_inset Formula $x$
\end_inset

 into a category 
\begin_inset Formula $y$
\end_inset

.
 A natural loss that comes up is the misclassification indicator function
 
\begin_inset Formula $\mathbf{1}\left(f_{\theta}\left(x\right),y\right)=\left\{ 0\text{ if }f_{\theta}\left(x\right)=y\text{ or }1\text{ otherwise}\right\} $
\end_inset

.
 It counts the examples that are misclassified.
 This function present the disadvantage of not being differentiable (it
 is not even continuous), and we will see in future sections that differentiabil
ity is a valuable property for machine learning.
 Instead, we usually make our function 
\begin_inset Formula $f_{\theta}$
\end_inset

 output a vector of the number of categories, which represents computed
 probabilites of being a member of each category (a scalar between 
\begin_inset Formula $0$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

).
 We use the loss called 
\shape italic
cross entropy 
\begin_inset Formula $l\left(f_{\theta}\left(x\right),y\right)=-\log\left(\left(f_{\theta}\left(x\right)\right)_{y}\right)$
\end_inset


\shape default
.
 This will push the probability of the correct category toward 
\begin_inset Formula $1$
\end_inset

.
 An example classification task is proposed by the ImageNet project 
\begin_inset CommandInset citation
LatexCommand citep
key "deng2009imagenet"

\end_inset

 where the task is to classify images to detect what they represent such
 as an animal, or a car and so on.
\end_layout

\begin_layout Subsection
Empirical risk and bias-variance tradeoff
\begin_inset CommandInset label
LatexCommand label
name "subsec:Empirical-risk-and"

\end_inset


\end_layout

\begin_layout Standard
In practice, often, we do not have access to a data generating function
 
\begin_inset Formula $p$
\end_inset

, but instead we have a limited number of samples from it.
 This dataset of examples gives us an estimate of the true risk, by replacing
 the expectation with a finite sum, called the empirical risk 
\begin_inset Formula $R$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\begin{array}{ccccc}
\mathcal{R}\left(\theta,p\right) & \approx & R\left(\theta,\mathcal{D}\right) & = & \frac{1}{n}\sum_{x\in\mathcal{D}}l\left(f_{\theta}\left(x\right),x\right)\end{array}\label{eq:empirical-risk}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $n$
\end_inset

 is the total number of examples in 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
\end_layout

\begin_layout Standard
For random values of the parameters 
\begin_inset Formula $\theta$
\end_inset

, the empirical risk and the true risk will have similar values.
 But this is not the case when the parameters have been tuned so that the
 empirical risk is minimum.
 In the extreme case, consider a model that has memorized all examples of
 the training set by heart.
 In order to make a prediction for a new example, this model will seek the
 closest example in 
\begin_inset Formula $\mathcal{D}$
\end_inset

, in term of the euclidean distance, and output the exact same answer than
 this closest example.
 This model is called a 1-nearest neighbour regressor or classifier regarding
 the considered task.
 In this case the empirical risk is 
\begin_inset Formula $0$
\end_inset

, but we have no guarantee that the model generalizes on new examples.
\end_layout

\begin_layout Standard
A model with too much expressivity, or 
\shape italic
variance
\shape default
, will be able to learn all examples in the training set by heart without
 having the ability to generalize on new examples, which is called 
\shape italic
overfitting
\shape default
.
 A model with not enough expressivity will not be able to perform well even
 on the training set, which is called 
\shape italic
underfitting
\shape default
.
 In the meantime it will have a similar performance on the true data generating
 distribution.
 We say that there is a 
\shape italic
bias
\shape default
 toward a family of model.
 The bias-variance tradeoff consists in selecting a model that has sufficient
 expressivity to have a good performance on the train set, while not having
 too much expressivity so that it will not overfit, and still have good
 performance on the true data generating distribution.
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Standard
A way of combatting overfitting is to use regularization.
 It is a way of constraining the values of the parameters of a function
 using priors.
 For example L2 regularization penalizes the squared norm of the parameter
 vector.
 It constrains all values to stay small.
\end_layout

\begin_layout Standard
Data augmentation is another mean of combatting overfitting.
 We can use the knowledge that we have of our dataset to create new examples.
 For example for a classification task of images, we know from our experience
 of the world that rotating or translating an image will not change its
 content.
 We can thus artificially augment our training set by including rotated
 and translated versions of the same images.
\end_layout

\begin_layout Section
Neural networks
\end_layout

\begin_layout Standard
Neural networks are a family of parametrized models.
 They have empirically proven very powerful at solving complex tasks.
 Along with the availability of easy to use frameworks to build neural networks
 and learn from data, new interests have developed from industry to integrate
 artificial intelligence inspired techniques in more and more products.
 The first commercial successes date back to the 90s when AT&T developed
 an automated system to read handwritten digits on bank checks, using convolutio
nal neural networks 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun1998gradient"

\end_inset

.
 Recent successes include advances in machine translation, image and voice
 recognition, close-to-realistic image generation.
 They have applications in online services integrated in smartphones, but
 also enable the invention of new automated systems that will benefit more
 traditional industries, (energy, agriculture, arts, ..)
\end_layout

\begin_layout Section
Common types of neural networks
\end_layout

\begin_layout Subsection
Multilayer perceptron
\end_layout

\begin_layout Standard
We now define the simplest neural network structure called the perceptron
 
\begin_inset CommandInset citation
LatexCommand citep
key "rosenblatt1961principles"

\end_inset

.
 From an input data vector 
\begin_inset Formula $x$
\end_inset

, it creates a prediction 
\begin_inset Formula $y$
\end_inset

 using the relation 
\begin_inset Formula $y\left(x\right)=f\left(\left\langle w,x\right\rangle +b\right)$
\end_inset

.
 
\begin_inset Formula $w$
\end_inset

 is called the weight vector, and 
\begin_inset Formula $b$
\end_inset

 is the bias.
 
\begin_inset Formula $f$
\end_inset

 is a function, and is sometimes called the nonlinearity or activation function
 as it allows the function 
\begin_inset Formula $y$
\end_inset

 to be different from just a linear function of its input 
\begin_inset Formula $x$
\end_inset

.
 From a trained perceptron, we take a decision for an example 
\begin_inset Formula $x$
\end_inset

 by comparing the value of the corresponding 
\begin_inset Formula $y$
\end_inset

 using a threshold value.
 Perceptrons were implemented before the invention of modern computers,
 as complex electronic circuits.
 The weights were encoded in hardware potentiometers and trained using an
 error-propagating process.
 Remarkably, these complex pieces of machinery were capable of obtaining
 good results for the task of recognizing simple shape images.
\end_layout

\begin_layout Standard
These perceptrons were designed to approximately replicate the computations
 made by a network of biological neurons.
 Each neuron gets input data from several other neurons, consisting in voltage
 spikes.
 The rate at which these spikes occur can be intepreted as whether a neuron
 is excited or not.
 Each neuron has different sensibilities regarding how it will react to
 an increase in spike rate from other neurons, this sensibility being mimicked
 by the weights in artificial neural networks.
 In its most simple modeling, the human brain is just a very complex network
 of these neurons.
 This is the inspiration for artificial neural networks.
\end_layout

\begin_layout Standard
This single perceptron is extended in a more complex model called the multilayer
 perceptron (MLP).
 It consists in alternatively stacking layers of linear transformation 
\begin_inset Formula $a=Wx+b$
\end_inset

 and nonlinearities 
\begin_inset Formula $y=f\left(a\right)$
\end_inset

, using a vectorized generalization of the perceptron: 
\begin_inset Formula $y\left(x\right)=f\left(Wx+b\right)$
\end_inset

.
 
\begin_inset Formula $W$
\end_inset

 is now a weight matrix, and 
\begin_inset Formula $b$
\end_inset

 a bias vector.
 
\begin_inset Formula $f$
\end_inset

 is often an elementwise function.
 We stack these transformations to get more complex functions.
 An example for 2 layers gives a function 
\begin_inset Formula $y\left(x\right)=f_{2}\left(W_{2}f_{1}\left(W_{1}x+b_{1}\right)+b_{2}\right)$
\end_inset

.
 The intermediate values obtained at each layer 
\begin_inset Formula $f_{1}\left(W_{1}x+b_{1}\right)$
\end_inset

 are called the hidden representations as they are new representations of
 the same input data, but encoded in a different way.
 A trained neural network will create representations that are better suited
 for its task.
 For example, if we imagine a task of classifying images between those which
 picture a dog and those with a cat, we are interested in a high level represent
ation of characteristics such as a long tail, whiskers or sharp ears.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/mlp.pdf
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
A multilayer perceptron consists in alternatively stacking layers of a linear
 transformation and a nonlinearity
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Convolutional networks
\end_layout

\begin_layout Standard
Convolutional networks 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun1989backpropagation"

\end_inset

 are well-suited for tasks involving sequential (timeseries) or spatial
 data (images).
 Instead of multiplying a weight matrix with the whole input vector as in
 MLPs, we split this input into smaller chunks of fixed sized corresponding
 to small vectors (1d case) or small matrices (2d case).
 The same weight matrix is applied to each of these smaller vectors so as
 to get a result at each position.
 In fact it amounts to a standard MLP with sparse connectivity and with
 the weights sharing values between corresponding positions.
\end_layout

\begin_layout Subsection
Autoencoders
\begin_inset CommandInset label
LatexCommand label
name "subsec:Autoencoders"

\end_inset


\end_layout

\begin_layout Standard
Autoencoders 
\begin_inset CommandInset citation
LatexCommand citep
key "hinton2006reducing,vincent2008extracting"

\end_inset

 are neural networks that are composed of an encoder part and a decoder
 part.
 The encoder takes the input and encodes encodes it to a new representation
 (often with less dimension than the input).
 The decoder takes the encoded input with the task of reconstructing the
 output.
 The encoded representation is often a layer with less neurons than the
 size of the input.
 The autoencoder is trained end-to-end, without manually taking care of
 the encoded representation.
 This representation is automatically created by learning from the data.
 In order to be efficient, a well designed autoencoder will need to create
 a relevant representation of the data.
 For example, if we want to encode pictures of faces, relevant representations
 could be the gender, the color and size of hair, and so on.
\end_layout

\begin_layout Section
More elaborated cost functions
\end_layout

\begin_layout Standard
We can often associate a task and its corresponding loss function: regression
 with the quadratic error loss, and classification with the cross entropy
 loss 
\begin_inset CommandInset ref
LatexCommand ref
reference "cost-function"

\end_inset

.
 Some more recent advances in neural networks make use of more complex cost
 functions.
\end_layout

\begin_layout Standard

\series bold
Neural art 
\series default

\begin_inset CommandInset citation
LatexCommand citep
key "gatys2015neural"

\end_inset

 and its feed-forward extension 
\begin_inset CommandInset citation
LatexCommand citep
key "ulyanov2016texture"

\end_inset

 tackle the task of generating artwork images from a real world picture,
 that mimick the style of a given painting.
 To this end, they create a cost function that measures how a generated
 image resembles both the picture and the painting:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\text{total}}\left(p,a,x\right) & = & \alpha\mathcal{L}_{\text{content}}\left(p,x\right)+\beta\mathcal{L}_{\text{style}}\left(a,x\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $p$
\end_inset

 is the picture, 
\begin_inset Formula $a$
\end_inset

 is the artwork that we want to extract the style, and 
\begin_inset Formula $x$
\end_inset

 is any image.
 
\begin_inset Formula $\mathcal{L}_{\text{content}}$
\end_inset

 is a loss function that measures how close 
\begin_inset Formula $x$
\end_inset

 is from 
\begin_inset Formula $p$
\end_inset

 in terms of contents, and 
\begin_inset Formula $\mathcal{L}_{\text{style}}$
\end_inset

 is a loss function that measures a distance from 
\begin_inset Formula $a$
\end_inset

 to 
\begin_inset Formula $x$
\end_inset

 in terms of artistic style.
 By minimizing 
\begin_inset Formula $\mathcal{L}_{\text{total}}\left(p,a,x\right)$
\end_inset

 with respect to 
\begin_inset Formula $x$
\end_inset

 for given 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $a$
\end_inset

, we obtain the desired image in 
\begin_inset Formula $x$
\end_inset

.
 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are scalar values that control the influence of each part of the loss.
 In the original paper 
\begin_inset CommandInset citation
LatexCommand citep
key "gatys2015neural"

\end_inset

 we start from a randomly initialized 
\begin_inset Formula $x$
\end_inset

 and we perform gradient descent on each pixel of 
\begin_inset Formula $x$
\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "ulyanov2016texture"

\end_inset

 we use a convolutional neural network to generate 
\begin_inset Formula $x$
\end_inset

, which takes the picture as input, and outputs the desired stylized image.
 This network is trained using 
\begin_inset Formula $\mathcal{L}_{\text{total}}$
\end_inset

.
 It has the main advantage of being very fast as generating new images once
 it has be trained on a specific artwork.
\end_layout

\begin_layout Standard
Another family of cost functions that becomes more and more popular is that
 of the discriminators in 
\series bold
Generative Adversarial Networks 
\series default

\begin_inset CommandInset citation
LatexCommand citep
key "goodfellow2014generative"

\end_inset

, that can be thought of as learned cost functions.
 In this setup, 2 networks are trained one against each other : the generator
 part takes random noise and generate a sample that tries to fool the discrimina
tor.
 The discriminator also is a trained network that tries to classify whether
 its input is from a given data distribution, or if it was generated by
 the generator.
 Training these networks is very unstable, and is the object of many research
 at the time of this writing.
 But provided that we successfully trained both parts, we get a generator
 that is able to generate new samples of complex data, such as realistic
 images.
\end_layout

\begin_layout Chapter
Optimization of neural networks
\end_layout

\begin_layout Section
Gradient descent and backpropagation
\begin_inset CommandInset label
LatexCommand label
name "sec:Gradient-descent-and"

\end_inset


\end_layout

\begin_layout Subsection
Learning using gradient descent
\end_layout

\begin_layout Standard
Once we have chosen a model, and supposing that this model is capable of
 solving a given task with a dataset of examples of this task, the main
 challenge is now to learn the parameters of the model from the data.
 Some simple models have closed form solutions, this is for example the
 case for a linear model and a regression task.
 For more complex models such as neural networks, we can not derive a simple
 formula for getting the values of all parameters given a dataset.
 In this case, we start from an initialized network and iterate updates
 for our parameters until we get the expected results.
 To this end, we must find an efficient way of getting an update 
\begin_inset Formula $\Delta\theta$
\end_inset

 of our parameters 
\begin_inset Formula $\theta$
\end_inset

.
 Considering that we aim at finding the minimum of the empirical risk, such
 an update is given by the steepest direction of descent of the empirical
 risk, given by minus the gradient of the empirical risk, with respect to
 the parameters, denoted by 
\begin_inset Formula $\nabla_{\theta}R$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\nabla_{\theta}R & = & \frac{1}{n}\sum_{i}\nabla_{\theta}l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Once we have a direction, we must choose how far to move in this direction.
 One way of choosing this rate is by using a line search algorithm.
 But its requires evaluating our objective several times, which can be costly
 for deep networks or big datasets.
 We will stick to a simple fixed scalar learning rate 
\begin_inset Formula $\lambda$
\end_inset

, so that each iteration becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta & \leftarrow & \theta-\lambda\nabla_{\theta}R
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Of course the learning rate 
\begin_inset Formula $\lambda$
\end_inset

 plays a very important role.
 If we choose a value that is too small then it will take many steps to
 reach the same performance, so it will take longer.
 If the value is too large then we can go too far, to a point in the space
 of parameters where the gradient has changed so the direction that we are
 following is no longer a descent direction.
 In this case, we can even decrease the performance.
 For a practical example think of a valley.
 We start from a side of the valley and follow the steepest descent direction.
 If we go too far we will pass the bottom of the valley and start going
 up again.
\end_layout

\begin_layout Subsection
Computing the gradients using backpropagation
\end_layout

\begin_layout Standard
It might be difficult to get an exact expression for the gradient of a complex
 function, such as a neural network.
 What enabled the success of neural networks was a smart use of the chain
 rule for splitting the computation of the gradient, into a sequence of
 linear algebra operations, that is described in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

.
 For example we can decompose the gradient going through a layer 
\begin_inset Formula $h_{l}=f_{l}\left(a_{l}\right)=f_{l}\left(W_{l}h_{l-1}+b_{l}\right)$
\end_inset

 using the expression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\nabla_{h_{l-1}}l & = & \left(\mathbf{J}_{a_{l}}^{h_{l-1}}\right)^{T}\nabla_{a_{l}}l\\
 & = & \left(\mathbf{J}_{a_{l}}^{h_{l-1}}\right)^{T}\left(\mathbf{J}_{h_{l}}^{a_{l}}\right)^{T}\nabla_{h_{l}}l
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We denote by 
\begin_inset Formula $\mathbf{J}_{f}^{x}$
\end_inset

 the jacobian of the vector function 
\begin_inset Formula $f$
\end_inset

 with respect to 
\begin_inset Formula $x$
\end_inset

.
 It is the matrix composed of the partial derivatives 
\begin_inset Formula $\left(\mathbf{J}_{f}^{x}\right)_{ij}=\frac{\partial f_{i}}{\partial x_{j}}$
\end_inset

, so it has dimension 
\begin_inset Formula $n_{f}\times n_{x}$
\end_inset

.
 In the particular case of neural networks we have simple expressions for
 the jacobians of the backpropagated signal (red arrows in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{J}_{a_{l}}^{h_{l-1}} & = & W_{l}\\
\mathbf{J}_{h_{l}}^{a_{l}} & = & \text{diag}\left(f_{l}'\left(a_{l}\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\text{diag}$
\end_inset

 is the operation that takes a vector and transforms it to a diagonal matrix
 with the values of the vector as diagonal terms.
 These jacobians can be thought of the gradient flow between layers.
\end_layout

\begin_layout Standard
We also have expressions for the jacobians of the activations with respect
 to the parameters (green arrows in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{J}_{a_{l}}^{W_{l}} & = & \nabla_{a_{l}}l\left(h_{l-1}\right)^{T}\\
\mathbf{J}_{a_{l}}^{b_{l}} & = & \nabla_{a_{l}}l
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/backprop.pdf
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Forward (in black) and backward (in red) propagation of the intermediate
 results of the process of computing the output of the network and the gradient
 corresponding to this output and the desired "true" output.
 The green arrows represent the computation of the gradients with respect
 to the parameters, given the gradients with respect to the pre-activations.
\begin_inset CommandInset label
LatexCommand label
name "fprop-bprop"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Automatic differentiation tools
\end_layout

\begin_layout Standard
A key component in training neural networks is the library that we use to
 implement our models.
 The difficulty of implementing backpropagation in all kinds of neural networks
 inspired models, is solved using an automatic differentiation tool, such
 as Theano 
\begin_inset CommandInset citation
LatexCommand citep
key "bastien2012theano"

\end_inset

.
 Using such a tool we can define a model and a scalar cost function, then
 call a method 
\family typewriter
grad
\family default
 that takes care of building the computational graph of all operations required
 to get the gradients with respect to the parameters.
\end_layout

\begin_layout Section
Stochastic gradient descent
\begin_inset CommandInset label
LatexCommand label
name "sec:Stochastic-gradient-descent"

\end_inset


\end_layout

\begin_layout Standard
While backpropagation is an efficient way of computing the exact gradient
 of the empirical risk with respect to the parameters, in practice we are
 not required to use its exact value, but rather we can use an estimate
 of the gradient, as long as this estimate will make our objective decrease.
 It is worth recalling at this point that even the exact gradient of the
 empirical risk is different of the real gradient we would like to follow,
 which is the gradient of the true expected risk 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Empirical-risk-and"

\end_inset

.
\end_layout

\begin_layout Standard
A good estimate is obtained by computing the gradient using a smaller subset
 of our dataset, called a mini-batch.
 Replacing the gradient descent update with this estimate is called mini-batch
 gradient descent, and the extreme case where we take a mini-batch size
 of 1 is called 
\series bold
stochastic gradient descent
\series default
 (SGD) 
\begin_inset CommandInset citation
LatexCommand citep
key "bottou2010large"

\end_inset

.
 The main benefit of using SGD instead of full gradient descent is that
 we can reduce the memory required to compute the gradient, in order to
 fit in the memory of a GPU, which is very efficient for computing vector
 operations such as the one required to get the values of our gradients.
 The memory required to backpropagate the intermediate gradients (red arrows
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

) is proportional to the size of the mini-batch, and so we face a trade-off
 between obtaining a noisier update direction with a small mini-batch, but
 very fast, and obtaining a direction that is more precise but that takes
 longer.
 Typically it is more efficient to compute several noisy gradients using
 mini-batches in the GPU memory and make several updates, than to compute
 a single more precise update on a bigger mini-batch or the full dataset.
\end_layout

\begin_layout Section
Hyperparameters
\end_layout

\begin_layout Standard
In the preceding sections we have introduced the learning rate (section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Gradient-descent-and"

\end_inset

) and the minibatch size (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Stochastic-gradient-descent"

\end_inset

).
 These values are called hyperparameters, which is another kind of parametrizati
on of our learning procedure.
 Hyperparameters also include the structure of our model, such as the number
 of hidden layers and hidden units, the number of training iterations, the
 coefficients of the regularization terms.
 The success of our learning procedure is dependant on the values of the
 hyperparameters.
 We do not find the optimal value using gradient descent, but instead we
 tune it by running several time the same experiment with different hyperparamet
er values, and compare the final value of the risk on a held-out set of
 examples called the validation set.
\end_layout

\begin_layout Standard
A difficulty in comparing optimization algorithms resides in the fact that
 there performances can change drastically for different values of hyperparamete
rs.
 Optimization papers sometimes mention heuristics that they experimentally
 found provide with a sensible value for some hyperparameters.
 But to overcome this difficulty and provide 
\begin_inset Quotes eld
\end_inset

fair
\begin_inset Quotes erd
\end_inset

 benchmarks, we usually tune the values of the hyperparameters by trying
 several sets of values.
 Hyperparameters tuning is a research field on its own, so we will just
 introduce 2 existing methods and we will later motivate our use of a new
 technique that we call biased random search (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "biased-random-search"

\end_inset

).
\end_layout

\begin_layout Standard
The most simple hyperparameter tuning procedure, called 
\series bold
grid search
\series default
, consists in selecting values at fixed length intervals, or using a logarithmic
 scale.
 A simple example would be a training procedure involving only one hyperparamete
r: the learning rate.
 We can launch several experiments for all values in 
\begin_inset Formula $\left\{ 10^{-3},10^{-2},10^{-1},1\right\} $
\end_inset

 for a fixed number of updates and select the learning rate for which we
 obtained the best value for our target criteria such as the validation
 loss.
 When generalizing to several hyperparameters, we have to select all combination
s of values, which make our search space grow exponentially, and similarly
 for the number of experiments we will have to run.
\end_layout

\begin_layout Standard
A first extension to grid search replaces the fixed length intervals by
 random samples in our search space.
 It is called 
\series bold
random search
\series default
.
 Its main advantage over grid search shows up when any hyperparameter has
 no important effect on the learning algorithm 
\begin_inset CommandInset citation
LatexCommand citep
key "bergstra2012random"

\end_inset

.
 It will explore more different values for the other hyperparameters.
 In this case, it clearly appears that they are correlated, in the sense
 that the best value for one hyperparameter depends on the chosen value
 for the other hyperparameter.
\end_layout

\begin_layout Standard
In the rest of this work, we will use an extension of random search that
 we call 
\series bold
biased random search
\series default
, and that we present in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "biased-random-search"

\end_inset

.
\end_layout

\begin_layout Standard
To give a more intuitive understanding of the effect of the hyperparameter
 values on the learning algorithm, we also introduce a graphical representation
 that helps making sense of the interaction between different hyperparameters,
 which we now describe.
 We launch a hyperparameter search on 2 hyperparameters and plot this point
 on a scatter plot, with a color scale depicting the final result of each
 experiment.
 By observing this plot, we can identify 2d patterns of the link between
 2 hyperparameters.
 As an illustration, we use the task described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:autoencoder-benchmark"

\end_inset

, using standard stochastic gradient descent and by keeping all hyperparameters
 values fixed, that is we use a fixed mini-batch size, and a fixed number
 of parameter updates.
 We tune 2 hyperparameters: the learning rate and the variance of the initial
 random weights, and we plot the result in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Relation-between-2"

\end_inset

.
 These plots show the interaction between 2 hyperparameters.
 We observe that the best values lie in a region with a very particular
 shape that can be assimilated to a tilted valley (it is not parallel to
 the x-axis nor the y-axis).
\end_layout

\begin_layout Standard
These plots and this random search technique are a key component for assessing
 the true performance of optimization techniques that we present in section
 4 and 5.
 Indeed it is easy to experimentally find that a new optimization technique
 which gives better performance than a baseline if we spend too much time
 tuning hyperparameters for our new technique but stick to default values
 for baselines.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/hp_search_correl.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Relation between 2 hyperparameters: for this experiment we can clearly see
 that the plotted hyperparameters are not independant one from each other.
 The color scale represents the final performance (best performing runs
 in blue).
\begin_inset CommandInset label
LatexCommand label
name "fig:Relation-between-2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Limits of (stochastic) gradient descent and some directions to overcome
 them
\end_layout

\begin_layout Standard
We can think of the task of training a neural network as the one of finding
 the minimum of a scalar field in 
\begin_inset Formula $n$
\end_inset

 dimensions, 
\begin_inset Formula $n$
\end_inset

 being the number of parameters.
 Each gradient descent step is a small shift in this field.
 We must ensure that this path in the field of the empirical risk is feasible.
 We now present reasons that can make this field have a pathological structure,
 and directions to avoid these difficulties.
\end_layout

\begin_layout Subsection
Gradient magnitudes
\end_layout

\begin_layout Standard
A common issue for deep networks or recurrents networks is how to control
 the magnitude of the gradient flow for many layers.
 If the spectral norm of the jacobians 
\begin_inset Formula $\mathbf{J}_{a_{l}}^{h_{l-1}}\mathbf{J}_{h_{l}}^{a_{l}}$
\end_inset

 is too large, then the norm of the gradient will exponentially grow for
 lower layers.
 This can happen if the weight matrices have singular values that are too
 large compared to 
\begin_inset Formula $1$
\end_inset

 while the derivatives of the activation functions take large values.
 In this case, we are in a situation of exploding gradients.
 This effect is amplified in recurrent networks, where the same weight matrix
 is repeatedly used in the backward pass.
 For such a ill-conditioned problem, gradient descent will not be effective.
 Indeed, in the case of exploding gradient, two layers separated by several
 others will have updates of different order of magnitude.
\end_layout

\begin_layout Standard
This effect can be mitigated using gradient clipping 
\begin_inset CommandInset citation
LatexCommand citep
key "pascanu2013difficulty"

\end_inset

.
 We can also use second order methods to compensate for gradient vanishing/explo
ding as proposed in 
\begin_inset CommandInset citation
LatexCommand citep
key "martens2011learning"

\end_inset

.
\end_layout

\begin_layout Subsection
Initialization
\begin_inset CommandInset label
LatexCommand label
name "initialization-of-weights"

\end_inset


\end_layout

\begin_layout Standard
Initialization of the weight matrices is of crucial importance.
 In terms of our empirical risk field in the space of parameters, it controls
 how far we start from a minimum.
 A good initialization scheme must at least make sure that there is enough
 signal flowing in forward and backward direction, that is: the weights
 must be chosen not too small, otherwise the forward signal will be smaller
 and smaller, and in the meantime the weights must not be too large, so
 as to avoid exploding gradients in the backward pass, and saturating functions
 in the forward pass.
\end_layout

\begin_layout Standard
The most common initialization scheme at the time of writing are Glorot
 initialization 
\begin_inset CommandInset citation
LatexCommand citep
key "glorot2010understanding"

\end_inset

 and He initialization 
\begin_inset CommandInset citation
LatexCommand citep
key "he2015delving"

\end_inset

.
 Glorot takes care of maintaining a training signal during the forward pass,
 and the backward pass, by sampling random weights from an uniform distribution
 with variance 
\begin_inset Formula $\frac{\alpha}{n_{in}+n_{out}}$
\end_inset

, while He argues that only the forward pass matters so the weights should
 be initialized from a distribution with variance 
\begin_inset Formula $\frac{\alpha}{n_{in}}$
\end_inset

.
 In both cases, 
\begin_inset Formula $\alpha$
\end_inset

 depends on the activation function.
 While 
\begin_inset CommandInset citation
LatexCommand citet
key "glorot2010understanding"

\end_inset

 propose default values for tanh and ReLU, we treated 
\begin_inset Formula $\alpha$
\end_inset

 as a hyperparameter in our experiments, and tuned it using a biased random
 search (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "biased-random-search"

\end_inset

).
\end_layout

\begin_layout Subsection
Gradient smoothing methods
\end_layout

\begin_layout Standard
A family of optimization tricks uses geometrical considerations in the space
 of parameter values.
 In this case with some common sense we can define a simple principle to
 derive better updates which is that for an equivalent decrease of the empirical
 risk, we must follow a direction of descent that has a smaller derivative
 for longer in order to achieve the same improvement as for a direction
 that has a larger derivative.
 Many popular techniques use this principle, the most successful ones at
 the time of writing being Adam 
\begin_inset CommandInset citation
LatexCommand citep
key "kingma2014adam"

\end_inset

, RMSProp 
\begin_inset CommandInset citation
LatexCommand citep
key "tieleman2012lecture"

\end_inset

, Nesterov momentum 
\begin_inset CommandInset citation
LatexCommand citep
key "nesterov1983method"

\end_inset

 and so on.
\end_layout

\begin_layout Chapter
Second order methods in neural networks
\end_layout

\begin_layout Standard
In this section, we introduce the well known second order methods known
 as Newton's method and the less popular but very effective natural gradient
 descent.
 We then derive the updates of this methods adapted to neural networks.
\end_layout

\begin_layout Section
Second order methods
\end_layout

\begin_layout Subsection
Newton steps
\end_layout

\begin_layout Standard
Second order methods refer to all optimization methods that make use of
 the second derivative or Hessian matrix of the function to be minimized.
 It follows from the Taylor series decomposition of the function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
f\left(x+\Delta x\right) & = & f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x+o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\left(\nabla^{2}f\right)_{x}$
\end_inset

 is the Hessian matrix of 
\begin_inset Formula $f$
\end_inset

, expressed at 
\begin_inset Formula $x$
\end_inset

.
 We use the little-o notation 
\begin_inset Formula $o$
\end_inset

 that represents an unknown function with the only property that 
\begin_inset Formula $\lim_{x\rightarrow0}\frac{o\left(x\right)}{x}=0$
\end_inset

.
 By constraining 
\begin_inset Formula $\left\Vert \Delta x\right\Vert _{2}^{2}$
\end_inset

 too stay small, we can ignore higher order terms (
\begin_inset Formula $o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)=0$
\end_inset

) and we have a quadratic approximation for 
\begin_inset Formula $f$
\end_inset

.
 Using this approximation in a minimization problem, we get the following
 minimization which has a closed form solution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x^{*} & = & \text{argmin}_{\Delta x}f\left(x+\Delta x\right)\\
 & \approx & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This expression is solved by taking the derivative with respect to 
\begin_inset Formula $\Delta x$
\end_inset

, and setting it to zero which yields:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left(\nabla^{2}f\right)_{x}\Delta x & = & -\left(\nabla f\right)_{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
If we assume that 
\begin_inset Formula $f$
\end_inset

 has a minimum in 
\begin_inset Formula $x^{*}$
\end_inset

, then the Hessian will be positive definite in 
\begin_inset Formula $x^{*}$
\end_inset

, and under the supplementary assumption that the Hessian is continuous,
 it will also be positive definite in a neighborhood of 
\begin_inset Formula $x^{*}$
\end_inset

.
 In this case, it is invertible and we get the solution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\Delta x & = & -\left(\nabla^{2}f\right)_{x}^{-1}\left(\nabla f\right)_{x}\label{eq:newton-step}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
This update 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:newton-step"

\end_inset

 is called the 
\series bold
Newton step
\series default
.
 By making several iterations of Newton, and under the assumption that we
 are close enough to a minimum so that 
\begin_inset Formula $\left(\nabla^{2}f\right)_{x}$
\end_inset

 remains positive definite, the updates will converge to this minimum.
\end_layout

\begin_layout Standard
The main difficulty of this algorithm is that it does not scale well when
 applied to problems with many variables such as neural network optimization.
 In this case 
\begin_inset Formula $f$
\end_inset

 is the empirical risk, and the variables that we are optimizing are the
 parameters of the network.
 The limitations come from the following aspects:
\end_layout

\begin_layout Enumerate

\shape italic
Getting the value of the Hessian matrix
\shape default
: Using an automatic differentiation software, we can get an expression
 for the Hessian, by differentiating the symbolic expression of the gradient.
 But unlike the computation of the gradient, the graph produced to compute
 the Hessian will have many more nodes.
 We will explore this question in more details in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

 and present an approximate value of the Hessian called Gauss-Newton.
\end_layout

\begin_layout Enumerate

\shape italic
Storing the Hessian matrix
\shape default
: The Hessian matrix is a square matrix of size 
\begin_inset Formula $n_{\text{parameters}}\times n_{parameters}$
\end_inset

.
 As the number of parameters grows, which is the case when building deep
 networks, the memory required to store the Hessian will grow in 
\begin_inset Formula $O\left(n^{2}\right)$
\end_inset

.
 We will present an approximation of the Hessian that saves memory in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Block-diagonal-Hessian"

\end_inset

.
\end_layout

\begin_layout Enumerate

\shape italic
Inverting the Hessian matrix
\shape default
: Inverting the Hessian matrix is also costly as it grows in 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

 with the size of the matrix.
 Some techniques use 2nd order information without inverting the Hessian
 such as 
\shape italic
Hessian Free
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "martens2010deep"

\end_inset

.
 We propose to factorize the Hessian so as to require inverting a smaller
 matrix while benefiting from some 2nd order information in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:focus-covariance"

\end_inset

.
\end_layout

\begin_layout Enumerate

\shape italic
Saddle points
\shape default
: The optimization problem of minimizing a cost function over a dataset
 has many more saddle points than local minima 
\begin_inset CommandInset citation
LatexCommand citep
key "dauphin2014identifying"

\end_inset

.
 In this case using the Hessian will fail as it will converge to a saddle
 point instead of escaping from it in order to find a minimum.
\end_layout

\begin_layout Subsection
The learning rate
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-learning-rate"

\end_inset


\end_layout

\begin_layout Standard
Amongst other hyperparameters, the learning rate of standard (stochastic)
 gradient descent plays a particular role which we will show in the following.
 We use the quadratic approximation for a function 
\begin_inset Formula $f$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x^{*} & = & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x+o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
If we replace the Hessian with a scaled diagonal matrix 
\begin_inset Formula $\lambda\mathbf{I}$
\end_inset

, we can simplify this expression to the following one that is often used
 for deriving the first order gradient descent update:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x^{*} & \approx & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{\lambda}{2}\Delta x^{T}\Delta x
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
By solving this minimization problem we recover the update 
\begin_inset Formula $\Delta x^{*}=-\frac{1}{\lambda}\left(\nabla f\right)_{x}$
\end_inset

 with 
\begin_inset Formula $\frac{1}{\lambda}$
\end_inset

 playing the role of the usual learning rate.
 But of course this 
\begin_inset Formula $\lambda$
\end_inset

 hides second order information.
 In fact, 
\begin_inset CommandInset citation
LatexCommand cite
key "lecun1993automatic"

\end_inset

 proposes to automatically adapt the value of the learning rate by using
 the biggest eigenvalue of the hessian as 
\begin_inset Formula $\lambda$
\end_inset

.
 In this case we are guaranteed that we do not go too far in the direction
 of greatest curvature (which is the corresponding eigenvector).
 But in exchange it will equivalently scale down an update in any other
 direction, even if an optimal step would require to go further in this
 direction.
\end_layout

\begin_layout Subsection
Validity of Newton for non quadratic functions and Tikhonov regularization
\begin_inset CommandInset label
LatexCommand label
name "tikhonov"

\end_inset


\end_layout

\begin_layout Standard
In the previous section, we considered that our function was approximated
 by its second order Taylor series decomposition.
 While this is true in a neighborhood of 
\begin_inset Formula $x$
\end_inset

, the approximation becomes less precise as we move away from 
\begin_inset Formula $x$
\end_inset

.
 In particular this is the case when the Newton step provide big updates,
 that is when the Hessian has at least one small eigenvalue.
 The corresponding eigenvector points in a direction that will have a low
 curvature using the quadratic approximation, so the minimum following this
 direction will be far away.
 But the actual function that we are minimizing is not a quadratic, and
 the terms hidden in 
\begin_inset Formula $o\left(\left\Vert \Delta x\right\Vert _{2}^{2}\right)$
\end_inset

 will become preponderant for bigger values of 
\begin_inset Formula $\Delta x$
\end_inset

.
\end_layout

\begin_layout Standard
To counter this undesirable effect, we simply add a regularization term
 that penalizes bigger values of 
\begin_inset Formula $\Delta x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x^{*} & = & \text{argmin}_{\Delta x}f\left(x+\Delta x\right)\\
 & \approx & \text{argmin}_{\Delta x}f\left(x\right)+\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\nabla^{2}f\right)_{x}\Delta x+\frac{\epsilon}{2}\left\Vert \Delta x\right\Vert _{2}^{2}\\
 & = & \text{argmin}_{\Delta x}\left(\nabla f\right)_{x}^{T}\Delta x+\frac{1}{2}\Delta x^{T}\left(\left(\nabla^{2}f\right)_{x}+\epsilon\mathbf{I}\right)\Delta x
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This gives the Tikhonov regularized version of the Newton step:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta x & = & -\left(\left(\nabla^{2}f\right)_{x}+\epsilon\mathbf{I}\right)^{-1}\left(\nabla f\right)_{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This new hyperparameter 
\begin_inset Formula $\epsilon$
\end_inset

 controls the size of the steps, and thus plays a very similar role to the
 learning rate.
\end_layout

\begin_layout Standard
In addition to this, we can also mention that it stabilizes the inversion
 when the condition number of 
\begin_inset Formula $\left(\nabla^{2}f\right)_{x}$
\end_inset

 is too big, and that it can account for the estimation error when we estimate
 
\begin_inset Formula $\left(\nabla^{2}f\right)_{x}$
\end_inset

 using a minibatch of examples instead of using the true risk.
\end_layout

\begin_layout Subsection
Gauss-Newton approximation of the Hessian
\begin_inset CommandInset label
LatexCommand label
name "subsec:gauss-newton"

\end_inset


\end_layout

\begin_layout Standard
In the case of neural network optimization, the Hessian matrix we need to
 evaluate is the second derivative of the empirical risk, with respect to
 the parameters.
 A first remark that we can make, is that it is also composed of a sum of
 second order derivatives, to be computed at each example of the dataset:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{H} & = & \frac{\partial^{2}R}{\partial\theta^{2}}\\
 & = & \frac{\partial^{2}}{\partial\theta^{2}}\left\{ \frac{1}{n}\sum_{i}l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} \\
 & = & \frac{1}{n}\sum_{i}\frac{\partial^{2}}{\partial\theta^{2}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
By making use of the chain rule we can also give an expression for the second
 derivative of the loss, for a single example.
 We start with the first derivative:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\}  & = & \mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}\left(\frac{\partial}{\partial f}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} \right)^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathbf{J}$
\end_inset

 is the jacobian of the output of the network 
\begin_inset Formula $f$
\end_inset

 with respect to the parameters 
\begin_inset Formula $\theta$
\end_inset

.
 In this notation we made the dependance in 
\begin_inset Formula $\theta$
\end_inset

 of both parts of the product explicit.
 Note that both parts also take different values for each examples 
\begin_inset Formula $x_{i}$
\end_inset

.
 We now derive this expression once more to obtain the Hessian:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}}{\partial\theta^{2}}\left\{ l\left(f\left(x_{i},\theta\right),y_{i}\right)\right\}  & = & \underbrace{\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}\frac{\partial^{2}}{\partial f^{2}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} \mathbf{J}_{\theta}\left(x_{i},\theta\right)}_{G_{f}\left(x_{i},\theta\right)}\\
 &  & +\sum_{j}\left(\nabla^{2}f_{\theta}\left(x_{i}\right)_{j}\right)\left(\frac{\partial}{\partial f_{j}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} \right)^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $G_{f}\left(x_{i},\theta\right)$
\end_inset

 is called the Gauss-Newton (GN) approximation of the Hessian 
\begin_inset CommandInset citation
LatexCommand citep
key "schraudolph2002fast"

\end_inset

.
 The remainder is proportional to 
\begin_inset Formula $\frac{\partial}{\partial f_{j}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} $
\end_inset

.
 As we get closer to the optimum, this part will go toward 
\begin_inset Formula $0$
\end_inset

 as it is a first derivative, so the approximation will get more precise.
 At a minimum for 
\begin_inset Formula $l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)$
\end_inset

, we will have 
\begin_inset Formula $\frac{\partial^{2}}{\partial\theta^{2}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} =G_{f}\left(x_{i},\theta\right)$
\end_inset

 so it is a reasonable approximation to use in practice.
 Note that a minimum for the empirical risk 
\begin_inset Formula $R\left(\theta\right)$
\end_inset

 will not necessarily be a minimum for each example 
\begin_inset Formula $l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)$
\end_inset

, especially if the capacity of the neural network is not sufficient to
 model the data distribution.
\end_layout

\begin_layout Standard
In terms of computational cost, we can also note that we can compute the
 GN part using standard backpropagation, but this time of the jacobian.
 The other term is much more complicated because it involves a second derivative
 of a composed function.
\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $G_{f}\left(x_{i},\theta\right)$
\end_inset

 presents a much more convenient expression for common loss functions, as
 the second derivative of the loss with respect to the ouput of the network
 simplifies (table 
\begin_inset CommandInset ref
LatexCommand ref
reference "gn-loss"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="right" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Loss function
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{\partial^{2}}{\partial f_{\theta}^{2}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} $
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
quadratic error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{I}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for binary decision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{y_{i}}{\left(f_{\theta}\left(x_{i}\right)\right)^{2}}+\frac{1-y_{i}}{\left(1-f_{\theta}\left(x_{i}\right)\right)^{2}}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for multiclass classification
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{diag}\left(\frac{y_{i}}{\left(f_{\theta}\left(x_{i}\right)\right)^{2}}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "gn-loss"

\end_inset

Expressions for the Gauss-Newton approximation of the Hessian, for a single
 example 
\begin_inset Formula $x_{i}$
\end_inset

.
 For the cross entropy, all operations (division, squarred value) are elementwis
e, and the 
\begin_inset Formula $\text{diag}$
\end_inset

 function transforms a vector into a diagonal matrix with the vector values
 on its diagonal.
 Full derivation in appendix.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We finally give an expression for the Gauss-Newton approximation of the
 Hessian for the empirical risk:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
G_{f}\left(\theta\right) & = & \frac{1}{n}\sum_{i}\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}\frac{\partial^{2}}{\partial f^{2}}\left\{ l\left(f_{\theta}\left(x_{i}\right),y_{i}\right)\right\} \mathbf{J}_{\theta}\left(x_{i},\theta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We will show in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Decomposition-using-the"

\end_inset

 that this matrix can be factorized to design optimization algorithms adapted
 to the particular structure of neural networks.
\end_layout

\begin_layout Subsection
Block diagonal Hessian 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Block-diagonal-Hessian"

\end_inset


\end_layout

\begin_layout Standard
Apart from the issue of computing a value for the hessian matrix, a main
 limit is that we need to invert it.
 The hessian matrix has size 
\begin_inset Formula $n_{\text{parameters}}\times n_{\text{parameters}}$
\end_inset

, and the procedure used for numerically inverting a square matrix requires
 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

 operations so it rapidly becomes untractable for deep networks.
 A first approximation we make is by ignoring the interactions between the
 parameters of different layers.
 We make the hessian block diagonal, each block having the size of the number
 of parameters of the corresponding layer.
 An interesting property of block diagonal matrices is that we get the inverse
 by inverting every smaller block separately:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left(\nabla^{2}f\right)^{-1} & \approx & \left(\begin{array}{cccc}
\mathbf{H}_{1} & 0 & \cdots & 0\\
0 & \mathbf{H}_{2} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \mathbf{H}_{n}
\end{array}\right)^{-1}=\left(\begin{array}{cccc}
\mathbf{H}_{1}^{-1} & 0 & \cdots & 0\\
0 & \mathbf{H}_{2}^{-1} &  & \vdots\\
\vdots &  & \ddots & 0\\
0 & \cdots & 0 & \mathbf{H}_{n}^{-1}
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It also makes the implementation easier, as we can treat each block 
\begin_inset Quotes eld
\end_inset

locally
\begin_inset Quotes erd
\end_inset

 in the network, and use its inverse to update the gradient direction for
 the corresponding block (or layer) using 
\begin_inset Formula $\theta_{i}\leftarrow\theta_{i}-\lambda\mathbf{H}_{i}^{-1}\frac{\partial C}{\partial\theta_{i}}$
\end_inset

.
 We do not need to store a big 
\begin_inset Formula $n_{\text{parameters}}\times n_{\text{parameters}}$
\end_inset

 matrix.
\end_layout

\begin_layout Section
Natural gradient methods
\end_layout

\begin_layout Standard
We now present the natural gradient.
 We give some context and interpretation for the natural gradient, and we
 give its expression for neural networks.
\end_layout

\begin_layout Subsection
Fisher Information Matrix
\end_layout

\begin_layout Standard
The Fisher information matrix (FIM) is well used in statistics.
 In the context of machine learning, and in particular deep learning, we
 use its inverse as a preconditioner for the gradient descent algorithm,
 similarly to the Newton algorithm (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:newton-step"

\end_inset

).
 In this section, we show how the FIM can be derived from the KL divergence
 and how we get a better 
\begin_inset Quotes eld
\end_inset

natural
\begin_inset Quotes erd
\end_inset

 gradient using this information.
 Let us first write the definition of the KL divergence for 2 distributions
 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\text{KL}\left(p\parallel q\right) & = & \mathbb{E}_{p}\left[\log\left(\frac{p}{q}\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From a broad view, it is a non-negative quantity that measures how much
 
\begin_inset Formula $q$
\end_inset

 differs from 
\begin_inset Formula $p$
\end_inset

.
 In particular, 
\begin_inset Formula $\text{KL}\left(p\parallel q\right)=0$
\end_inset

 when 
\begin_inset Formula $p=q$
\end_inset

.
 Note that it is not symmetric, so it cannot be considered a true metric.
 We now use the probabilistic interpretation of neural networks, and consider
 that the examples from a dataset are drawn from a joint distribution 
\begin_inset Formula $p_{\theta}\left(x,y\right)=p_{\theta}\left(y|x\right)p\left(x\right)$
\end_inset

 where 
\begin_inset Formula $p_{\theta}\left(y|x\right)$
\end_inset

 is the function that we model with the neural network, and 
\begin_inset Formula $p\left(x\right)$
\end_inset

 is the input distribution.
\end_layout

\begin_layout Standard
One can view natural gradient as using KL divergence as a regularizer when
 doing gradient descent.
 We will denote by 
\begin_inset Formula $p_{\theta}\left(x,y\right)$
\end_inset

 a parametric model and 
\begin_inset Formula $\Delta\theta$
\end_inset

 a change in its parameter values.
 
\begin_inset Formula $\text{KL}\left(p_{\theta}\left(x,y\right)\parallel p_{\theta+\Delta\theta}\left(x,y\right)\right)$
\end_inset

 is used as our regularizer, so that each change 
\begin_inset Formula $\Delta\theta$
\end_inset

 gives a desired change magnitude in the distribution space that we control
 using a new hyperparameter.
 Instead of using the full expression for 
\begin_inset Formula $\text{KL}\left(p_{\theta}\left(x,y\right)\parallel p_{\theta+\Delta\theta}\left(x,y\right)\right)$
\end_inset

 we will use its second order Taylor series around 
\begin_inset Formula $\theta$
\end_inset

 (for full derivation see for instance 
\begin_inset CommandInset citation
LatexCommand cite
key "pascanu2013revisiting"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\text{KL}\left(p_{\theta}\left(x,y\right)\Vert p_{\theta+\Delta\theta}\left(x,y\right)\right) & = & \Delta\theta^{T}\mathbf{F}\Delta\theta+o(\left\Vert \Delta\theta\right\Vert _{2}^{2})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathbf{F}=\mathbb{E}_{p_{\theta}\left(x,y\right)}\left[\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)^{T}\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)\right]$
\end_inset

 is the Fisher information matrix (FIM), which can be used directly as a
 regularizer as we shall see shortly.
 Interestingly, even if the KL divergence is not symmetric, its second order
 approximation is, as we also have 
\begin_inset Formula $\text{KL}\left(p_{\theta+\Delta\theta}\Vert p_{\theta}\right)=\Delta\theta^{T}\mathbf{F}\Delta\theta+o(\left\Vert \Delta\theta\right\Vert _{2}^{2})$
\end_inset

 (note that we swapped the terms in the KL).
\end_layout

\begin_layout Subsection
Natural gradient descent
\end_layout

\begin_layout Standard
As noted in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-learning-rate"

\end_inset

, the parameter update vector used in ordinary gradient descent can be obtained
 as the result of the following minimization problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta\theta^{*} & = & \text{argmin}_{\Delta\theta}\Delta\theta^{T}\nabla_{\theta}R+\frac{1}{2\lambda}\Delta\theta^{T}\Delta\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $R$
\end_inset

 is the empirical risk, as previously defined in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:empirical-risk"

\end_inset

 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Empirical-risk-and"

\end_inset

.
\end_layout

\begin_layout Standard
This expression can be easily solved giving the usual gradient descent update
 
\begin_inset Formula $\Delta\theta=-\lambda\nabla_{\theta}R$
\end_inset

.
 The parameter 
\begin_inset Formula $\lambda$
\end_inset

 is the usual learning rate, and controls how much each parameter can change.
 We will now add a new regularizer using the FIM, and transform the minimization
 problem into:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta\theta^{*} & = & \text{argmin}_{\Delta\theta}\Delta\theta^{T}\nabla_{\theta}R+\frac{1}{2\lambda}\Delta\theta^{T}\Delta\theta+\frac{1}{2\epsilon}\Delta\theta^{T}\mathbf{F}\Delta\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We now constrain our gradient step to be small in term of change of parameter
 values, and also to be small in term of how much the resulting distribution
 changes.
 This expression can be solved to give 
\begin_inset Formula $\Delta\theta^{*}=-\lambda\left(\mathbf{I}+\frac{\lambda}{\epsilon}\mathbf{F}\right)^{-1}\nabla_{\theta}R$
\end_inset

.
 This expression also gives an insight for the role of 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\epsilon$
\end_inset

, which control 2 different but related quantities expressed by our constraints.
 This new update is called the natural gradient 
\begin_inset CommandInset citation
LatexCommand citep
key "amari1998natural"

\end_inset

.
\end_layout

\begin_layout Subsection
An expression for the FIM using jacobians
\end_layout

\begin_layout Standard
Using the probabilistic interpretation of neural networks, the FIM can be
 expressed 
\begin_inset Formula $\mathbf{F}=\mathbb{E}_{p_{\theta}\left(x,y\right)}\left[\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)^{T}\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)\right]$
\end_inset

 which simplifies in:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{F} & = & \mathbb{E}_{p_{\theta}\left(x,y\right)}\left[\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)^{T}\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)\right]\\
 & = & \mathbb{E}_{x\sim p\left(x\right)}\left[\mathbb{E}_{y\sim p_{\theta}\left(y|x\right)}\left[\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)^{T}\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)\right]\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $\log p_{\theta}\left(x,y\right)=\log p_{\theta}\left(y|x\right)+\log p\left(x\right)$
\end_inset

 and 
\begin_inset Formula $p\left(x\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

 then this can be further simplified in:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{F} & = & \mathbb{E}_{x\sim p\left(x\right)}\left[\mathbb{E}_{y\sim p_{\theta}\left(y|x\right)}\left[\left(\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\right)^{T}\left(\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\right)\right]\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Interestingly, for the usual distributions expressed by neural networks,
 we can derive an exact expression for the inner expectation.
 The FIM takes the following simple form as shown by 
\begin_inset CommandInset citation
LatexCommand cite
key "pascanu2013revisiting"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{F} & = & \mathbb{E}_{x\sim p\left(x\right)}\left[\mathbf{J}_{\theta}\left(x,\theta\right)^{T}D\left(f_{\theta}\left(x\right)\right)\mathbf{J}_{\theta}\left(x,\theta\right)^{T}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The values for 
\begin_inset Formula $x$
\end_inset

 are drawn from the data generating distribution 
\begin_inset Formula $p$
\end_inset

.
 Similarly to section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

, the notation 
\begin_inset Formula $\mathbf{J}_{\theta}\left(x,\theta\right)^{T}$
\end_inset

 is used for the jacobian of the output of the network (i.e.
 the probability expressed at a given 
\begin_inset Formula $x$
\end_inset

 : 
\begin_inset Formula $p\left(y\mid x\right)$
\end_inset

), with respect to the parameters.
 In other words, it measures how much the output of the network 
\begin_inset Formula $p\left(y|x\right)$
\end_inset

 will change for a given 
\begin_inset Formula $x$
\end_inset

 if we change the parameters.
 For usual loss functions, 
\begin_inset Formula $D$
\end_inset

 is a diagonal matrix with non negative diagonal terms, and depends of the
 cost function used.
 For the quadratic loss it is the identity (table 
\begin_inset CommandInset ref
LatexCommand ref
reference "expressions-fisher"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="right" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Loss function
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D\left(f_{\theta}\left(x\right)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
quadratic error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{I}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for binary decision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{1}{f_{\theta}\left(x\right)\left(1-f_{\theta}\left(x\right)\right)}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for multiclass classification
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{diag}\left(\frac{1}{f_{\theta}\left(x\right)}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "expressions-fisher"

\end_inset

Expressions for the FIM, for a single sample 
\begin_inset Formula $x_{i}$
\end_inset

.
 For the cross entropy, all operations (division, squarred value) are elementwis
e, and the 
\begin_inset Formula $\text{diag}$
\end_inset

 function transforms a vector into a diagonal matrix with the vector values
 on its diagonal.
 Full derivation in appendix.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Approximating the FIM
\end_layout

\begin_layout Standard
Similarly to the Hessian, the FIM is difficult to compute because of its
 size (
\begin_inset Formula $n_{parameters}\times n_{parameters}$
\end_inset

) and because in general we do not have an expression for 
\begin_inset Formula $q$
\end_inset

 but only samples from a training dataset.
 As for Newton, we can make the two following approximations:
\end_layout

\begin_layout Itemize
A first approximation that we can make is by ignoring the interactions between
 layers.
 In this case the FIM takes the form of a block diagonal matrix, where each
 block is a square matrix which has the size of the parameters of a layer.
 For a neural network with 
\begin_inset Formula $n_{layers}$
\end_inset

 layers this reduces the FIM into 
\begin_inset Formula $n_{layers}$
\end_inset

 smaller matrices.
 We will denote by 
\begin_inset Formula $\mathbf{F}_{i}$
\end_inset

 the block corresponding to layer 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
A second common approximation we make in practice is to use the empirical
 FIM for a training dataset of 
\begin_inset Formula $n$
\end_inset

 examples 
\begin_inset Formula $x_{i}$
\end_inset

: 
\begin_inset Formula $\mathbf{F}=\frac{1}{n}\sum_{i}\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}D\left(f_{\theta}\left(x\right)\right)\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}$
\end_inset

.
\end_layout

\begin_layout Section
Gauss-Newton and Fisher share a very similar structure
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Subsection
Relation between the FIM and the GN approximation of the Hessian
\end_layout

\begin_layout Standard
We have just shown that the Gauss-Newton of the empirical risk with respect
 to the parameters, and the Fisher Information Matrix share a similar structure
 that is composed of the jacobians of the output of the network with respect
 to the parameters, and a symmetric matrix:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{1}{n}\sum_{i}\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}D\left(f_{\theta}\left(x_{i}\right)\underbrace{,y_{i}}_{\text{opt}}\right)\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}\label{eq:general_form}
\end{equation}

\end_inset

The main difference is in this symmetric matrix 
\begin_inset Formula $D\left(f_{\theta}\left(x\right),y\right)$
\end_inset

.
 For Fisher methods it does not depend on any true target and it is just
 an intrinsic property of a neural network, associated with an input distributio
n.
 We can thus remove the 
\begin_inset Formula $y$
\end_inset

: 
\begin_inset Formula $D\left(f_{\theta}\left(x\right)\right)$
\end_inset

.
 In the case of the GN matrix it depends on the true target 
\begin_inset Formula $y$
\end_inset

 in general, with a notable exception for the quadratic error.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="right" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="right" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gauss-Newton
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fisher
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D\left(f_{\theta}\left(x\right),y\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D\left(f_{\theta}\left(x\right)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
quadratic error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{I}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{I}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for binary decision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{y_{i}}{\left(f_{\theta}\left(x_{i}\right)\right)^{2}}+\frac{1-y_{i}}{\left(1-f_{\theta}\left(x_{i}\right)\right)^{2}}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\frac{1}{f_{\theta}\left(x\right)\left(1-f_{\theta}\left(x\right)\right)}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cross entropy for multiclass classification
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{diag}\left(\frac{y_{i}}{\left(f_{\theta}\left(x_{i}\right)\right)^{2}}\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{diag}\left(\frac{1}{f_{\theta}\left(x\right)}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "gn-fim"

\end_inset

Expressions for the middle term 
\begin_inset Formula $D\left(f_{\theta}\left(x\right),y\right)$
\end_inset

 and 
\begin_inset Formula $D\left(f_{\theta}\left(x\right)\right)$
\end_inset

 for GN and FIM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
An original interpretation from the output of the network
\end_layout

\begin_layout Standard
In the cases where 
\begin_inset Formula $D$
\end_inset

 is a diagonal matrix (i.e.
 cross entropy and quadratic error, see eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:general_form"

\end_inset

 and table 
\begin_inset CommandInset ref
LatexCommand ref
reference "gn-fim"

\end_inset

), we can rewrite both GN and FIM matrices applied to an update as a norm
 in the space of the output of the network:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta\theta^{*} & = & \underset{\Delta\theta}{\text{argmin}}\left(\nabla R\right)_{\theta}^{T}\Delta\theta+\frac{1}{2}\Delta\theta^{T}\frac{1}{n}\sum_{i}\mathbf{J}_{\theta}\left(x_{i},\theta\right)^{T}D\left(f\left(x_{i},\theta\right),y_{i}\right)\mathbf{J}_{\theta}\left(x_{i},\theta\right)\Delta\theta+\frac{1}{2\lambda}\Delta\theta^{T}\Delta\theta\\
 & = & \underset{\Delta\theta}{\text{argmin}}\left(\nabla R\right)_{\theta}^{T}\Delta\theta+\frac{1}{2}\frac{1}{n}\sum_{i}\left\langle D\left(f\left(x_{i},\theta\right),y_{i}\right)\mathbf{J}_{\theta}\left(x_{i},\theta\right)\Delta\theta,\mathbf{J}_{\theta}\left(x_{i},\theta\right)\Delta\theta\right\rangle +\frac{1}{2\lambda}\Delta\theta^{T}\Delta\theta\\
 & = & \underset{\Delta\theta}{\text{argmin}}\left(\nabla R\right)_{\theta}^{T}\Delta\theta+\frac{1}{2}\frac{1}{n}\sum_{i}\left\langle D\left(f\left(x_{i},\theta\right),y_{i}\right)\Delta f_{\theta}\left(x_{i},\Delta\theta\right),\Delta f_{\theta}\left(x_{i},\Delta\theta\right)\right\rangle +\frac{1}{2\lambda}\Delta\theta^{T}\Delta\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We denoted by 
\begin_inset Formula $\Delta f_{\theta}\left(x_{i},\Delta\theta\right)=\mathbf{J}_{\theta}\left(x_{i},\theta\right)\Delta\theta$
\end_inset

 a first order approximation of the change in the value of 
\begin_inset Formula $f_{\theta}\left(x_{i}\right)$
\end_inset

 induced by a change 
\begin_inset Formula $\Delta\theta$
\end_inset

 of 
\begin_inset Formula $\theta$
\end_inset

 for example 
\begin_inset Formula $x_{i}$
\end_inset

.
 With this decomposition we can understand GN and natural gradient as being
 a regularizer for each example, using the metrics 
\begin_inset Formula $D\left(f\left(x_{i},\theta\right),y_{i}\right)$
\end_inset

 that depends on the considered example.
 We regularize for several undesirable effect:
\end_layout

\begin_layout Itemize
We ensure that 
\begin_inset Formula $\Delta f_{\theta}\left(x_{i},\Delta\theta\right)$
\end_inset

 cannot take a large value.
 This distributes the effect of the update evenly between examples, instead
 of having a large change in 
\begin_inset Formula $f_{\theta}\left(x_{i}\right)$
\end_inset

 for a single example, and smaller changes for others.
\end_layout

\begin_layout Itemize
We weight this changes using 
\begin_inset Formula $D\left(f\left(x_{i},\theta\right),y_{i}\right)$
\end_inset

.
 For the cross entropies for instance we observe that this term grows with
 
\begin_inset Formula $\frac{1}{f_{\theta}\left(x\right)}$
\end_inset

 (the vector of probabilities of each class).
 If this vector is not evenly distributed, that is if for one class 
\begin_inset Formula $t$
\end_inset

 we have a larger value of 
\begin_inset Formula $\left(f_{\theta}\left(x\right)\right)_{t}$
\end_inset

, all other values will be close to 
\begin_inset Formula $0$
\end_inset

, which means that 
\begin_inset Formula $\left(\frac{1}{f_{\theta}\left(x\right)}\right)_{i\neq t}$
\end_inset

 will take a very large value.
 In this case we put more weight on the examples for which our model is
 more confident of its prediction.
\end_layout

\begin_layout Section
A cheaper Gauss-Newton matrix for cross-entropy
\begin_inset CommandInset label
LatexCommand label
name "sec:gn-trick-cheap"

\end_inset


\end_layout

\begin_layout Standard
We now present a computational trick for computing the Gauss-Newton in the
 case of cross-entropy.
 Interestingly, in the expression of Gauss-Newton for log losses we have
 the unexpected equivalence 
\begin_inset Formula $\left(\frac{\partial^{2}l}{\partial f_{\theta}^{2}}\right)_{tt}=-\left(\frac{\partial l}{\partial f_{\theta}}\right)_{t}^{2}$
\end_inset

 for the true class 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $\left(\frac{\partial^{2}l}{\partial f_{\theta}^{2}}\right)_{ii}=0=-\left(\frac{\partial l}{\partial f_{\theta}}\right)_{t}^{2}$
\end_inset

 when 
\begin_inset Formula $i\neq t$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
l\left(f_{\theta}\left(x\right),y\right) & = & -\sum_{i}y_{i}\log\left(\left(f_{\theta}\left(x\right)\right)_{i}\right)\\
\left(\frac{\partial}{\partial f_{\theta}}l\left(f_{\theta}\left(x\right),y\right)\right)_{t} & = & -\frac{1}{\left(f_{\theta}\left(x\right)\right)_{t}}\\
\left(\frac{\partial^{2}}{\partial f_{\theta}^{2}}l\left(f_{\theta}\left(x\right),y\right)\right)_{tt} & = & \frac{1}{\left(f_{\theta}\left(x\right)\right)_{t}^{2}}=\left(\frac{\partial}{\partial f_{\theta}}l\left(f_{\theta}\left(x\right),y\right)\right)_{t}^{2}
\end{eqnarray*}

\end_inset

The reason is that the second derivative of the 
\begin_inset Formula $\log$
\end_inset

 function (
\begin_inset Formula $x\longmapsto-\frac{1}{x^{2}}$
\end_inset

) is minus the square of its first derivative (
\begin_inset Formula $x\longmapsto\frac{1}{x}$
\end_inset

).
 Getting back to the expression of the GN matrix (here for a single example),
 we can combine the second derivative with the jacobians and get a simple
 expression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
G_{f}\left(x,\theta\right) & = & \mathbf{J}_{\theta}\left(x,\theta\right)^{T}\frac{\partial^{2}}{\partial f^{2}}\left\{ l\left(f_{\theta}\left(x\right),y\right)\right\} \mathbf{J}_{\theta}\left(x,\theta\right)\nonumber \\
 & = & \mathbf{J}_{\theta}\left(x,\theta\right)^{T}\left(\frac{\partial l}{\partial f_{\theta}}\right)^{T}\left(\frac{\partial l}{\partial f_{\theta}}\right)\mathbf{J}_{\theta}\left(x,\theta\right)\nonumber \\
 & = & \nabla_{\theta}l\left(f_{\theta}\left(x\right),y\right)\nabla_{\theta}l\left(f_{\theta}\left(x\right),y\right)^{T}\label{eq:simplified-gn}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
This gradient in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:simplified-gn"

\end_inset

 is the exact same as the gradient used to compute the update in gradient
 descent.
 So for no additional cost we get the expression of the GN matrix.
 Note that we still need to invert it, which is a 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

 operation in the size of the matrix.
\end_layout

\begin_layout Standard
This gives an explanation of the outer product metrics mentionned in 
\begin_inset CommandInset citation
LatexCommand cite
key "ollivier2013riemannian"

\end_inset

.
 To the best of our knowledge this result has not been published before,
 which is very suprising as it gives a very cheap way of computing the GN
 matrix.
\end_layout

\begin_layout Chapter
Experimental setup
\end_layout

\begin_layout Standard
In order to be able to assess the performance of the ideas and algorithms
 in the next chapters, we now present our experimental setup.
\end_layout

\begin_layout Section
Biased random search
\begin_inset CommandInset label
LatexCommand label
name "biased-random-search"

\end_inset


\end_layout

\begin_layout Standard
While comparing optimization techniques on real tasks, we found that it
 was very difficult to provide a fair benchmark, because a slight change
 in a hyperparameter value can drastically improve or alter its performance.
 Indeed, with simple hyperparameter adjustments, we were often able to improve
 the benchmarks reported as state-of-the-art in previous applied optimization
 papers.
\end_layout

\begin_layout Standard
More sophisticated approaches to automatic hyperparameter tuning exist,
 such as Bayesian optimization (see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "snoek2012practical"

\end_inset

).
 While hyperparameter tuning is an active research area on its own, it is
 not the focus of our work.
 We just use a simple technique that refines random search, by allocating
 more ressources to explore regions in the hyperparameter space that are
 more likely to give a good performance.
 We now describe this method that we call 
\series bold
biased random search
\series default
 (algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "biased_random_search_algo"

\end_inset

), and we validate its performance using a simple experiment.
\end_layout

\begin_layout Standard
During the hyperparameter tuning procedure, we create a model of our cost
 landscape in the space of hyperparameters.
 As the number of experiments grows, the cost landscape is refined.
 We use this estimated cost landscape to bias our random search, so that
 regions of the hyperparameter space that are expected to provide a better
 result will have higher probability of being explored.
 In practice, we use a simple 1-nearest neighbor regressor 
\begin_inset CommandInset citation
LatexCommand citep
key "altman1992introduction"

\end_inset

 to model the cost landscape.
 Using the estimated value of the criteria 
\begin_inset Formula $c_{estimate}$
\end_inset

, we decide to keep the sampled value with probability 
\begin_inset Formula $p$
\end_inset

, or otherwise we reject the value and sample a new one, and so on until
 we get a value that is not rejected, which will be our next experiment.
 We can choose the value of 
\begin_inset Formula $p$
\end_inset

 using different heuristics, in practice we use 
\begin_inset Formula $p=\frac{c_{max}-c_{estimate}}{c_{max}-c_{min}}$
\end_inset

 (in this notation, the criteria needs to be minimized) where 
\begin_inset Formula $c_{max}$
\end_inset

 and 
\begin_inset Formula $c_{min}$
\end_inset

 are defined as the current maximum and minimum value that we have obtained
 so far.
 This value for 
\begin_inset Formula $p$
\end_inset

 will almost surely reject values that are close to the worst experiments,
 and almost surely accept values that are close to the best experiments.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\begin_layout Plain Layout


\backslash
Require{$
\backslash
mathcal{M}$ used to model the cost landscape in the space of HP}
\end_layout

\begin_layout Plain Layout


\backslash
Require{$
\backslash
mathcal{D}$ the domain of HP that we will explore}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
mathcal{H} 
\backslash
leftarrow 
\backslash
left[ 
\backslash
, 
\backslash
right]$}
\backslash
Comment{History of explored HP values and corresponding result}
\end_layout

\begin_layout Plain Layout


\backslash
While{not converged}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
text{rejected}
\backslash
leftarrow
\backslash
text{true}$}
\end_layout

\begin_layout Plain Layout


\backslash
While{$
\backslash
text{rejected}$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$a
\backslash
sim U
\backslash
left(
\backslash
mathcal{D}
\backslash
right)$}
\backslash
Comment{Sample values for HP}
\end_layout

\begin_layout Plain Layout


\backslash
State{$c_{estimate} 
\backslash
leftarrow 
\backslash
mathcal{M} 
\backslash
left( 
\backslash
mathcal{H}, a 
\backslash
right) $}
\backslash
Comment{Estimate $c$ for HP $a$ using history $
\backslash
mathcal{H}$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$p
\backslash
leftarrow
\backslash
frac{c_{max}-c_{estimate}}{c_{max}-c_{min}}$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$x
\backslash
sim U
\backslash
left( 
\backslash
left[ 0, 1
\backslash
right] 
\backslash
right)$}
\end_layout

\begin_layout Plain Layout


\backslash
If{$x < p$} 
\backslash
State {$
\backslash
text{rejected}
\backslash
leftarrow
\backslash
text{false}$}
\backslash
EndIf
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
State{$result 
\backslash
leftarrow run
\backslash
left( a 
\backslash
right)$}
\backslash
Comment{Run experiment with HP values $a$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
mathcal{H} 
\backslash
leftarrow  
\backslash
mathcal{H} + 
\backslash
left( a, result 
\backslash
right)$}
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Biased random search
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "biased_random_search_algo"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To assess the performance of biased random search we ran 100 searches of
 100 experiments on a simple task where we tuned 2 hyperparameters.
 We observe that it consistently finds comparable or better results than
 standard random search.
 We report the results in the following table and in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "hptune-comparison"

\end_inset

 (lower is better).
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="right" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HP tuning procedure
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Grid search
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
27.23
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.42
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random search
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
27.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.28
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="right" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Biased random search
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
26.61
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.13
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "hptune-comparison"

\end_inset

 we can clearly see that with biased random search the majority of experiments
 is launched around the region with best performing hyperparameter values.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/hp_search.pdf
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of hyperparameter tuning methods.
 On the left a grid search, in the middle a random search and on the right
 a biased random search.
 Each experiment consisted in 100 iterations of SGD from a randomly initialized
 network.
 We tune 2 hyperparameters on the x and y axis (what they represent is not
 relevant here).
 The color scale represents the final loss attained after a fixed number
 of iterations.
 The best experiments are in blue, the worst experiments in yellow.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "hptune-comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
A standard benchmark: Autoencoding written digits
\begin_inset CommandInset label
LatexCommand label
name "sec:autoencoder-benchmark"

\end_inset


\end_layout

\begin_layout Standard
We now describe the main benchmark that we will be using in the rest of
 this document.
 The dataset MNIST 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun2010mnist"

\end_inset

 is composed of 60.000 
\begin_inset Formula $28\times28$
\end_inset

 grayscale images of handwritten digits, and the corresponding value of
 the digit that is represented in the image.
 For this benchmark, we use an autoencoder (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Autoencoders"

\end_inset

) with layer sizes 
\begin_inset Formula $\left\{ 784,1000,500,250,30,250,500,1000,784\right\} $
\end_inset

.
 The autoencoder encodes the input image into a vector of size 
\begin_inset Formula $30$
\end_inset

, and then decodes it to reconstruct the original image.
 We use the reconstruction error 
\begin_inset Formula $l\left(f\left(x\right),y\right)=\left\Vert f\left(x\right)-y\right\Vert _{2}^{2}$
\end_inset

.
 The benchmark consists in minimizing the empirical risk over the train
 set after a fixed time on the same architecture.
\end_layout

\begin_layout Standard
This benchmark has a long history in the neural network optimization litterature
 
\begin_inset CommandInset citation
LatexCommand citep
key "hinton2006reducing,martens2010deep,martens2015optimizing,desjardins2015natural"

\end_inset

.
 To assess the performance of an algorithm, we can use 2 metrics: the empirical
 risk after a given number of iterations of the algorithm, and the empirical
 risk after a fixed elapsed time for a given computer.
 In real world tasks, the latter is more useful.
 It gives a better understanding of the trade-off between a more complex
 update that takes longer to compute and gives a better improvement, and
 a fast update that gives a small improvement, but that can be iterated
 several times in the meantime.
\end_layout

\begin_layout Standard
The limits of the benchmark are many.
 In particular the fact that the state of the art papers in computer vision
 do not use MLPs and sigmoid activations but rather variants of mixed convolutio
nal networks and residual connections, and variants of ReLU activations.
 Another limit is in the use of the quadratic loss.
 Nonetheless, we still use this benchmark as it is used by several other
 papers which allows for a fair comparison, and because it is reasonably
 deep (8 layers) and wide (the biggest weight matrix has size 
\begin_inset Formula $1000\times784$
\end_inset

).
\end_layout

\begin_layout Section
A classification task on an image dataset
\end_layout

\begin_layout Standard
The second benchmark that we use is a multilayer perceptron with rectifier
 activation functions, trained to recognize images amongst 10 classes on
 the CIFAR-10 dataset 
\begin_inset CommandInset citation
LatexCommand citep
key "torralba200880"

\end_inset

.
 It is composed of 60.000 
\begin_inset Formula $32\times32$
\end_inset

 color images, meaning that each image is composed of 
\begin_inset Formula $32*32*3=3072$
\end_inset

 pixels.
 The network has 8 hidden layers of size 100 making it reasonably deep but
 still fast to train in order to experiment with many algorithms.
 We train it using multiclass cross entropy.
\end_layout

\begin_layout Standard
This architecture is far from producing state of the art results for this
 task.
 In particular, it starts overfitting for a very small number of updates.
 Instead, we use it to compare optimization algorithms, which means that
 we are more interested in its performance on the train set.
 If we were interested in generalization performance, we could add regularizatio
n to better condition the optimization problem.
\end_layout

\begin_layout Chapter
Proof of concept: Evolution of the backpropagated gradient while updating
 the parameters
\end_layout

\begin_layout Standard
In this section, we present a prototype technique to account for the interaction
s between parameters of different layers while computing updates.
 While we could not come up with an efficient algorithm to implement this
 technique, early results show that it could be useful in deep networks.
\end_layout

\begin_layout Section
How is the gradient modified when changing the value of the parameters of
 a layer
\end_layout

\begin_layout Standard
In usual gradient descent, we compute the gradient of the empirical risk
 with respect to all parameters, then we update all parameters at once.
 But it is not really clear that in doing so we will actually decrease the
 value of the empirical risk.
 The direction provided by the gradient is locally a descent direction.
 But how much locally? As we increase the number of parameters, we might
 need to use an optimal learning rate that is even too small to make any
 perceptible progress overall.
\end_layout

\begin_layout Standard
We experiment with a technique that aims at improving the update directions.
 It is a modification of the gradient, that can be computed following the
 same chaining of operations as computing the gradient using forward and
 backward propagation, but requiring more computation.
\end_layout

\begin_layout Standard
We now present the technique, and describe the derivation of this new update
 direction.
\end_layout

\begin_layout Standard
Since we are using backpropagation, then the process of getting the partial
 derivatives is sequential, that is, we get the derivatives of the top layers
 first, and afterwards we get the derivatives of the bottom layers.
 Now suppose that we apply the update for the parameters of the top layers
 before backpropagating through them.
 We are now optimizing an updated function.
 Instead of using the backropagated gradient that we have obtained so far,
 we could reestimate the forward and backward pass for this updated function
 and get a new backpropagated signal.
 To a certain extent it could be seen as doing coordinate descent, but instead
 of optimizing each parameter separately, we group them by layer, and we
 optimize each layer separately.
\end_layout

\begin_layout Standard
What we propose lies somewhere in-between: instead of recomputing the whole
 forward pass and the backward pass up until the current layer, we estimate
 how updating the parameters of the top layers will modify the backpropagated
 signal.
\end_layout

\begin_layout Standard
To illustrate the idea, we focus on the transformation computed by a single
 layer:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
h_{l} & = & f\left(a_{l}\right)\\
a_{l} & = & Wh_{l-1}+b
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Using backpropagation through this layer we get the partial derivative of
 the loss function 
\begin_inset Formula $\ell$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial\ell}{\partial h_{l-1}} & = & \frac{\partial\ell}{\partial a_{l}}\frac{\partial a_{l}}{\partial h_{l-1}}\nonumber \\
 & = & \frac{\partial\ell}{\partial a_{l}}W\label{eq:dldhl}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
This partial derivative is thus a function of 
\begin_inset Formula $W$
\end_inset

 in an explicit way.
 It is also a function of 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 through the other term 
\begin_inset Formula $\frac{\partial\ell}{\partial a_{l}}$
\end_inset

.
 Can we get an update expression for 
\begin_inset Formula $\frac{\partial\ell}{\partial h_{l-1}}\left(W+\Delta W,b+\Delta b\right)$
\end_inset

? This gradient 
\begin_inset Formula $\frac{\partial\ell}{\partial h_{l-1}}$
\end_inset

 is in turn used for computing the gradient of each preceding (deeper) layer
 using the chain rule.
 By obtaining a more accurate value for 
\begin_inset Formula $\frac{\partial\ell}{\partial h_{l-1}}\left(W+\Delta W,b+\Delta b\right)$
\end_inset

 we expect to improve all consecutive parameter updates.
\end_layout

\begin_layout Section
A first order update of a first order derivative
\end_layout

\begin_layout Standard
We now focus on a single layer.
 Suppose that we update 
\begin_inset Formula $W\leftarrow W+\Delta W$
\end_inset

 and 
\begin_inset Formula $b\leftarrow b+\Delta b$
\end_inset

.
 We want to estimate 
\begin_inset Formula $\frac{\partial\ell}{\partial h_{l-1}}\left(W+\Delta W,b+\Delta b\right)$
\end_inset

, using a first order approximation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial\ell}{\partial h_{l-1}}\left(W+\Delta W,b+\Delta b\right)\approx & \frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)+\left(\frac{\partial}{\partial\text{vec}\left(W\right)}\left\{ \frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)\right\} \text{vec}\left(\Delta W\right)\right)^{T}\\
 & +\left(\frac{\partial}{\partial b}\left\{ \frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)\right\} \Delta b\right)^{T}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We used the vec operator in order to have a matrix expression for the second
 derivative with respect to 
\begin_inset Formula $W$
\end_inset

.
 Using the expression in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dldhl"

\end_inset

 for 
\begin_inset Formula $\frac{\partial\ell}{\partial h_{l-1}}$
\end_inset

 we see that it requires deriving 2 terms: 
\begin_inset Formula $\frac{\partial\ell}{\partial a_{l}}$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\text{vec}\left(W\right)}\left\{ \frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)\right\} \text{vec}\left(\Delta W\right) & = & \frac{\partial}{\partial\text{vec}\left(W\right)}\left\{ \frac{\partial\ell}{\partial a_{l}}W\right\} \text{vec}\left(\Delta W\right)\\
 & = & \frac{\partial}{\partial\text{vec}\left(W\right)}\left\{ \text{vec}\left(W\right)^{T}\left(\mathbf{I}\otimes\left(\frac{\partial\ell}{\partial a_{l}}\right)^{T}\right)\right\} \text{vec}\left(\Delta W\right)\\
 & = & \left(\left(\mathbf{I}\otimes\left(\frac{\partial\ell}{\partial a_{l}}\right)\right)+W^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}\frac{\partial a_{l}}{\partial\text{vec}\left(W\right)}\right)\text{vec}\left(\Delta W\right)\\
 & = & \left(\left(\mathbf{I}\otimes\left(\frac{\partial\ell}{\partial a_{l}}\right)\right)+W^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}\left(h_{l-1}^{T}\otimes\mathbf{I}\right)\right)\text{vec}\left(\Delta W\right)\\
 & = & \frac{\partial\ell}{\partial a_{l}}\Delta W+W^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}\Delta Wh_{l-1}\\
\frac{\partial}{\partial b}\left\{ \frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)\right\} \Delta b & = & \frac{\partial}{\partial b}\left\{ \frac{\partial\ell}{\partial a_{l}}W\right\} \Delta b\\
 & = & W^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}\frac{\partial a_{l}}{\partial b}\Delta b\\
 & = & W^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}\Delta b
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Overall we get the following form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\ell}{\partial h_{l-1}}\left(W+\Delta W,b+\Delta b\right) & \approx & \frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)+\Delta W^{T}\left(\frac{\partial\ell}{\partial a_{l}}\right)^{T}+\left(\Delta Wh_{l-1}+\Delta b\right)^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}W
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This can be further simplified by using the chain rule for 
\begin_inset Formula $\frac{\partial\ell}{\partial h_{l-1}}\left(W,b\right)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\ell}{\partial h_{l-1}}\left(W+\Delta W,b+\Delta b\right) & \approx & W^{T}\left(\frac{\partial\ell}{\partial a_{l}}\right)^{T}+\Delta W^{T}\left(\frac{\partial\ell}{\partial a_{l}}\right)^{T}+\left(\Delta Wh_{l-1}+\Delta b\right)^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}W\\
 & = & \left(W+\Delta W\right)^{T}\left(\frac{\partial\ell}{\partial a_{l}}\right)^{T}+\left(\Delta Wh_{l-1}+\Delta b\right)^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}W
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This approximate new backpropagated gradient thus decomposes into 2 terms.
 The first one 
\begin_inset Formula $\left(W+\Delta W\right)^{T}\left(\frac{\partial\ell}{\partial a_{l}}\right)^{T}$
\end_inset

 is very similar to the usual backpropagated gradient but accounts for the
 updated value of 
\begin_inset Formula $W$
\end_inset

.
 The second term 
\begin_inset Formula $\left(\Delta Wh_{l-1}+\Delta b\right)^{T}\frac{\partial^{2}\ell}{\partial a_{l}^{2}}W$
\end_inset

 measures a global change in the gradient.
 It requires using the Hessian of the preactivation.
 This Hessian estimates how much will the gradient 
\begin_inset Formula $\frac{\partial\ell}{\partial a_{l}}$
\end_inset

 change when the value of 
\begin_inset Formula $a_{l}$
\end_inset

 changes, which is exactly what is computed by 
\begin_inset Formula $\Delta Wh_{l-1}+\Delta b$
\end_inset

.
\end_layout

\begin_layout Standard
The Hessian 
\begin_inset Formula $\frac{\partial^{2}\ell}{\partial a_{l}^{2}}$
\end_inset

 is currently the limiting factor for this technique to be truly efficient.
 It must be computed for every example.
 It has the size of the number of output units for this layer, and must
 be computed for each example.
 It is thus smaller compared to the true Hessian that we use in Newton's
 method, which has the size 
\begin_inset Formula $n_{parameters}\times n_{parameters}$
\end_inset

 and also requires computing a Hessian for each example that is summed in
 order to get the Hessian of the empirical risk.
\end_layout

\begin_layout Standard
This Hessian 
\begin_inset Formula $\frac{\partial^{2}\ell}{\partial a_{l}^{2}}$
\end_inset

 can also be approximated by Gauss-Newton (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

): 
\begin_inset Formula $\frac{\partial^{2}\ell}{\partial a_{l}^{2}}\approx\mathbf{J}_{a_{l}}{}^{T}\frac{\partial^{2}\ell}{\partial f^{2}}\mathbf{J}_{a_{l}}$
\end_inset

 where the jacobians 
\begin_inset Formula $\mathbf{J}_{a_{l}}=\frac{\partial f}{\partial a_{l}}$
\end_inset

 are the jacobians of the output of the network with respect to the preactivatio
n of the current layer 
\begin_inset Formula $a_{l}$
\end_inset

.
 This approximation drastically reduces the computation required during
 backpropagation, and experimentally proves to be equally efficient as using
 the true Hessian.
\end_layout

\begin_layout Section
Updated backpropagation algorithm
\end_layout

\begin_layout Standard
Starting from the usual gradient computation, we propose to replace the
 backpropagation step by backpropagating this updated gradient.
 We call this technique updated backpropagation (UBP), and we describe a
 simple algorithm that implements it (algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:updbprop"

\end_inset

).
\end_layout

\begin_layout Standard
In addition to backpropagating the gradient, we must also backpropagate
 the jacobians 
\begin_inset Formula $\frac{\partial f}{\partial a_{l}}$
\end_inset

 for each example.
 This can be done in a similar fashion than the gradient, the main difference
 being that the jacobians are matrices for each example, whereas the gradients
 are vectors.
 The size of the jacobians grows with the size of the output of the network.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\begin_layout Plain Layout


\backslash
Require{$
\backslash
mathcal{D}$ a minibatch of $n$ examples}
\end_layout

\begin_layout Plain Layout


\backslash
Require{$
\backslash
lambda$ learning rate}
\end_layout

\begin_layout Plain Layout


\backslash
State{$dh_i 
\backslash
leftarrow 
\backslash
left( 
\backslash
frac{
\backslash
partial l}{
\backslash
partial f_i} 
\backslash
right)^T$}
\backslash
Comment{Gradient of the loss w.r.t the output of the NN for examples $i$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$J_i 
\backslash
leftarrow 
\backslash
sqrt{
\backslash
frac{
\backslash
partial^{2}l}{
\backslash
partial f^{2}}}$}
\backslash
Comment{Jacobian of the loss of the NN for examples $i$}
\end_layout

\begin_layout Plain Layout


\backslash
ForAll{$l 
\backslash
in $ layers from top to bottom}
\end_layout

\begin_layout Plain Layout


\backslash
State{$da_i 
\backslash
leftarrow 
\backslash
left( 
\backslash
frac{
\backslash
partial f_l^{(i)}}{
\backslash
partial a_l^{(i)}} 
\backslash
right)^T dh_i$}
\backslash
Comment{Derivative of the loss w.r.t the preactivation for examples $i$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$J_i 
\backslash
leftarrow J_i 
\backslash
frac{
\backslash
partial f_l^{(i)}}{
\backslash
partial a_l^{(i)}}$}
\backslash
Comment{For elementwise functions $
\backslash
frac{
\backslash
partial f_l^{(i)}}{
\backslash
partial a_l^{(i)}}$ is diagonal}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
Delta b 
\backslash
leftarrow - 
\backslash
lambda 
\backslash
sum_i^n da_i $}
\backslash
Comment{bias update}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
Delta W 
\backslash
leftarrow - 
\backslash
lambda 
\backslash
sum_i^n da_i h_{l-1}^T$}
\backslash
Comment{weights update}
\end_layout

\begin_layout Plain Layout


\backslash
State{$dh_i 
\backslash
leftarrow 
\backslash
left(W + 
\backslash
Delta W 
\backslash
right)^T da_i + W^{T}J_i^T J_i 
\backslash
left(
\backslash
Delta Wh_{l-1}^{(i)}+
\backslash
Delta b
\backslash
right)$}
\backslash
Comment{Updated backprop.}
\end_layout

\begin_layout Plain Layout


\backslash
State{$J_i 
\backslash
leftarrow J_i W$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$b
\backslash
leftarrow b +
\backslash
Delta b$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$W
\backslash
leftarrow W +
\backslash
Delta W$}
\end_layout

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Updated backpropagation
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:updbprop"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Standard
We use the autoencoder benchmark to compare the performance of UBP with
 stochastic gradient descent.
 Our results are plotted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "ubp-fig"

\end_inset

.
 In terms of updates, we observe that this method significantly outperforms
 SGD (note that this is a logarithmic scale).
 However it takes 10 times longer to obtain an update using UBP making it
 unpractical on this task.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/uab.pdf
	width 70text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Training error for standard backpropagation (in blue) and updated backpropagatio
n (in orange)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "ubp-fig"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Limits of this method
\end_layout

\begin_layout Standard
A first obvious limit is that it is very costly to compute the jacobians,
 and also to store them in memory.
\end_layout

\begin_layout Standard
A second limit which can also be very problematic is that similarly to the
 exploding gradient issue in deep or recurrent networks, we observe that
 this update can cause the backpropagated gradients to explode.
 Indeed, the problem resides in the fact that we have to multiply by the
 square matrix 
\begin_inset Formula $\left(\frac{\partial f}{\partial a_{l}}\right)^{T}\frac{\partial^{2}l}{\partial f^{2}}\frac{\partial f}{\partial a_{l}}$
\end_inset

 at each layer.
 This matrix is a measure of how much will the preactivation of the current
 layer change when we update its value by 
\begin_inset Formula $\left(\Delta Wh+\Delta b\right)$
\end_inset

.
 But by use the chain rule, we can see that we will once again multiply
 by this matrix when we compute the update of the next layer (which is below
 as backpropagation computes the derivatives backward from the top layer
 to the bottom layer).
 In the next layer, this matrix will have the expression 
\begin_inset Formula $\left(\frac{\partial f}{\partial a_{l-1}}\right)^{T}\frac{\partial^{2}l}{\partial f^{2}}\frac{\partial f}{\partial a_{l-1}}=\left(\frac{\partial a_{l}}{\partial a_{l-1}}\right)^{T}\left(\frac{\partial f}{\partial a_{l}}\right)^{T}\frac{\partial^{2}l}{\partial f^{2}}\frac{\partial f}{\partial a_{l}}\frac{\partial a_{l}}{\partial a_{l-1}}$
\end_inset

, so it will contain the former matrix.
 In this case, repeating this process multiple times will do something like
 a power method, which will either explode or vanish depending on the biggest
 eigenvalue of 
\begin_inset Formula $\left(\frac{\partial f}{\partial a_{l-1}}\right)^{T}\frac{\partial^{2}l}{\partial f^{2}}\frac{\partial f}{\partial a_{l-1}}$
\end_inset

.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
This current implementation is certainly not statisfying as a way to accelerate
 optimization, because of its computational cost.
 Instead we just see it as a proof of concept that it is possible to act
 on the backpropagated signal in order to improve it.
 There are probably more efficient ways of doing similar things, which could
 improve training very deep networks, where it could account for the interaction
s between layers that are separated by several others.
\end_layout

\begin_layout Chapter
Factorized second order
\begin_inset CommandInset label
LatexCommand label
name "chap:Factorized-second-order"

\end_inset


\end_layout

\begin_layout Standard
The expressions that we obtained for the FIM and the GN so far are generic
 in the sense that they could be applied to any model and any empirical
 risk composed of a sum of terms.
 We will now exploit the very particular structure of neural networks, to
 obtain a better understanding of how to apply these techniques for real
 tasks.
\end_layout

\begin_layout Standard
In an unconvenional way, we will start by presenting the local criterion
 that we introduce, which allowed us to get competitive results, and then
 we will introduce more general expressions and algorithms.
\end_layout

\begin_layout Section
A local criterion and the importance of the covariance of inputs in a layer
\end_layout

\begin_layout Subsection
Derivation of a new update
\begin_inset CommandInset label
LatexCommand label
name "lol"

\end_inset


\end_layout

\begin_layout Standard
Neural networks are usually trained using gradients computed all the way
 from the loss function to the parameters.
 Inspired by target propagation 
\begin_inset CommandInset citation
LatexCommand citep
key "bengio2014auto,lee2015difference"

\end_inset

, we explored an alternative which consists in replacing the last step of
 the backpropagation algorithm: the one of finding updates to the parameters
 given a derivative on the preactivations.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

 (page 
\begin_inset CommandInset ref
LatexCommand pageref
reference "fprop-bprop"

\end_inset

) we keep the usual computation for backpropagation (red lines) and replace
 the part in green.
\end_layout

\begin_layout Standard
We now focus on a single layer 
\begin_inset Formula $h_{l}=f_{l}\left(a_{l}\right)=f_{l}\left(W_{l}h_{l-1}+b_{l}\right)$
\end_inset

 and from now on we will drop the subscript 
\begin_inset Formula $l$
\end_inset

 for brevity and write 
\begin_inset Formula $h'=f\left(a\right)=f\left(Wh+b\right)$
\end_inset

.
 The gradients on the preactivation are given by 
\begin_inset Formula $\nabla_{a}\ell=\left(\frac{\partial\ell}{\partial a}\right)^{T}$
\end_inset

 as usual.
 We formulate our local criterion as finding updates 
\begin_inset Formula $\Delta W^{*},\Delta b^{*}$
\end_inset

 so that in expectation we will match the opposite gradients of preactivations
 times a learning rate 
\begin_inset Formula $-\lambda\nabla_{a}\ell$
\end_inset

.
 We call this optimization problem 
\begin_inset Quotes eld
\end_inset

local
\begin_inset Quotes erd
\end_inset

 in the sense that it is formulated locally to a single layer.
 We formulate our criterion as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta W^{*},\Delta b^{*} & = & \text{argmin}_{\Delta W,\Delta b}\frac{1}{n}\sum_{i}\left\Vert \Delta Wh^{(i)}+\Delta b-\left(-\lambda\nabla_{a^{(i)}}\ell\right)\right\Vert _{2}^{2}+\epsilon\left\Vert \Delta W\right\Vert _{2}^{2}\\
 & = & \text{argmin}_{\Delta W,\Delta b}\frac{1}{n}\sum_{i}\left\Vert \Delta Wh^{(i)}+\Delta b+\lambda\nabla_{a^{(i)}}\ell\right\Vert _{2}^{2}+\epsilon\left\Vert \Delta W\right\Vert _{2}^{2}\\
 & = & \text{argmin}_{\Delta W,\Delta b}\ell_{C}\left(\Delta W,\Delta b\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We denoted by 
\begin_inset Formula $\ell_{C}\left(\Delta W,\Delta b\right)$
\end_inset

 this new local criterion, to be solved for each layer.
 Instead of using gradient descent to find the optimal values for 
\begin_inset Formula $\Delta W^{*},\Delta b^{*}$
\end_inset

, we directly solve this expression by taking derivatives with respect to
 
\begin_inset Formula $\Delta W$
\end_inset

 and 
\begin_inset Formula $\Delta b$
\end_inset

 and setting them to 
\begin_inset Formula $0$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\nabla_{\Delta W}\ell_{C} & = & \frac{1}{n}\sum_{i}\left(\Delta Wh^{(i)}+\Delta b+\lambda\nabla_{a^{(i)}}\ell\right)h^{(i)T}+\epsilon\Delta W\nonumber \\
 & = & \Delta b\frac{1}{n}\sum_{i}h^{(i)T}+\epsilon\Delta W+\Delta W\frac{1}{n}\sum_{i}h^{(i)}h^{(i)T}+\frac{\lambda}{n}\sum_{i}\left(\nabla_{a^{(i)}}\ell\right)h^{(i)T}\label{eq:dwlocal}\\
\nabla_{\Delta b}\ell_{C} &  & \frac{1}{n}\sum_{i}\left(\Delta Wh^{(i)}+\Delta b+\lambda\nabla_{a^{(i)}}\ell\right)\nonumber \\
 & = & \Delta b+\Delta W\frac{1}{n}\sum_{i}h^{(i)}+\frac{\lambda}{n}\sum_{i}\nabla_{a^{(i)}}\ell\label{eq:dblocal}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
By solving 
\begin_inset Formula $\nabla_{\Delta b}\ell_{C}=0$
\end_inset

 for 
\begin_inset Formula $\Delta b$
\end_inset

 in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dblocal"

\end_inset

 we get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\Delta b^{*} & = & -\frac{\lambda}{n}\sum_{i}\nabla_{a^{(i)}}\ell-\Delta W\frac{1}{n}\sum_{i}h^{(i)}\label{eq:dbinter}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
By solving 
\begin_inset Formula $\nabla_{\Delta W}\ell_{C}=0$
\end_inset

 for 
\begin_inset Formula $\Delta W$
\end_inset

 in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dwlocal"

\end_inset

 and replacing 
\begin_inset Formula $\Delta b$
\end_inset

 using eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dbinter"

\end_inset

 we get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta W^{*}\left(\frac{1}{n}\sum_{i}h^{(i)}h^{(i)T}-\frac{1}{n}\sum_{i}h^{(i)}\frac{1}{n}\sum_{i}h^{(i)T}+\epsilon\mathbf{I}\right) & = & -\frac{\lambda}{n}\sum_{i}\left(\nabla_{a^{(i)}}\ell\right)h^{(i)T}+\frac{\lambda}{n}\sum_{i}\left(\nabla_{a^{(i)}}\ell\right)\frac{1}{n}\sum_{i}h^{(i)T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
By putting both expressions together and simplifying the covariances we
 obtain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta W^{*}\left(C+\epsilon\mathbf{I}\right) & = & -\frac{\lambda}{n}\sum_{i}\left(\nabla_{a^{(i)}}\ell\right)\left(h^{(i)}-\frac{1}{n}\sum_{i}h^{(i)}\right)^{T}\\
\Delta b^{*} & = & -\frac{\lambda}{n}\sum_{i}\nabla_{a^{(i)}}\ell-\frac{1}{n}\Delta W^{*}\sum_{i}h^{(i)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We denoted by 
\begin_inset Formula $C=\frac{1}{n}\sum_{i}\left(h^{(i)}-\frac{1}{n}\sum_{i}h^{(i)}\right)\left(h^{(i)}-\frac{1}{n}\sum_{i}h^{(i)}\right)^{T}$
\end_inset

 the covariance matrix of the activation of the previous layer.
 This expressions can be solved by inverting the square matrix 
\begin_inset Formula $\left(C+\epsilon\mathbf{I}\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Comparison with standard SGD
\end_layout

\begin_layout Standard
The updates for standard SGD are 
\begin_inset Formula $\Delta_{SGD}b=-\frac{\lambda}{n}\sum_{i}\nabla_{a^{(i)}}\ell$
\end_inset

 and 
\begin_inset Formula $\Delta_{SGD}W=-\frac{\lambda}{n}\sum_{i}\left(\nabla_{a^{(i)}}\ell\right)h^{(i)T}$
\end_inset

.
\end_layout

\begin_layout Standard
The 
\series bold
update for 
\begin_inset Formula $b$
\end_inset


\series default
 gets a new term 
\begin_inset Formula $-\frac{1}{n}\Delta W\sum_{i}h^{(i)}$
\end_inset

 that permits taking into account the update of 
\begin_inset Formula $W$
\end_inset

.
 In practice, we found that it did not change much as 
\begin_inset Formula $\Delta W$
\end_inset

 is typically at least one order of magnitude smaller than 
\begin_inset Formula $\frac{\lambda}{n}\sum_{i}\nabla_{a^{(i)}}\ell$
\end_inset

.
\end_layout

\begin_layout Standard
The 
\series bold
update for 
\begin_inset Formula $W$
\end_inset

 
\series default
is different in 2 ways.
 First, it is also scaled using the inverse covariance matrix of the input
 
\begin_inset Formula $C^{-1}$
\end_inset

.
 Secondly, it is centered since we substract the expectation of 
\begin_inset Formula $h$
\end_inset

.
 This is related to an old well used trick 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun1998gradient,schraudolph2012centering"

\end_inset

.
\end_layout

\begin_layout Subsection
What is behind this local criteria
\end_layout

\begin_layout Standard
This new update is somewhere between usual gradient descent, and something
 that is inspired from target propagation.
 From a theoretical point of view it is not yet clear why it would provide
 sensible updates.
 Also surprising is the effectiveness of these new updates as we will see
 in experimental section.
 In the following sections, we will show that it is actually linked to second
 order methods applied to the particular structure of neural networks.
\end_layout

\begin_layout Section
Decomposition using the Kronecker product
\begin_inset CommandInset label
LatexCommand label
name "sec:Decomposition-using-the"

\end_inset


\end_layout

\begin_layout Standard
In this section, we will show a convenient factorization of the Gauss-Newton
 approximation of the Hessian, that was first applied to the Fisher Information
 Matrix in the litterature 
\begin_inset CommandInset citation
LatexCommand citep
key "martens2015optimizing"

\end_inset

.
 To this end, we will use an operation called the Kronecker product that
 permits giving simple expressions for the GN matrix.
 For 2 matrices 
\begin_inset Formula $A$
\end_inset

 of size 
\begin_inset Formula $m\times n$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 of size 
\begin_inset Formula $p\times q$
\end_inset

 it produces a new matrix 
\begin_inset Formula $A\otimes B$
\end_inset

 of size 
\begin_inset Formula $mp\times nq$
\end_inset

 defined by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
A\otimes B & = & \left(\begin{array}{ccc}
a_{11}B & \cdots & a_{1n}B\\
\vdots & \ddots & \vdots\\
a_{m1}B & \cdots & a_{mn}B
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Its most interesting property in the context of neural networks is its relations
hip with the 
\begin_inset Formula $vec$
\end_inset

 operation, that 
\begin_inset Quotes eld
\end_inset

flattens
\begin_inset Quotes erd
\end_inset

 a matrix into a vector.
 It is of great use for 2nd order, because the weight matrices can be vectorized
 using 
\begin_inset Formula $vec$
\end_inset

, to give matrix expressions for the Hessian, which otherwise could not
 be written.
 We will make use of the property:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
vec\left(AXB\right) & = & \left(B^{T}\otimes A\right)vec\left(X\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Getting back to the expression for the Gauss-Newton matrix derived in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

, we use the block diagonal approximation and focus on a single layer defined
 by the linear transformation 
\begin_inset Formula $a=Wh+b$
\end_inset

 and the nonlinearity 
\begin_inset Formula $h'=f\left(a\right)$
\end_inset

.
 We start from the jacobian of the output of the network, with respect to
 the output of the linear transformation 
\begin_inset Formula $a$
\end_inset

, denoted by 
\begin_inset Formula $\mathbf{J}_{a}$
\end_inset

.
 From this jacobian computed by backpropagation, we can get the jacobian
 with respect to the parameters of the layer by making use of the chain
 rule 
\begin_inset Formula $\mathbf{J}_{\theta}=\mathbf{J}_{a}\mathbf{J}_{\theta}^{a}$
\end_inset

.
 We use the notation 
\begin_inset Formula $\mathbf{J}_{\theta}^{a}$
\end_inset

 for the jacobian of 
\begin_inset Formula $a$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

.
 In order to get an expression for this jacobian, we now make use of the
 
\begin_inset Formula $vec$
\end_inset

 operator to transform 
\begin_inset Formula $W$
\end_inset

 into a vector:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a & = & vec\left(a\right)\\
 & = & vec\left(Wh\right)+b\\
 & = & \left(h^{T}\otimes\mathbf{I}\right)vec\left(W\right)+b
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\mathbf{I}$
\end_inset

 is the identity, of the same size as 
\begin_inset Formula $a$
\end_inset

, that is the output size of the layer.
 We can now give an expression for 
\begin_inset Formula $\mathbf{J}_{vec\left(W\right)}^{a}$
\end_inset

 and 
\begin_inset Formula $\mathbf{J}_{b}^{a}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{J}_{b}^{a} & = & \mathbf{I}\\
\mathbf{J}_{vec\left(W\right)}^{a} & = & h^{T}\otimes\mathbf{I}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Or, if we stack 
\begin_inset Formula $vec\left(W\right)$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 in a vector 
\begin_inset Formula $\theta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{J}_{\theta}^{a} & = & \left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\mathbf{I}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And finally by the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mathbf{J}_{\theta} & = & \mathbf{J}_{a}\left(\left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\mathbf{I}\right)\nonumber \\
 & = & \left(1\otimes\mathbf{J}_{a}\right)\left(\left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\mathbf{I}\right)\nonumber \\
 & = & \left(\begin{array}{cc}
h^{T} & 1\end{array}\right)\otimes\mathbf{J}_{a}\label{eq:jaco_factorization}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left(\begin{array}{cc}
h^{T} & 1\end{array}\right)$
\end_inset

 is the concatenation of the row vector 
\begin_inset Formula $h^{T}$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
This jacobian is a first order measure of how much the output of the network
 will change if we change the values of the parameters of this layer, for
 a single example.
 Let us now recall the expression of the GN matrix 
\begin_inset Formula $G_{f}=\frac{1}{n}\sum_{i}\mathbf{J}_{\theta}\left(x_{i}\right)^{T}D\left(x_{i}\right)\mathbf{J}_{\theta}\left(x_{i}\right)$
\end_inset

 from section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:gauss-newton"

\end_inset

.
 We can rewrite this expression using the factorization in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:jaco_factorization"

\end_inset

:
\begin_inset Formula 
\begin{eqnarray}
G_{f} & = & \frac{1}{n}\sum_{i}\left[\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)\otimes\mathbf{J}_{a_{i}}\right]^{T}D\left(x_{i}\right)\left[\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)\otimes\mathbf{J}_{a_{i}}\right]\nonumber \\
 & = & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)^{T}\left(\begin{array}{cc}
h_{i}^{T} & 1\end{array}\right)\otimes\left(\mathbf{J}_{a_{i}}^{T}D\left(x_{i}\right)\mathbf{J}_{a_{i}}\right)\nonumber \\
 & = & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\left(\mathbf{J}_{a_{i}}^{T}D\left(x_{i}\right)\mathbf{J}_{a_{i}}\right)\label{eq:factored}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
We used the property that 
\begin_inset Formula $\left(A\otimes B\right)\left(C\otimes D\right)=AC\otimes BD$
\end_inset

 when the sizes of the matrices 
\begin_inset Formula $A,B,C,D$
\end_inset

 match.
 This factorization is interesting because it separates the GN matrix into
 a contribution from the backpropagated jacobian (red arrow in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fprop-bprop"

\end_inset

 page 
\begin_inset CommandInset ref
LatexCommand pageref
reference "fprop-bprop"

\end_inset

), and a part that only uses the forward statistics and that is local to
 a layer.
 While these 2 contributions are clearly factorized for a single example,
 it is not clear whether the resulting sum can still be factorized using
 a kronecker product.
 As we will show in the next section, similar factorizations were exploited
 in KFAC 
\begin_inset CommandInset citation
LatexCommand citep
key "martens2015optimizing"

\end_inset

 and Natural Neural Networks 
\begin_inset CommandInset citation
LatexCommand citep
key "desjardins2015natural"

\end_inset

 to build efficient optimization algorithms.
 Note that in this formulation in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:factored"

\end_inset

 we contribute an explicit distinction between the weight matrix 
\begin_inset Formula $W$
\end_inset

 and the bias 
\begin_inset Formula $b$
\end_inset

 whereas in previous work authors usually put all parameters together in
 a weight matrix with an extra column.
 As we will see in future sections, this is key to understanding the role
 of centering the updates.
\end_layout

\begin_layout Subsection
Decomposition into 2 smaller matrices
\end_layout

\begin_layout Standard
In second order algorithms, inverting the Hessian matrix is often the limiting
 factor as its computational cost is 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

.
 The Kronecker product has the pleasing property that it turns the inversion
 of a big matrix into inverting 2 smaller matrices since 
\begin_inset Formula $\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}$
\end_inset

.
 In our case, if such a decomposition existed we would reduce the computational
 cost from 
\begin_inset Formula $O\left(n_{in}^{3}n_{out}^{3}\right)$
\end_inset

 to 
\begin_inset Formula $O\left(n_{in}^{3}\right)+O\left(n_{out}^{3}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Unfortunately, we can not write the GN matrix nor the FIM using 2 matrices
 because it is a sum of Kronecker products, so we aim at finding approximate
 factorizations that will have the required form.
\end_layout

\begin_layout Subsection
Focus on the covariance part of the decomposition
\begin_inset CommandInset label
LatexCommand label
name "subsec:focus-covariance"

\end_inset


\end_layout

\begin_layout Standard
We now suppose that we can use the following approximation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\left(\mathbf{J}_{a_{i}}^{T}D\left(x_{i}\right)\mathbf{J}_{a_{i}}\right) & \approx & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\alpha\mathbf{I}=G_{in}\label{eq:approx_alpha}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\alpha$
\end_inset

 is the same for all examples, it does not depend on 
\begin_inset Formula $i$
\end_inset

.
 This approximation means that we ignore the interactions between the output
 preactivations.
 Instead we just focus on some statistics of the activations of the current
 layer.
\end_layout

\begin_layout Standard
Looking back at the use we will make of this preconditioner, namely 
\begin_inset Formula $\Delta\theta=-\lambda G_{in}^{-1}\nabla_{\theta}R$
\end_inset

 or equivalently 
\begin_inset Formula $G_{in}\Delta\theta=-\lambda\nabla_{\theta}R$
\end_inset

, we can observe that this will penalize an update direction of 
\begin_inset Formula $\theta$
\end_inset

 if the corresponding activation has a high variance, as measured by 
\begin_inset Formula $h_{i}h_{i}^{T}$
\end_inset

.
 This makes sense since in this case changing the value here will change
 the next forward propagated signal more that if the variance of the correspondi
ng activation were lower.
 This would result in a bigger expected change in the output.
 
\end_layout

\begin_layout Standard
The left part 
\begin_inset Formula $A=\frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)$
\end_inset

 corresponds to some statistics on the input of the considered layer.
 It has the size 
\begin_inset Formula $\left(n_{in}+1\right)\times\left(n_{in}+1\right)$
\end_inset

 with the line/column corresponding to the bias.
 We will now derive the update that corresponds to using this matrix 
\begin_inset Formula $G_{in}$
\end_inset

 as a preconditioner:
\end_layout

\begin_layout Standard
We need to invert 
\begin_inset Formula $A$
\end_inset

.
 This matrix can be inverted blockwise.
 We denote by 
\begin_inset Formula $C=\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{j}h_{j}\right)\left(h_{i}-\frac{1}{n}\sum_{j}h_{j}\right)^{T}$
\end_inset

the covariance matrix of the input vector of the linear layer.
 We get the inverse:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left(\begin{array}{cc}
\frac{1}{n}\sum_{i}h_{i}h_{i}^{T} & \frac{1}{n}\sum_{i}h_{i}\\
\frac{1}{n}\sum_{i}h_{i}^{T} & 1
\end{array}\right)^{-1} & = & \left(\begin{array}{cc}
C^{-1} & -C^{-1}\frac{1}{n}\sum_{i}h_{i}\\
-\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1} & 1+\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1}\frac{1}{n}\sum_{i}h_{i}
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Applying this preconditioner to a gradient update we can get a new update
 for the weight matrix and the bias.
 Let us first recall the gradient for a minibatch of examples.
 In order to be able to use it with our preconditioner we put the parameters
 into a vector 
\begin_inset Formula $\theta=\left(\begin{array}{cc}
\text{vec}\left(W\right)^{T} & b^{T}\end{array}\right)^{T}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\nabla_{\theta}R & = & -\frac{\lambda}{n}\sum_{j}\left(\begin{array}{c}
h_{j}\\
1
\end{array}\right)\otimes\nabla_{a_{j}}\ell
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The corresponding update is a Newton step using the approximate Hessian
 (see eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:newton-step"

\end_inset

 page 
\begin_inset CommandInset ref
LatexCommand pageref
reference "eq:newton-step"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
G_{in}^{-1}\nabla_{\theta}R & = & \frac{1}{n}\sum_{j}\left(\begin{array}{cc}
C^{-1} & -C^{-1}\frac{1}{n}\sum_{i}h_{i}\\
-\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1} & 1+\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1}\frac{1}{n}\sum_{i}h_{i}
\end{array}\right)\left(\begin{array}{c}
h_{j}\\
1
\end{array}\right)\otimes\frac{-\lambda}{\alpha}\nabla_{a_{j}}\ell
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From this expression we can write the update for 
\begin_inset Formula $W$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta\text{vec}\left(W\right) & = & C^{-1}\frac{-\lambda}{n}\sum_{j}h_{j}\otimes\frac{1}{\alpha}\nabla_{a_{j}}\ell-C^{-1}\frac{1}{n}\sum_{i}h_{i}\otimes\frac{-\lambda}{n}\sum_{j}\frac{1}{\alpha}\nabla_{a_{j}}\ell\\
 & = & \frac{1}{\alpha}C^{-1}\frac{-\lambda}{n}\sum_{j}\left(h_{j}-\frac{1}{n}\sum_{i}h_{i}\right)\otimes\nabla_{a_{j}}\ell
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{equation}
\boxed{\Delta W=\frac{-\lambda}{\alpha}\frac{1}{n}\sum_{j}\nabla_{a_{j}}\ell\left(h_{j}-\frac{1}{n}\sum_{i}h_{i}\right)^{T}C^{-1}}\label{eq:upd_w}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And the update for 
\begin_inset Formula $b$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta b & = & -\frac{-\lambda}{n}\sum_{j}\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1}h_{j}\frac{1}{\alpha}\nabla_{a_{j}}\ell+\left(1+\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1}\frac{1}{n}\sum_{i}h_{i}\right)\frac{-\lambda}{n}\sum_{j}\frac{1}{\alpha}\nabla_{a_{j}}\ell\\
 & = & \frac{-\lambda}{n}\sum_{j}\frac{1}{\alpha}\nabla_{a_{j}}\ell+\frac{\lambda}{n}\sum_{j}\frac{1}{n}\sum_{i}h_{i}^{T}C^{-1}\left(h_{j}-\frac{1}{n}\sum_{i}h_{i}\right)\frac{1}{\alpha}\nabla_{a_{j}}\ell
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{equation}
\boxed{\Delta b=-\frac{\lambda}{\alpha}\frac{1}{n}\sum_{j}\nabla_{a_{j}}\ell-\Delta W\frac{1}{n}\sum_{i}h_{i}}\label{eq:upd_b}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Using an argument based on second order methods, we thus get back to the
 very same update as in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "lol"

\end_inset

.
\end_layout

\begin_layout Section
Algorithms
\end_layout

\begin_layout Standard
We now present 2 algorithms.
 The first one is very simple and just aims at isolating the centering trick,
 in order to assess how much of the gain of performance comes from just
 centering the update, and how much comes from the whole covariance.
 The other one is the full update that we just derived.
\end_layout

\begin_layout Subsection
Centered gradient descent
\begin_inset CommandInset label
LatexCommand label
name "subsec:Centered-gradient-descent"

\end_inset


\end_layout

\begin_layout Standard
Following the update for 
\begin_inset Formula $W$
\end_inset

 derived in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:upd_w"

\end_inset

, we simply replace the usual update for the weight matrices by a centered
 version 
\begin_inset Formula $\Delta W=\frac{1}{n}\sum_{j}\nabla_{a_{j}}l\left(h_{j}-\overbrace{\frac{1}{n}\sum_{i}h_{i}}^{\text{centering}}\right)^{T}$
\end_inset

.
 The gradient, as well as the inner expectation, are computed using a minibatch.
 Some authors refer to a very similar idea as 
\shape italic
mean-only batch normalization
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "salimans2016weight"

\end_inset

 where the value of 
\begin_inset Formula $h$
\end_inset

 is replaced by 
\begin_inset Formula $h-\mathbb{E}\left[h\right]$
\end_inset

 in the forward pass, with the expectation being computed using a mini-batch.
 The difference here is that we do not reparametrize the forward propagation,
 instead we just follow a slightly different direction which is not the
 gradient but a centered gradient, as suggested by eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:upd_w"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\begin_layout Plain Layout


\backslash
While{not converged}
\end_layout

\begin_layout Plain Layout


\backslash
State{Sample a minibatch $
\backslash
mathcal{D}$}
\end_layout

\begin_layout Plain Layout


\backslash
For{all layers}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
Delta_{a_i}
\backslash
leftarrow - 
\backslash
nabla_{a_i} l
\backslash
left(f
\backslash
left(x_i
\backslash
right), y_i
\backslash
right) 
\backslash
forall i 
\backslash
in 
\backslash
mathcal{D}$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$b
\backslash
leftarrow b + 
\backslash
lambda 
\backslash
frac{1}{n} 
\backslash
sum_i 
\backslash
Delta_{a_i}$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$W
\backslash
leftarrow W + 
\backslash
lambda 
\backslash
frac{1}{n} 
\backslash
sum_i 
\backslash
Delta_{a_i} 
\backslash
left(h_{i}-
\backslash
frac{1}{n}
\backslash
sum_{j}h_{j}
\backslash
right)^T $}
\end_layout

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Centered gradient descent
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Amortized covariance preconditioner
\begin_inset CommandInset label
LatexCommand label
name "subsec:Amortized-covariance-preconditio"

\end_inset


\end_layout

\begin_layout Standard
In the updates derived from the covariance (for 
\begin_inset Formula $b$
\end_inset

 eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:upd_b"

\end_inset

 and for 
\begin_inset Formula $W$
\end_inset

 eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:upd_w"

\end_inset

), we require an inverse of the matrix 
\begin_inset Formula $C=\frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{j}h_{j}\right)\left(h_{i}-\frac{1}{n}\sum_{j}h_{j}\right)^{T}$
\end_inset

.
 This matrix has the size of the input of a layer 
\begin_inset Formula $n_{in}$
\end_inset

.
 While it is smaller than the full GN or FIM for a single layer of size
 
\begin_inset Formula $\left(n_{in}+1\right)\times n_{out}$
\end_inset

, it is still not very efficient to estimate the inverse at each iteration.
 Meanwhile, these statistics do not change much between iterations so a
 natural idea is to amortize the cost of inversion over several updates.
\end_layout

\begin_layout Standard
A question remains for the choice of 
\begin_inset Formula $\alpha$
\end_inset

 that we used to approximate the real GN matrix in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_alpha"

\end_inset

.
 We adopt two approaches.
 The first one consists in treating it as a fixed value, so it is a hyperparamet
er that we tune using our biased random search (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "biased-random-search"

\end_inset

).
 The second one is a very experimental heuristic, which consists in taking
 the maximum value of the squarred gradient 
\begin_inset Formula $\alpha=\max_{i\in\text{minibatch},j\leq n_{out}}\left(\nabla_{a_{i}}l\right)_{j}^{2}$
\end_inset

.
 This gives a different value for each layer, and also different for each
 minibatch.
 We found it worked very well experimentally, and we justify it as being
 a rough heuristic estimate of the curvature of the empirical risk, with
 respect to the output of the layer.
\end_layout

\begin_layout Standard
For numerical stability and to account for the imprecision of 
\begin_inset Formula $C$
\end_inset

 between two estimates, we use Tikhonov regularization (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "tikhonov"

\end_inset

).
 This adds a scalar value 
\begin_inset Formula $\epsilon$
\end_inset

 to the diagonal of our approximate GN, which in this case is scaled by
 
\begin_inset Formula $\frac{1}{\alpha}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T} & h_{i}\\
h_{i}^{T} & 1
\end{array}\right)\otimes\alpha\mathbf{I}_{out}+\epsilon\mathbf{I} & = & \frac{1}{n}\sum_{i}\left(\begin{array}{cc}
h_{i}h_{i}^{T}+\frac{\epsilon}{\alpha}\mathbf{I} & h_{i}\\
h_{i}^{T} & 1+\frac{\epsilon}{\alpha}
\end{array}\right)\otimes\alpha\mathbf{I}_{out}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
and the updated covariance matrix becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
C & = & \frac{1}{n}\sum_{i}\left(h_{i}-\frac{1}{n}\sum_{j}h_{j}\right)\left(h_{i}-\frac{1}{n}\sum_{j}h_{j}\right)^{T}+\frac{\epsilon}{\alpha}\mathbf{I}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\begin_layout Plain Layout


\backslash
Require{$
\backslash
mathbf{N}$  estimate statistics every $
\backslash
mathbf{N}$ minibatches}
\end_layout

\begin_layout Plain Layout


\backslash
State{$n_{updates} 
\backslash
leftarrow 0$}
\end_layout

\begin_layout Plain Layout


\backslash
While{not converged}
\end_layout

\begin_layout Plain Layout


\backslash
If{$n_{updates}
\backslash
mod
\backslash
mathbf{N}=0$}
\backslash
Comment{Amortization}
\end_layout

\begin_layout Plain Layout


\backslash
State{Sample a minibatch $
\backslash
mathcal{D}$ and compute forward pass}
\end_layout

\begin_layout Plain Layout


\backslash
For{each layer $j$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$C^{
\backslash
left(j
\backslash
right)} 
\backslash
leftarrow 
\backslash
text{cov}_
\backslash
mathcal{D} 
\backslash
left(h^{
\backslash
left(j
\backslash
right)}, h^{
\backslash
left(j
\backslash
right)} 
\backslash
right)$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$inv
\backslash
_C^{
\backslash
left(j
\backslash
right)} 
\backslash
leftarrow 
\backslash
text{inverse} 
\backslash
left( C^{
\backslash
left(j
\backslash
right)}+
\backslash
frac{
\backslash
epsilon}{
\backslash
alpha}
\backslash
mathbf{I} 
\backslash
right)$}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\begin_layout Plain Layout


\backslash
State{Sample a minibatch $
\backslash
mathcal{D}$ and compute forward pass}
\end_layout

\begin_layout Plain Layout


\backslash
For{each layer $j$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$
\backslash
Delta_{a_i^{
\backslash
left(j
\backslash
right)}}
\backslash
leftarrow - 
\backslash
nabla_{a_i^{
\backslash
left(j
\backslash
right)}} l
\backslash
left(f
\backslash
left(x_i
\backslash
right), y_i
\backslash
right) 
\backslash
forall i 
\backslash
in 
\backslash
mathcal{D}$}
\end_layout

\begin_layout Plain Layout


\backslash
State{$b
\backslash
leftarrow b + 
\backslash
frac{
\backslash
lambda}{
\backslash
alpha} 
\backslash
frac{1}{n} 
\backslash
sum_{i 
\backslash
in 
\backslash
mathcal{D}} 
\backslash
Delta_{a_i^{
\backslash
left(j
\backslash
right)}}$}
\end_layout

\begin_layout Plain Layout


\backslash
Comment{eq 
\backslash
ref{eq:upd_b}}
\end_layout

\begin_layout Plain Layout


\backslash
State{$W
\backslash
leftarrow W + 
\backslash
frac{
\backslash
lambda}{
\backslash
alpha} 
\backslash
frac{1}{n} 
\backslash
sum_{i 
\backslash
in 
\backslash
mathcal{D}} 
\backslash
Delta_{a_i^{
\backslash
left(j
\backslash
right)}} 
\backslash
left(h_{i}^{
\backslash
left(j
\backslash
right)}-
\backslash
frac{1}{n}
\backslash
sum_{k 
\backslash
in 
\backslash
mathcal{D}}h_{k}^{
\backslash
left(j
\backslash
right)}
\backslash
right)^T inv
\backslash
_C^{
\backslash
left(j
\backslash
right)}$}
\end_layout

\begin_layout Plain Layout


\backslash
Comment{eq 
\backslash
ref{eq:upd_w}}
\end_layout

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\begin_layout Plain Layout


\backslash
State{$n_{updates} 
\backslash
leftarrow n_{updates} + 1$}
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Amortized covariance preconditioner (ACP)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Other related approximate second order algorithms
\end_layout

\begin_layout Standard
The 2 following techniques have been proposed using the same factorization
 of the FIM that we wrote in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:factored"

\end_inset

.
 In addition to their factorization we introduced the explicit separation
 of weight matrix and bias, which we use in the following notations.
\end_layout

\begin_layout Subsection
KFAC
\end_layout

\begin_layout Standard
KFAC 
\begin_inset CommandInset citation
LatexCommand citep
key "martens2015optimizing"

\end_inset

 proposed to split the sum of Kronecker products into a product of sums:
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{F} & \approx & \mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} & h\\
h^{T} & 1
\end{array}\right)\right]\otimes\mathbb{E}\left[\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The Kronecker product has the nice property that for 2 invertible square
 matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

, 
\begin_inset Formula $\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}$
\end_inset

.
 It follows that inverting the FIM now requires inverting 2 smaller matrices.
 As for the approximation we made in eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_alpha"

\end_inset

 section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:focus-covariance"

\end_inset

, we lose the coupling between both parts of the kronecker product of each
 example in the FIM (see eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:factored"

\end_inset

 page 
\begin_inset CommandInset ref
LatexCommand pageref
reference "eq:factored"

\end_inset

).
 In our experiments we found a comparable performance between KFAC and ACP.
\end_layout

\begin_layout Standard
KFAC has been introduced for the natural gradient, but as we showed in the
 previous sections, it can also be adapted to Gauss-Newton.
\end_layout

\begin_layout Subsection
Natural Neural Networks
\end_layout

\begin_layout Standard
Natural neural networks 
\begin_inset CommandInset citation
LatexCommand citep
key "desjardins2015natural"

\end_inset

 exploit the same factorization by focusing on the input covariance part
 of each layer.
 They propose a reparametrization that makes 
\begin_inset Formula $\mathbb{E}\left[hh^{T}\right]$
\end_inset

 equal the identity.
 They also notice that in order for their method to work well, they have
 to use the centering trick.
 To this view, they change the original linear transformation 
\begin_inset Formula $a=Wh+b$
\end_inset

 to become:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a & = & VU\left(h-\mu\right)+d
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $V$
\end_inset

 is the new weight matrix and 
\begin_inset Formula $d$
\end_inset

 are the new biases.
 
\begin_inset Formula $\mu=\mathbb{E}\left[h\right]$
\end_inset

 is the mean value for 
\begin_inset Formula $h$
\end_inset

 and 
\begin_inset Formula $U$
\end_inset

 is the square root of the inverse covariance of 
\begin_inset Formula $h$
\end_inset

, defined by 
\begin_inset Formula $U^{2}=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-1}$
\end_inset

, denoted by 
\begin_inset Formula $U=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-\frac{1}{2}}$
\end_inset

.
 
\begin_inset Formula $U$
\end_inset

 and 
\begin_inset Formula $\mu$
\end_inset

 are not trained using gradient descent but instead they are estimated using
 data from the training set.
\end_layout

\begin_layout Standard
The new parameters 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 are trained using gradient descent.
 We will denote by 
\begin_inset Formula $h_{e}=U\left(h-\mu\right)$
\end_inset

 the new 
\begin_inset Quotes eld
\end_inset

effective
\begin_inset Quotes erd
\end_inset

 input to the linear transformation induced by the weight matrix 
\begin_inset Formula $V$
\end_inset

.
 Let us first remark that 
\begin_inset Formula $\mathbb{E}\left[h_{e}\right]=U\left(\mathbb{E}\left[h\right]-\mu\right)=U\left(\mu-\mu\right)=0$
\end_inset

, so the reparametrized input is centered on average.
 A second remark is that 
\begin_inset Formula $\mathbb{E}\left[h_{e}h_{e}^{T}\right]=U\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]U^{T}=\mathbf{I}$
\end_inset

.
 By construction 
\begin_inset Formula $U$
\end_inset

 cancels out the covariance.
 Wrapping everything together we thus have the desired property that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}\left[\left(\begin{array}{cc}
h_{e}h_{e}^{T} & h_{e}\\
h_{e}^{T} & 1
\end{array}\right)\right] & = & \left(\begin{array}{cc}
\mathbb{E}\left[h_{e}h_{e}^{T}\right] & \mathbb{E}\left[h_{e}\right]\\
\mathbb{E}\left[h_{e}^{T}\right] & 1
\end{array}\right)\\
 & = & \left(\begin{array}{cc}
\mathbf{I} & 0\\
0 & 1
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The approximate FIM for the reparametrization thus has a better form.
 In this case the natural gradient update will be closer to the usual gradient
 update.
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Centering tricks
\end_layout

\begin_layout Standard
We compare our algorithms using the autoencoder benchmark presented in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:autoencoder-benchmark"

\end_inset

.
 We ran all experiments on the same architecture using a Titan Black GPU.
\end_layout

\begin_layout Standard
For each experiment we plot the expected loss on the train set, and on a
 test set that we did not use for learning (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "centering"

\end_inset

).
 To assess the practical performance, the x-axis represents the actual time
 spent on each experiment.
 We selected the best hyperparameters using biased random search, and we
 only plot the best experiment.
 For the test experiments, we also only plot the best result for each technique.
 We ran each experiment for 
\begin_inset Formula $3\times10^{5}$
\end_inset

 updates.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/centering.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of centering methods on MNIST autoencoder: blue: standard SGD
 with fixed learning rate; orange: mean-only batch norm; green: centered
 updates as presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Centered-gradient-descent"

\end_inset

; red: ACP with fixed 
\begin_inset Formula $\alpha$
\end_inset

 as presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Amortized-covariance-preconditio"

\end_inset

.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "centering"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison of 2nd order approximate methods
\end_layout

\begin_layout Standard
In the second experiment, we compare all second order approximations to
 a baseline using batch normalization on the autoencoder on MNIST.
 For all experiments we use an amortization factor of 100, that is we update
 the statistics and invert the corresponding matrices every 100 updates.
 We use a minibatch size of 200 for each update as well as for the statistics.
 See figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "mnist-autoencoder"

\end_inset

.
\end_layout

\begin_layout Standard
In a third experiment, we compare all second order approximate methods on
 the classification task on CIFAR10.
 Similarly to the autoencoder, we use a minibatch size of 200, and we recompute
 the inverse statistics every 100 updates.
 As all methods are able to overfit very rapidly, we also report the error
 rate on both the train set and a separated test set.
 See figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cifar10"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/2nd.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of second order methods on MNIST autoencoder: blue: ACP with
 fixed 
\begin_inset Formula $\alpha$
\end_inset

 as presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Amortized-covariance-preconditio"

\end_inset

; orange: ACP with heuristic for 
\begin_inset Formula $\alpha$
\end_inset

 as presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Amortized-covariance-preconditio"

\end_inset

; green: Natural Neural Network; red: KFAC; purple: batch norm with standard
 SGD; brown: standard SGD
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "mnist-autoencoder"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/cifar10.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of second order methods on CIFAR10 classification: blue: ACP
 with fixed 
\begin_inset Formula $\alpha$
\end_inset

 as presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Amortized-covariance-preconditio"

\end_inset

; orange: ACP with heuristic for 
\begin_inset Formula $\alpha$
\end_inset

 as presented in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Amortized-covariance-preconditio"

\end_inset

; green: Natural Neural Network; red: KFAC for Gauss-Newton; purple: batch
 norm with standard SGD; brown: standard SGD; pink: KFAC for natural gradient
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "cifar10"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Interpretation and conclusions
\end_layout

\begin_layout Subsubsection

\series bold
How do all second order methods compare ?
\end_layout

\begin_layout Standard
In terms of updates, our best performer is the natural gradient approximated
 by KFAC on both experiments.
 But in the case of the autoencoder, we see that because it is faster to
 compute only the forward statistics, ACP and KFAC compare similarly.
 Note that for the autoencoder, the best test set is attained by ACP, and
 also in a shorter time than all other methods.
\end_layout

\begin_layout Subsubsection

\series bold
Does the improvement come from the centering trick or from the covariances
 ?
\end_layout

\begin_layout Standard
In the centering experiment, we clearly see that full ACP outperforms all
 centered activation methods by a large margin.
 Centered gradient descent does not even improve on SGD.
 This is in contradiction with our early experiments on a smaller number
 of iterations, which showed much better progress for early training.
 The best performing centered gradient is mean only batch norm.
 In addition to centering the gradients, it also probably better condition
 the problem by modifying the backpropagated gradients.
\end_layout

\begin_layout Standard
We can thus conclude that the improvement of ACP and other second order
 methods does not only come from centering the activations, but also from
 the full covariance.
\end_layout

\begin_layout Subsubsection

\series bold
Stability
\end_layout

\begin_layout Standard
There is a stability issue with ACP visible on the blue curve in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "cifar10"

\end_inset

.
 Recall that we divide the Tikhonov regularization factor by a varying 
\begin_inset Formula $\alpha$
\end_inset

, proportional to the gradient incoming in the current layer.
 This stability issue could probably be eased with regularization by adding
 a small scalar value before dividing.
\end_layout

\begin_layout Subsubsection

\series bold
How does KFAC compare to KFAC for Gauss-Newton ?
\end_layout

\begin_layout Standard
As mentionned, the FIM and the GN are the same for the quadratic error.
 But this is not the case for the cross entropy.
 Both methods can be decomposed using the Kronecker product of 2 small matrices,
 so we want to compare how they perform.
 As can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "cifar10"

\end_inset

 by comparing the red curve for GN and the pink curve for FIM, we observe
 that the natural gradient version performs much better than the GN version,
 with respect to both the number of updates and the wallclock time.
 It is remarkable as the natural gradient version takes much longer to recompute
 stats, because it needs the whole jacobian of the output of the network,
 whereas GN only needs the gradient, so there is no additional cost for
 the backpropagation as we mentionned in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:gn-trick-cheap"

\end_inset

.
\end_layout

\begin_layout Chapter*
Conclusions
\end_layout

\begin_layout Standard
In this work, we derived new expressions for the well known GN matrix and
 FIM.
 We showed that there is a common factor in both matrices that is composed
 of the covariance matrix of the activation at each layer, in the case of
 a block diagonal approximation.
 By separating the bias and the weight matrices we introduced a new mathematical
 explanation for the well-known centering trick.
 Using this new expressions we derived a new algorithm ACP that loosely
 resembles 2 state of the art methods inspired by natural gradient.
 We benchmarked our new algorithm against these methods and showed that
 they all perform similarly.
\end_layout

\begin_layout Standard
We also introduced a tentative modification of backpropagation in order
 to obtain better derivatives.
 This algorithm showed promising result since it provided better updates
 than vanilla gradient descent.
 However there remains some limits to applying this technique in a real
 setup as it is still too computationnally expensive.
\end_layout

\begin_layout Standard
A natural follow-up to this work is to extend it to other architectures
 such as recurrent neural networks as formally initiated in 
\begin_inset CommandInset citation
LatexCommand cite
key "ollivier2015riemannian"

\end_inset

 or convolutional networks such as in 
\begin_inset CommandInset citation
LatexCommand cite
key "grosse2016kronecker"

\end_inset

.
 Networks with very small outputs can also be good candidates, as computing
 the jacobians and thus the FIM is linear in the number of outputs.
 Amongst them is the very popular family of GAN networks where the output
 is a single unit, and where the natural gradient could be used as a way
 to stabilize the training by balancing the rate of change of the output
 from each part (generator and discriminator).
\end_layout

\begin_layout Standard
Another direction of pursuing this research is to look for better approximations
 of the layer FIM/GN than the one of splitting into 2 expectations.
 Indeed, by better we do not mean an approximation that is closer in norm
 to the real FIM, but instead to an approximation that will give updates
 that are more efficient.
 Amortization can also be improved, by monitoring how our inverse statistics
 stay close to the true statistics, and just performing updates of these
 preconditioners when it is necessary, allowing for less updates (and less
 matrix inversions) for layers where the statistics do not change much.
\end_layout

\begin_layout Standard
As a last word, let us just state that second order methods have proven
 very powerful in a wide variety of optimization problems, but suffer from
 their computational complexity and the difficulty to use them in a setup
 with a lot of variables to optimize.
 We hope that by pursuing this effort of clarifying things and finding approxima
te methods that are computationnally cheaper, we will carry on contributing
 to more efficient neural network training.
\end_layout

\begin_layout Standard
\begin_inset Branch hidden
status collapsed

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "plain"

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
