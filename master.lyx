#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsbook
\use_default_options true
\begin_removed_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_removed_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Chapter
Neural networks
\end_layout

\begin_layout Section
Artificial intelligence
\end_layout

\begin_layout Section
Machine learning
\end_layout

\begin_layout Subsection
Parametric functions and learning
\end_layout

\begin_layout Itemize
task : given input datum, give an answer
\end_layout

\begin_layout Itemize
common probabilistic tasks 
\end_layout

\begin_layout Itemize
common industry tasks
\end_layout

\begin_layout Subsection
Empirical risk
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Itemize
bias / variance trade off
\end_layout

\begin_layout Itemize
overfitting
\end_layout

\begin_layout Section
Neural networks
\end_layout

\begin_layout Itemize
stack layers that produce new representations of the incoming data
\end_layout

\begin_layout Itemize
standard architecture : linear transformation / non-linearity elementwise
\end_layout

\begin_layout Section
Common types of neural networks
\end_layout

\begin_layout Subsection
Recurrent neural networks
\end_layout

\begin_layout Subsection
Convolutions
\end_layout

\begin_layout Subsection
Autoencoders
\end_layout

\begin_layout Section
More elaborated cost functions
\end_layout

\begin_layout Itemize
quadratic, cross entropy
\end_layout

\begin_layout Itemize
neural art
\end_layout

\begin_layout Itemize
GANs
\end_layout

\begin_layout Chapter
An example task for neural networks: learning to generate texture images
\end_layout

\begin_layout Chapter
Optimization of neural networks
\end_layout

\begin_layout Section
Gradient descent and backpropagation
\end_layout

\begin_layout Itemize
no closed form solution
\end_layout

\begin_layout Itemize
gradient descent
\end_layout

\begin_layout Itemize
backpropagation
\end_layout

\begin_layout Section
Stochastic gradient descent
\end_layout

\begin_layout Itemize
gradient estimate
\end_layout

\begin_layout Itemize
computational considerations
\end_layout

\begin_layout Itemize
regularization effect of stochasticity ?
\end_layout

\begin_layout Section
Hyperparameters and focus on the learning rate
\end_layout

\begin_layout Itemize
hyperparameters (difference with parameters)
\end_layout

\begin_layout Itemize
learning rate using the hessian + mathematical results
\end_layout

\begin_layout Section
Limits of (stochastic) gradient descent
\end_layout

\begin_layout Section
A standard benchmark: Autoencoding written digits
\end_layout

\begin_layout Itemize
presentation mnist
\end_layout

\begin_layout Itemize
task, history (hinton)
\end_layout

\begin_layout Itemize
limits of the architecture
\end_layout

\begin_deeper
\begin_layout Itemize
sigmoids
\end_layout

\begin_layout Itemize
multi layer perceptron instead of conv
\end_layout

\end_deeper
\begin_layout Itemize
limits of the benchmark
\end_layout

\begin_deeper
\begin_layout Itemize
specific task
\end_layout

\begin_layout Itemize
quadratic loss
\end_layout

\end_deeper
\begin_layout Itemize
HP plots
\end_layout

\begin_layout Chapter
Advanced optimization
\end_layout

\begin_layout Section
Gradient smoothing methods
\end_layout

\begin_layout Itemize
cost landscape with valleys and plateaus
\end_layout

\begin_layout Itemize
momentum
\end_layout

\begin_layout Section
Batch normalization
\end_layout

\begin_layout Itemize
covariate shift
\end_layout

\begin_layout Itemize
effect on the forward pass, and the gradient
\end_layout

\begin_layout Section
Second order methods
\end_layout

\begin_layout Itemize
quadratic hypothesis
\end_layout

\begin_layout Itemize
Newton, gauss-newton
\end_layout

\begin_layout Section
Gradient linearization
\end_layout

\begin_layout Itemize
motivation: do not update all parameters at once but instead layer by layer,
 updating the gradient to account for modification of the parameters before
 updating other layers
\end_layout

\begin_layout Itemize
expressions for common loss functions
\end_layout

\begin_layout Section
Second order: a new perspective
\end_layout

\begin_layout Itemize
factorization of the hessian & 
\end_layout

\begin_layout Itemize
measure of the induced change on the output of the function/on the loss
\end_layout

\begin_layout Section
Methods using the FIM
\end_layout

\begin_layout Itemize
kfac
\end_layout

\begin_layout Itemize
nnn
\end_layout

\begin_layout Itemize
overcoming catastrophic forgetting
\end_layout

\begin_layout Section
Links with well-known methods
\end_layout

\begin_layout Itemize
rmsprop
\end_layout

\begin_layout Itemize
msprop
\end_layout

\begin_layout Chapter*
Conclusions
\end_layout

\end_body
\end_document
