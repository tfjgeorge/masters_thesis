%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\begin{document}
This thesis presents my work during my master at MILA under the supervision
of Pascal Vincent.

This work is mostly focused on optimization in artificial neural networks.
My contributions are a deeper understanding of the many techniques
involving second order applied to neural networks, the derivation
of new expressions tuned for the particular structure of neural networks,
and their use in the definition of a new algorithm that competes with
current state of the art on a standard benchmark. In addition to this,
I present a new optimization method that slightly modifies the usual
backpropagation algorithm for computing derivatives.

This document is organized as follows:
\begin{itemize}
\item the first chapter sets up the basic framework of machine learning
and introduces neural networks ;
\item in chapter 2 we introduce the usual methods of optimization that have
enabled the recent successes in deep learning ;
\item in chapter 3 we review 2 second order methods called \textit{Gauss
Newton} and \textit{natural gradient}, and we show how they relate
and how they differ ;
\item in chapter 4 we describe the experimental setup that we use next to
assess the performance of our algorithms ;
\item in chapter 5 we introduce a new technique that modifies backpropagation
;
\item the last chapter presents a factorization that we can use to approximate
second order methods, and a new algorithm that exploits this factorization.
\end{itemize}

\end{document}
