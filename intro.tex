%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\begin{document}
This thesis presents my work during my master at MILA under the supervision
of Pascal Vincent.

This work is mostly focused on optimization in artificial neural networks.
My main contributions are a deeper understanding of the very particular
structure of the Fisher Information matrix and the Hessian matrix
when used in the setting of neural networks. It gives new insights
on some methods that have proven to be very good empirically such
as batch normalization or RMSProp, but without a formal understanding
of why they perform so well.

The first part sets up the basic framework of machine learning and
introduces neural networks.

In the second part, we introduce the usual methods of optimization
that have enabled the recent successes in machine learning and in
particular vision from 2012.

The third part showcases the results of my research. It is mostly
a dive in into the mathematical expressions behind second order methods,
the definition of algorithms that make use of some approximations
of these expressions, and the implementation and benchmarking of these
algorithms.
\end{document}
