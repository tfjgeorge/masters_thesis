#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
This thesis presents my work during my master at MILA under the supervision
 of Pascal Vincent.
\end_layout

\begin_layout Standard
Artificial neural networks are a powerful machine learning tool for modeling
 complex functions.
 Training a neural network for a given task often reduces to minimizing
 a scalar function of several millions of variables, which are the parameters
 of the model.
 While optimization is a full field of research on its own, usual methods
 do not scale to the order of magnitude of several millions of variables.
 For this reason neural networks practitioners stick to first order optimization
 methods, while not benefiting of the acceleration provided by using more
 powerful methods.
 Amongst the family of optimization methods, second order methods are a
 conceptually simple way of accelerating optimization.
 But practically, they require too much memory and computational power in
 order to be really useful when scaled to millions of parameters.
 We circumvent these practical constraints by approximating second order
 methods, trading off between computational cost, and speed up.
\end_layout

\begin_layout Standard
This work is mostly focused on optimization applied to artificial neural
 networks.
 My contributions are a deeper understanding of the many techniques involving
 second order techniques applied to neural networks, the derivation of new
 expressions tuned for the particular structure of neural networks, and
 their use in the definition of a new algorithm that competes with current
 state of the art on a standard benchmark.
 Crucially, this benchmark is a deep network with several millions of parameters
, and is trained to convergence in approximately 1h on a single computer.
\end_layout

\begin_layout Standard
In the process, I also explore alternatives to the backpropagation technique,
 which is used to efficiently obtain gradients in neural network optimization.
 As a core component of training neural networks, backpropagation has been
 the object of much research efforts since it was first used in the 1980s,
 but it has remained exactly the same since then.
 We contribute to this research by exploiting the sequential computations
 of backpropagation.
 We derive an alternative to backpropagation and experimentally show that
 it is able to find better update directions, at the cost of more computation.
 This contribution is of no practical use as is, because it requires too
 much computation.
 However it is a proof a concept that backpropagation can be improved.
 As the foundation of the whole training procedure of neural networks, a
 computationnally cheaper method of improving backpropagation would impact
 all other optimization methods that rely on computing the gradients.
\end_layout

\begin_layout Standard
This document is organized as follows:
\end_layout

\begin_layout Itemize
the first chapter sets up the basic framework of machine learning and introduces
 neural networks ;
\end_layout

\begin_layout Itemize
in chapter 2 we introduce the usual methods of optimization that have enabled
 the recent successes in deep learning ;
\end_layout

\begin_layout Itemize
in chapter 3 we review 2 second order methods called 
\shape italic
Gauss Newton
\shape default
 and 
\shape italic
natural gradient
\shape default
, and we show how they relate and how they differ ;
\end_layout

\begin_layout Itemize
in chapter 4 we describe the experimental setup that we use next to assess
 the performance of our algorithms, and we contribute a simple hyperparameter
 tuning procedure ;
\end_layout

\begin_layout Itemize
in chapter 5 we contribute a new technique that modifies backpropagation
 ;
\end_layout

\begin_layout Itemize
the last chapter presents a factorization that we can use to efficiently
 approximate second order methods.
 We contribute a detailed derivation of second order methods for the particular
 structure of neural networks, and we highlight the links with some old
 tricks.
 We also contribute a new optimization algorithm that exploits this factorizatio
n.
\end_layout

\end_body
\end_document
